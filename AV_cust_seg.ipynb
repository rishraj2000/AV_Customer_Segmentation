{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AV_cust_seg.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzXVp88laCb0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "08ba1aa5-2d5e-4841-f529-f8058a1e2507"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from matplotlib.image import imread\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khmvFr5tbvYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('train.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWCPb31lyDl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dft = pd.read_csv('test.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaL7dW5Lb3MC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "19b2bb8f-6617-4347-e0d0-d71f4b4c2d6a"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Ever_Married</th>\n",
              "      <th>Age</th>\n",
              "      <th>Graduated</th>\n",
              "      <th>Profession</th>\n",
              "      <th>Work_Experience</th>\n",
              "      <th>Spending_Score</th>\n",
              "      <th>Family_Size</th>\n",
              "      <th>Var_1</th>\n",
              "      <th>Segmentation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>462809</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>22</td>\n",
              "      <td>No</td>\n",
              "      <td>Healthcare</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Low</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Cat_4</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>462643</td>\n",
              "      <td>Female</td>\n",
              "      <td>Yes</td>\n",
              "      <td>38</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Engineer</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Average</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Cat_4</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>466315</td>\n",
              "      <td>Female</td>\n",
              "      <td>Yes</td>\n",
              "      <td>67</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Engineer</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Low</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Cat_6</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>461735</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>67</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Lawyer</td>\n",
              "      <td>0.0</td>\n",
              "      <td>High</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Cat_6</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>462669</td>\n",
              "      <td>Female</td>\n",
              "      <td>Yes</td>\n",
              "      <td>40</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Entertainment</td>\n",
              "      <td>NaN</td>\n",
              "      <td>High</td>\n",
              "      <td>6.0</td>\n",
              "      <td>Cat_6</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       ID  Gender Ever_Married  ...  Family_Size  Var_1 Segmentation\n",
              "0  462809    Male           No  ...          4.0  Cat_4            D\n",
              "1  462643  Female          Yes  ...          3.0  Cat_4            A\n",
              "2  466315  Female          Yes  ...          1.0  Cat_6            B\n",
              "3  461735    Male          Yes  ...          2.0  Cat_6            B\n",
              "4  462669  Female          Yes  ...          6.0  Cat_6            A\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtPB3lrfb9km",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "745c4c79-5049-4eb1-f80c-b7553a83ce09"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Age</th>\n",
              "      <th>Work_Experience</th>\n",
              "      <th>Family_Size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>8068.000000</td>\n",
              "      <td>8068.000000</td>\n",
              "      <td>7239.000000</td>\n",
              "      <td>7733.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>463479.214551</td>\n",
              "      <td>43.466906</td>\n",
              "      <td>2.641663</td>\n",
              "      <td>2.850123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2595.381232</td>\n",
              "      <td>16.711696</td>\n",
              "      <td>3.406763</td>\n",
              "      <td>1.531413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>458982.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>461240.750000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>463472.500000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>465744.250000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>467974.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  ID          Age  Work_Experience  Family_Size\n",
              "count    8068.000000  8068.000000      7239.000000  7733.000000\n",
              "mean   463479.214551    43.466906         2.641663     2.850123\n",
              "std      2595.381232    16.711696         3.406763     1.531413\n",
              "min    458982.000000    18.000000         0.000000     1.000000\n",
              "25%    461240.750000    30.000000         0.000000     2.000000\n",
              "50%    463472.500000    40.000000         1.000000     3.000000\n",
              "75%    465744.250000    53.000000         4.000000     4.000000\n",
              "max    467974.000000    89.000000        14.000000     9.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdQkAIU5u9vH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Ever_Married'] = df['Ever_Married'].fillna(0)\n",
        "df['Graduated'] = df['Graduated'].fillna(0)\n",
        "df['Ever_Married'] = df['Ever_Married'].fillna(0)\n",
        "####leave profession\n",
        "df['Work_Experience'] = df['Work_Experience'].fillna(0)\n",
        "df['Family_Size'] = df['Family_Size'].fillna(0)\n",
        "df['Var_1'] = df['Var_1'].fillna('no_cat')\n",
        "################\n",
        "dft['Ever_Married'] = dft['Ever_Married'].fillna(0)\n",
        "dft['Graduated'] = dft['Graduated'].fillna(0)\n",
        "dft['Ever_Married'] = dft['Ever_Married'].fillna(0)\n",
        "####leave profession\n",
        "dft['Work_Experience'] = dft['Work_Experience'].fillna(0)\n",
        "dft['Family_Size'] = dft['Family_Size'].fillna(0)\n",
        "dft['Var_1'] = dft['Var_1'].fillna('no_cat')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GgPcQT5xaXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Profession'] = df['Profession'].fillna('no_work')\n",
        "dft['Profession'] = dft['Profession'].fillna('no_work')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbr5quy1cA8l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "b05bf844-9124-45e1-c305-290c39174108"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8068 entries, 0 to 8067\n",
            "Data columns (total 11 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   ID               8068 non-null   int64  \n",
            " 1   Gender           8068 non-null   object \n",
            " 2   Ever_Married     8068 non-null   object \n",
            " 3   Age              8068 non-null   int64  \n",
            " 4   Graduated        8068 non-null   object \n",
            " 5   Profession       8068 non-null   object \n",
            " 6   Work_Experience  8068 non-null   float64\n",
            " 7   Spending_Score   8068 non-null   object \n",
            " 8   Family_Size      8068 non-null   float64\n",
            " 9   Var_1            8068 non-null   object \n",
            " 10  Segmentation     8068 non-null   object \n",
            "dtypes: float64(2), int64(2), object(7)\n",
            "memory usage: 693.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLj7ReljvJ6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "d0c40d72-fa28-4264-b172-d3df3289a599"
      },
      "source": [
        "dft.info()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2627 entries, 0 to 2626\n",
            "Data columns (total 10 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   ID               2627 non-null   int64  \n",
            " 1   Gender           2627 non-null   object \n",
            " 2   Ever_Married     2627 non-null   object \n",
            " 3   Age              2627 non-null   int64  \n",
            " 4   Graduated        2627 non-null   object \n",
            " 5   Profession       2627 non-null   object \n",
            " 6   Work_Experience  2627 non-null   float64\n",
            " 7   Spending_Score   2627 non-null   object \n",
            " 8   Family_Size      2627 non-null   float64\n",
            " 9   Var_1            2627 non-null   object \n",
            "dtypes: float64(2), int64(2), object(6)\n",
            "memory usage: 205.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_CZVFbecRvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ = df[['Gender', 'Profession']]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7gkJL8bdSiw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "1b82ba2c-6f15-4c2d-f944-6c68bacf353d"
      },
      "source": [
        "df_"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Profession</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>Healthcare</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Female</td>\n",
              "      <td>Engineer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Female</td>\n",
              "      <td>Engineer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Male</td>\n",
              "      <td>Lawyer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Female</td>\n",
              "      <td>Entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8063</th>\n",
              "      <td>Male</td>\n",
              "      <td>no_work</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8064</th>\n",
              "      <td>Male</td>\n",
              "      <td>Executive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8065</th>\n",
              "      <td>Female</td>\n",
              "      <td>Healthcare</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8066</th>\n",
              "      <td>Female</td>\n",
              "      <td>Healthcare</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8067</th>\n",
              "      <td>Male</td>\n",
              "      <td>Executive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8068 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Gender     Profession\n",
              "0       Male     Healthcare\n",
              "1     Female       Engineer\n",
              "2     Female       Engineer\n",
              "3       Male         Lawyer\n",
              "4     Female  Entertainment\n",
              "...      ...            ...\n",
              "8063    Male        no_work\n",
              "8064    Male      Executive\n",
              "8065  Female     Healthcare\n",
              "8066  Female     Healthcare\n",
              "8067    Male      Executive\n",
              "\n",
              "[8068 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubv6J_YbdTTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "f5bd6d36-3a17-48b3-9002-c617d24fc0ed"
      },
      "source": [
        "df_.pivot(columns=\"Profession\", values=\"Gender\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Profession</th>\n",
              "      <th>Artist</th>\n",
              "      <th>Doctor</th>\n",
              "      <th>Engineer</th>\n",
              "      <th>Entertainment</th>\n",
              "      <th>Executive</th>\n",
              "      <th>Healthcare</th>\n",
              "      <th>Homemaker</th>\n",
              "      <th>Lawyer</th>\n",
              "      <th>Marketing</th>\n",
              "      <th>no_work</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8063</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8064</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8065</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8066</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8067</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8068 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Profession Artist Doctor Engineer  ... Lawyer Marketing no_work\n",
              "0             NaN    NaN      NaN  ...    NaN       NaN     NaN\n",
              "1             NaN    NaN   Female  ...    NaN       NaN     NaN\n",
              "2             NaN    NaN   Female  ...    NaN       NaN     NaN\n",
              "3             NaN    NaN      NaN  ...   Male       NaN     NaN\n",
              "4             NaN    NaN      NaN  ...    NaN       NaN     NaN\n",
              "...           ...    ...      ...  ...    ...       ...     ...\n",
              "8063          NaN    NaN      NaN  ...    NaN       NaN    Male\n",
              "8064          NaN    NaN      NaN  ...    NaN       NaN     NaN\n",
              "8065          NaN    NaN      NaN  ...    NaN       NaN     NaN\n",
              "8066          NaN    NaN      NaN  ...    NaN       NaN     NaN\n",
              "8067          NaN    NaN      NaN  ...    NaN       NaN     NaN\n",
              "\n",
              "[8068 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0SvfQk-eyab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dft.drop('ID', axis=1, inplace=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4SasPqwfHQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop('ID', axis=1, inplace = True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VMY21cLy4O9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Segmentation'] = df['Segmentation'].replace(['A'],0)\n",
        "df['Segmentation'] = df['Segmentation'].replace(['B'],1)\n",
        "df['Segmentation'] = df['Segmentation'].replace(['C'],2)\n",
        "df['Segmentation'] = df['Segmentation'].replace(['D'],3)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj4zKZ7c5S7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Ever_Married'] = df['Ever_Married'].replace(['Yes'],1)\n",
        "df['Ever_Married'] = df['Ever_Married'].replace(['No'],0)\n",
        "dft['Ever_Married'] = dft['Ever_Married'].replace(['Yes'],1)\n",
        "dft['Ever_Married'] = dft['Ever_Married'].replace(['No'],0)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7RcVYuB8kff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Graduated'] = df['Graduated'].replace(['No'],0)\n",
        "df['Graduated'] = df['Graduated'].replace(['Yes'],1)\n",
        "dft['Graduated'] = dft['Graduated'].replace(['No'],0)\n",
        "dft['Graduated'] = dft['Graduated'].replace(['Yes'],1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smmapdb61THC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "0fb300cb-60a8-46f0-e874-341014a4b8d0"
      },
      "source": [
        "df.Var_1.unique()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Cat_4', 'Cat_6', 'Cat_7', 'Cat_3', 'Cat_1', 'Cat_2', 'no_cat',\n",
              "       'Cat_5'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLv1pgI5007P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "b83e0886-6163-47f2-e547-4fa316f3b932"
      },
      "source": [
        "df"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Ever_Married</th>\n",
              "      <th>Age</th>\n",
              "      <th>Graduated</th>\n",
              "      <th>Profession</th>\n",
              "      <th>Work_Experience</th>\n",
              "      <th>Spending_Score</th>\n",
              "      <th>Family_Size</th>\n",
              "      <th>Var_1</th>\n",
              "      <th>Segmentation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>Healthcare</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Low</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Cat_4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Female</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "      <td>Engineer</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Average</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Cat_4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Female</td>\n",
              "      <td>1</td>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>Engineer</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Low</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Cat_6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Male</td>\n",
              "      <td>1</td>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>Lawyer</td>\n",
              "      <td>0.0</td>\n",
              "      <td>High</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Cat_6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Female</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>Entertainment</td>\n",
              "      <td>0.0</td>\n",
              "      <td>High</td>\n",
              "      <td>6.0</td>\n",
              "      <td>Cat_6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8063</th>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>no_work</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Low</td>\n",
              "      <td>7.0</td>\n",
              "      <td>Cat_1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8064</th>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>Executive</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Low</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Cat_4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8065</th>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "      <td>Healthcare</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Low</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Cat_6</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8066</th>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>Healthcare</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Low</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Cat_6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8067</th>\n",
              "      <td>Male</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>Executive</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Average</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Cat_4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8068 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Gender  Ever_Married  Age  ...  Family_Size  Var_1  Segmentation\n",
              "0       Male             0   22  ...          4.0  Cat_4             3\n",
              "1     Female             1   38  ...          3.0  Cat_4             0\n",
              "2     Female             1   67  ...          1.0  Cat_6             1\n",
              "3       Male             1   67  ...          2.0  Cat_6             1\n",
              "4     Female             1   40  ...          6.0  Cat_6             0\n",
              "...      ...           ...  ...  ...          ...    ...           ...\n",
              "8063    Male             0   22  ...          7.0  Cat_1             3\n",
              "8064    Male             0   35  ...          4.0  Cat_4             3\n",
              "8065  Female             0   33  ...          1.0  Cat_6             3\n",
              "8066  Female             0   27  ...          4.0  Cat_6             1\n",
              "8067    Male             1   37  ...          3.0  Cat_4             1\n",
              "\n",
              "[8068 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDiy3bpC01gH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, val = train_test_split(df, test_size=0.2)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmf0i1gB3AE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = dft"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXeQnrbf3HxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('Segmentation')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWPgwwmx30tQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "938468ba-a330-422d-c2b4-154c828e3203"
      },
      "source": [
        "train.info()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 6454 entries, 3932 to 3317\n",
            "Data columns (total 10 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   Gender           6454 non-null   object \n",
            " 1   Ever_Married     6454 non-null   int64  \n",
            " 2   Age              6454 non-null   int64  \n",
            " 3   Graduated        6454 non-null   int64  \n",
            " 4   Profession       6454 non-null   object \n",
            " 5   Work_Experience  6454 non-null   float64\n",
            " 6   Spending_Score   6454 non-null   object \n",
            " 7   Family_Size      6454 non-null   float64\n",
            " 8   Var_1            6454 non-null   object \n",
            " 9   Segmentation     6454 non-null   int64  \n",
            "dtypes: float64(2), int64(4), object(4)\n",
            "memory usage: 554.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQhtj5-68UdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1ee2ac38-47ce-47a9-9738-6e5ad40255c5"
      },
      "source": [
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6454 train examples\n",
            "1614 validation examples\n",
            "2627 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOxe-LCI3XIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "#test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdBpqYtl85jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dftest_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe)))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAW5FOk-9Mre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ds = dftest_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eku0RHCt9jqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "age = feature_column.numeric_column(\"Age\")\n",
        "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FyvuG-e-VKS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bac89e2e-d950-4a00-c481-7fd1720f0155"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Ever_Married</th>\n",
              "      <th>Age</th>\n",
              "      <th>Graduated</th>\n",
              "      <th>Profession</th>\n",
              "      <th>Work_Experience</th>\n",
              "      <th>Spending_Score</th>\n",
              "      <th>Family_Size</th>\n",
              "      <th>Var_1</th>\n",
              "      <th>Segmentation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>Healthcare</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Low</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Cat_4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Female</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "      <td>Engineer</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Average</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Cat_4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Female</td>\n",
              "      <td>1</td>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>Engineer</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Low</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Cat_6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Male</td>\n",
              "      <td>1</td>\n",
              "      <td>67</td>\n",
              "      <td>1</td>\n",
              "      <td>Lawyer</td>\n",
              "      <td>0.0</td>\n",
              "      <td>High</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Cat_6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Female</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>Entertainment</td>\n",
              "      <td>0.0</td>\n",
              "      <td>High</td>\n",
              "      <td>6.0</td>\n",
              "      <td>Cat_6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Gender  Ever_Married  Age  ...  Family_Size  Var_1  Segmentation\n",
              "0    Male             0   22  ...          4.0  Cat_4             3\n",
              "1  Female             1   38  ...          3.0  Cat_4             0\n",
              "2  Female             1   67  ...          1.0  Cat_6             1\n",
              "3    Male             1   67  ...          2.0  Cat_6             1\n",
              "4  Female             1   40  ...          6.0  Cat_6             0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpad0_y__ItD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4e9a96ca-251a-4367-854f-a826a88dcb3e"
      },
      "source": [
        "df.Profession.unique()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Healthcare', 'Engineer', 'Lawyer', 'Entertainment', 'Artist',\n",
              "       'Executive', 'Doctor', 'Homemaker', 'Marketing', 'no_work'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj5UYaNb_VNR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "69866f85-b753-45fe-f2ba-adeda83db274"
      },
      "source": [
        "df.Var_1.unique()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Cat_4', 'Cat_6', 'Cat_7', 'Cat_3', 'Cat_1', 'Cat_2', 'no_cat',\n",
              "       'Cat_5'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYB8LBHr-gN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gender = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Gender', ['Male', 'Female'])\n",
        "prof = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Profession', ['Healthcare', 'Engineer', 'Lawyer', 'Entertainment', 'Artist',\n",
        "       'Executive', 'Doctor', 'Homemaker', 'Marketing', 'no_work'])\n",
        "ss = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Spending_Score', ['Low', 'Average', 'High'])\n",
        "var = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'Var_1', ['Cat_4', 'Cat_6', 'Cat_7', 'Cat_3', 'Cat_1', 'Cat_2', 'no_cat',\n",
        "       'Cat_5'])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJnDTqkCAiaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gender_oh = feature_column.indicator_column(gender)\n",
        "ss_oh = feature_column.indicator_column(ss)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahHQhwLvACnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prof_em = feature_column.embedding_column(prof, dimension=16)\n",
        "var_em = feature_column.embedding_column(var, dimension=16)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aASYFWsAt36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_columns = []\n",
        "\n",
        "# numeric cols\n",
        "for header in ['Age', 'Ever_Married', 'Graduated', 'Work_Experience', 'Family_Size']:\n",
        "  feature_columns.append(feature_column.numeric_column(header))\n",
        "\n",
        "gender_oh = feature_column.indicator_column(gender)\n",
        "feature_columns.append(gender_oh)\n",
        "\n",
        "ss_oh = feature_column.indicator_column(ss)\n",
        "feature_columns.append(ss_oh)\n",
        "\n",
        "prof_em = feature_column.embedding_column(prof, dimension=16)\n",
        "feature_columns.append(prof_em)\n",
        "\n",
        "var_em = feature_column.embedding_column(var, dimension=16)\n",
        "feature_columns.append(var_em)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2JCiNEiEyg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
        "feature_columns.append(age_buckets)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YoKKXV1DePG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "0a1f7dd3-ab09-4752-bd24-41efad1b59a1"
      },
      "source": [
        "feature_columns"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[NumericColumn(key='Age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='Ever_Married', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='Graduated', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='Work_Experience', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='Family_Size', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='Gender', vocabulary_list=('Male', 'Female'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
              " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='Spending_Score', vocabulary_list=('Low', 'Average', 'High'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
              " EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='Profession', vocabulary_list=('Healthcare', 'Engineer', 'Lawyer', 'Entertainment', 'Artist', 'Executive', 'Doctor', 'Homemaker', 'Marketing', 'no_work'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=16, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f772a5ca160>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True),\n",
              " EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='Var_1', vocabulary_list=('Cat_4', 'Cat_6', 'Cat_7', 'Cat_3', 'Cat_1', 'Cat_2', 'no_cat', 'Cat_5'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=16, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f77302d36a0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True),\n",
              " BucketizedColumn(source_column=NumericColumn(key='Age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O4Zv_7PB2Y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Usb4p2ElB86O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = dftest_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwq-pCm3GeYA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "442fa8c5-9cc1-4a8a-cd48-4f4126da7033"
      },
      "source": [
        "train_ds"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ({Gender: (None,), Ever_Married: (None,), Age: (None,), Graduated: (None,), Profession: (None,), Work_Experience: (None,), Spending_Score: (None,), Family_Size: (None,), Var_1: (None,)}, (None,)), types: ({Gender: tf.string, Ever_Married: tf.int64, Age: tf.int64, Graduated: tf.int64, Profession: tf.string, Work_Experience: tf.float64, Spending_Score: tf.string, Family_Size: tf.float64, Var_1: tf.string}, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKEtjzdPE8VH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de2f3114-4689-4f9e-9aa6-b8cea73713a4"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  layers.Dense(256, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(64, activation='relu'),\n",
        "  layers.Dense(32, activation='relu'),\n",
        "  layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='RMSProp',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history=model.fit(train_ds, validation_data=val_ds,\n",
        "          epochs=10)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Gender': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Ever_Married': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=int64>, 'Age': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'Graduated': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int64>, 'Profession': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Work_Experience': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, 'Spending_Score': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Family_Size': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, 'Var_1': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=string>}\n",
            "Consider rewriting this model with the Functional API.\n",
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Gender': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Ever_Married': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=int64>, 'Age': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'Graduated': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int64>, 'Profession': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Work_Experience': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, 'Spending_Score': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Family_Size': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, 'Var_1': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=string>}\n",
            "Consider rewriting this model with the Functional API.\n",
            "188/202 [==========================>...] - ETA: 0s - loss: 1.2555 - accuracy: 0.4224WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Gender': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Ever_Married': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=int64>, 'Age': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'Graduated': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int64>, 'Profession': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Work_Experience': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, 'Spending_Score': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Family_Size': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, 'Var_1': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=string>}\n",
            "Consider rewriting this model with the Functional API.\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 1.2508 - accuracy: 0.4245 - val_loss: 1.1531 - val_accuracy: 0.4734\n",
            "Epoch 2/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1253 - accuracy: 0.4884 - val_loss: 1.2894 - val_accuracy: 0.4226\n",
            "Epoch 3/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.1009 - accuracy: 0.4967 - val_loss: 1.2023 - val_accuracy: 0.4703\n",
            "Epoch 4/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0813 - accuracy: 0.5070 - val_loss: 1.0610 - val_accuracy: 0.5260\n",
            "Epoch 5/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0692 - accuracy: 0.5158 - val_loss: 1.0990 - val_accuracy: 0.5161\n",
            "Epoch 6/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0616 - accuracy: 0.5160 - val_loss: 1.0756 - val_accuracy: 0.5223\n",
            "Epoch 7/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0535 - accuracy: 0.5178 - val_loss: 1.2888 - val_accuracy: 0.4709\n",
            "Epoch 8/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0509 - accuracy: 0.5279 - val_loss: 1.0620 - val_accuracy: 0.5273\n",
            "Epoch 9/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0429 - accuracy: 0.5299 - val_loss: 1.0618 - val_accuracy: 0.5204\n",
            "Epoch 10/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0404 - accuracy: 0.5273 - val_loss: 1.0559 - val_accuracy: 0.5397\n",
            "Epoch 11/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0335 - accuracy: 0.5372 - val_loss: 1.0436 - val_accuracy: 0.5403\n",
            "Epoch 12/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0277 - accuracy: 0.5384 - val_loss: 1.0685 - val_accuracy: 0.5304\n",
            "Epoch 13/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0259 - accuracy: 0.5412 - val_loss: 1.0970 - val_accuracy: 0.5403\n",
            "Epoch 14/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0237 - accuracy: 0.5414 - val_loss: 1.0943 - val_accuracy: 0.5050\n",
            "Epoch 15/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0192 - accuracy: 0.5418 - val_loss: 1.0554 - val_accuracy: 0.5421\n",
            "Epoch 16/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0136 - accuracy: 0.5415 - val_loss: 1.0592 - val_accuracy: 0.5465\n",
            "Epoch 17/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0096 - accuracy: 0.5496 - val_loss: 1.0538 - val_accuracy: 0.5483\n",
            "Epoch 18/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0040 - accuracy: 0.5521 - val_loss: 1.0581 - val_accuracy: 0.5291\n",
            "Epoch 19/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 1.0019 - accuracy: 0.5525 - val_loss: 1.0586 - val_accuracy: 0.5260\n",
            "Epoch 20/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9979 - accuracy: 0.5531 - val_loss: 1.0910 - val_accuracy: 0.5452\n",
            "Epoch 21/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9950 - accuracy: 0.5566 - val_loss: 1.0558 - val_accuracy: 0.5372\n",
            "Epoch 22/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9922 - accuracy: 0.5555 - val_loss: 1.1450 - val_accuracy: 0.4969\n",
            "Epoch 23/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9892 - accuracy: 0.5601 - val_loss: 1.1031 - val_accuracy: 0.5322\n",
            "Epoch 24/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9849 - accuracy: 0.5581 - val_loss: 1.0711 - val_accuracy: 0.5260\n",
            "Epoch 25/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9825 - accuracy: 0.5610 - val_loss: 1.0962 - val_accuracy: 0.5372\n",
            "Epoch 26/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9781 - accuracy: 0.5604 - val_loss: 1.0930 - val_accuracy: 0.5155\n",
            "Epoch 27/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9747 - accuracy: 0.5646 - val_loss: 1.0823 - val_accuracy: 0.5279\n",
            "Epoch 28/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9723 - accuracy: 0.5700 - val_loss: 1.0824 - val_accuracy: 0.5366\n",
            "Epoch 29/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.9685 - accuracy: 0.5724 - val_loss: 1.1019 - val_accuracy: 0.5173\n",
            "Epoch 30/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9674 - accuracy: 0.5736 - val_loss: 1.1189 - val_accuracy: 0.5019\n",
            "Epoch 31/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9591 - accuracy: 0.5759 - val_loss: 1.1311 - val_accuracy: 0.5198\n",
            "Epoch 32/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9559 - accuracy: 0.5733 - val_loss: 1.1849 - val_accuracy: 0.5359\n",
            "Epoch 33/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9548 - accuracy: 0.5809 - val_loss: 1.1238 - val_accuracy: 0.5341\n",
            "Epoch 34/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9513 - accuracy: 0.5755 - val_loss: 1.1504 - val_accuracy: 0.5223\n",
            "Epoch 35/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9450 - accuracy: 0.5806 - val_loss: 1.1392 - val_accuracy: 0.5112\n",
            "Epoch 36/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9432 - accuracy: 0.5843 - val_loss: 1.1634 - val_accuracy: 0.5260\n",
            "Epoch 37/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9410 - accuracy: 0.5812 - val_loss: 1.1352 - val_accuracy: 0.5081\n",
            "Epoch 38/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9364 - accuracy: 0.5806 - val_loss: 1.1352 - val_accuracy: 0.5279\n",
            "Epoch 39/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9308 - accuracy: 0.5846 - val_loss: 1.1710 - val_accuracy: 0.5180\n",
            "Epoch 40/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9249 - accuracy: 0.5892 - val_loss: 1.2347 - val_accuracy: 0.5242\n",
            "Epoch 41/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9227 - accuracy: 0.5885 - val_loss: 1.2135 - val_accuracy: 0.5192\n",
            "Epoch 42/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9182 - accuracy: 0.5948 - val_loss: 1.2064 - val_accuracy: 0.5180\n",
            "Epoch 43/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9153 - accuracy: 0.5911 - val_loss: 1.2472 - val_accuracy: 0.5167\n",
            "Epoch 44/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9168 - accuracy: 0.5920 - val_loss: 1.1801 - val_accuracy: 0.5143\n",
            "Epoch 45/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9107 - accuracy: 0.5970 - val_loss: 1.2196 - val_accuracy: 0.5161\n",
            "Epoch 46/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9067 - accuracy: 0.5959 - val_loss: 1.2812 - val_accuracy: 0.5136\n",
            "Epoch 47/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.9011 - accuracy: 0.6049 - val_loss: 1.2728 - val_accuracy: 0.5291\n",
            "Epoch 48/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8997 - accuracy: 0.5968 - val_loss: 1.2350 - val_accuracy: 0.5130\n",
            "Epoch 49/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8870 - accuracy: 0.6043 - val_loss: 1.3270 - val_accuracy: 0.5180\n",
            "Epoch 50/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8932 - accuracy: 0.6105 - val_loss: 1.3301 - val_accuracy: 0.5192\n",
            "Epoch 51/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8806 - accuracy: 0.6060 - val_loss: 1.3312 - val_accuracy: 0.5074\n",
            "Epoch 52/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8843 - accuracy: 0.6095 - val_loss: 1.2811 - val_accuracy: 0.5112\n",
            "Epoch 53/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8780 - accuracy: 0.6125 - val_loss: 1.3100 - val_accuracy: 0.5217\n",
            "Epoch 54/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8787 - accuracy: 0.6085 - val_loss: 1.3900 - val_accuracy: 0.5074\n",
            "Epoch 55/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8668 - accuracy: 0.6130 - val_loss: 1.3447 - val_accuracy: 0.5093\n",
            "Epoch 56/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8653 - accuracy: 0.6193 - val_loss: 1.4408 - val_accuracy: 0.5099\n",
            "Epoch 57/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8565 - accuracy: 0.6213 - val_loss: 1.4380 - val_accuracy: 0.5081\n",
            "Epoch 58/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8520 - accuracy: 0.6227 - val_loss: 1.4400 - val_accuracy: 0.5266\n",
            "Epoch 59/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8521 - accuracy: 0.6238 - val_loss: 1.4151 - val_accuracy: 0.5074\n",
            "Epoch 60/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8446 - accuracy: 0.6232 - val_loss: 1.3875 - val_accuracy: 0.5118\n",
            "Epoch 61/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8390 - accuracy: 0.6342 - val_loss: 1.5714 - val_accuracy: 0.5266\n",
            "Epoch 62/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8396 - accuracy: 0.6260 - val_loss: 1.4550 - val_accuracy: 0.5242\n",
            "Epoch 63/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8317 - accuracy: 0.6371 - val_loss: 1.6443 - val_accuracy: 0.5019\n",
            "Epoch 64/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8314 - accuracy: 0.6340 - val_loss: 1.5001 - val_accuracy: 0.5118\n",
            "Epoch 65/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8272 - accuracy: 0.6317 - val_loss: 1.5058 - val_accuracy: 0.4969\n",
            "Epoch 66/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8191 - accuracy: 0.6407 - val_loss: 1.5799 - val_accuracy: 0.4690\n",
            "Epoch 67/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8195 - accuracy: 0.6387 - val_loss: 1.4423 - val_accuracy: 0.5000\n",
            "Epoch 68/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8169 - accuracy: 0.6405 - val_loss: 1.5030 - val_accuracy: 0.5155\n",
            "Epoch 69/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8110 - accuracy: 0.6412 - val_loss: 1.6407 - val_accuracy: 0.5012\n",
            "Epoch 70/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8112 - accuracy: 0.6425 - val_loss: 1.6742 - val_accuracy: 0.5180\n",
            "Epoch 71/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8048 - accuracy: 0.6483 - val_loss: 1.5086 - val_accuracy: 0.4820\n",
            "Epoch 72/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.8005 - accuracy: 0.6398 - val_loss: 1.8865 - val_accuracy: 0.5118\n",
            "Epoch 73/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7964 - accuracy: 0.6456 - val_loss: 1.6043 - val_accuracy: 0.5043\n",
            "Epoch 74/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7914 - accuracy: 0.6509 - val_loss: 1.6730 - val_accuracy: 0.5056\n",
            "Epoch 75/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7873 - accuracy: 0.6474 - val_loss: 1.8204 - val_accuracy: 0.4876\n",
            "Epoch 76/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7831 - accuracy: 0.6545 - val_loss: 1.5916 - val_accuracy: 0.5087\n",
            "Epoch 77/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7740 - accuracy: 0.6591 - val_loss: 1.8394 - val_accuracy: 0.4895\n",
            "Epoch 78/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7827 - accuracy: 0.6534 - val_loss: 1.8742 - val_accuracy: 0.4994\n",
            "Epoch 79/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7726 - accuracy: 0.6568 - val_loss: 1.7370 - val_accuracy: 0.4857\n",
            "Epoch 80/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7702 - accuracy: 0.6596 - val_loss: 1.7645 - val_accuracy: 0.4957\n",
            "Epoch 81/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7649 - accuracy: 0.6636 - val_loss: 1.8910 - val_accuracy: 0.4944\n",
            "Epoch 82/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7624 - accuracy: 0.6604 - val_loss: 2.1252 - val_accuracy: 0.4994\n",
            "Epoch 83/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7620 - accuracy: 0.6604 - val_loss: 1.7588 - val_accuracy: 0.5000\n",
            "Epoch 84/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7545 - accuracy: 0.6666 - val_loss: 1.8848 - val_accuracy: 0.5155\n",
            "Epoch 85/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7629 - accuracy: 0.6615 - val_loss: 2.0966 - val_accuracy: 0.5031\n",
            "Epoch 86/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7502 - accuracy: 0.6639 - val_loss: 1.9765 - val_accuracy: 0.4709\n",
            "Epoch 87/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7524 - accuracy: 0.6715 - val_loss: 1.9389 - val_accuracy: 0.4932\n",
            "Epoch 88/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7421 - accuracy: 0.6692 - val_loss: 2.2076 - val_accuracy: 0.4994\n",
            "Epoch 89/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7390 - accuracy: 0.6749 - val_loss: 2.0446 - val_accuracy: 0.4901\n",
            "Epoch 90/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7357 - accuracy: 0.6788 - val_loss: 2.1309 - val_accuracy: 0.4950\n",
            "Epoch 91/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7376 - accuracy: 0.6797 - val_loss: 1.9645 - val_accuracy: 0.4950\n",
            "Epoch 92/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7249 - accuracy: 0.6797 - val_loss: 2.0595 - val_accuracy: 0.4808\n",
            "Epoch 93/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7280 - accuracy: 0.6786 - val_loss: 2.1621 - val_accuracy: 0.4833\n",
            "Epoch 94/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7251 - accuracy: 0.6830 - val_loss: 2.1744 - val_accuracy: 0.4876\n",
            "Epoch 95/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7219 - accuracy: 0.6819 - val_loss: 2.4366 - val_accuracy: 0.4932\n",
            "Epoch 96/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7179 - accuracy: 0.6858 - val_loss: 1.9676 - val_accuracy: 0.4752\n",
            "Epoch 97/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7169 - accuracy: 0.6869 - val_loss: 2.3451 - val_accuracy: 0.5050\n",
            "Epoch 98/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7132 - accuracy: 0.6836 - val_loss: 2.2972 - val_accuracy: 0.4975\n",
            "Epoch 99/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7156 - accuracy: 0.6864 - val_loss: 2.4642 - val_accuracy: 0.4796\n",
            "Epoch 100/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7262 - accuracy: 0.6870 - val_loss: 2.3764 - val_accuracy: 0.4771\n",
            "Epoch 101/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7051 - accuracy: 0.6831 - val_loss: 2.4753 - val_accuracy: 0.4963\n",
            "Epoch 102/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7000 - accuracy: 0.6842 - val_loss: 2.3812 - val_accuracy: 0.4777\n",
            "Epoch 103/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.6949 - val_loss: 2.3402 - val_accuracy: 0.4913\n",
            "Epoch 104/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6960 - accuracy: 0.6881 - val_loss: 2.2265 - val_accuracy: 0.4820\n",
            "Epoch 105/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.6955 - val_loss: 2.5510 - val_accuracy: 0.4734\n",
            "Epoch 106/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6869 - accuracy: 0.6997 - val_loss: 2.9612 - val_accuracy: 0.4919\n",
            "Epoch 107/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.7024 - accuracy: 0.6924 - val_loss: 2.4319 - val_accuracy: 0.4808\n",
            "Epoch 108/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.6982 - val_loss: 2.6107 - val_accuracy: 0.4845\n",
            "Epoch 109/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6841 - accuracy: 0.6968 - val_loss: 2.6405 - val_accuracy: 0.4882\n",
            "Epoch 110/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.7017 - val_loss: 2.9006 - val_accuracy: 0.4864\n",
            "Epoch 111/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6851 - accuracy: 0.7000 - val_loss: 2.4747 - val_accuracy: 0.4870\n",
            "Epoch 112/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.7028 - val_loss: 2.5311 - val_accuracy: 0.4876\n",
            "Epoch 113/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.7078 - val_loss: 2.5371 - val_accuracy: 0.5025\n",
            "Epoch 114/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6756 - accuracy: 0.7036 - val_loss: 2.6601 - val_accuracy: 0.4796\n",
            "Epoch 115/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.7044 - val_loss: 2.8926 - val_accuracy: 0.4802\n",
            "Epoch 116/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6656 - accuracy: 0.7086 - val_loss: 2.6489 - val_accuracy: 0.4765\n",
            "Epoch 117/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.7078 - val_loss: 2.5719 - val_accuracy: 0.4783\n",
            "Epoch 118/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.7140 - val_loss: 2.5786 - val_accuracy: 0.4882\n",
            "Epoch 119/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6656 - accuracy: 0.7090 - val_loss: 2.9809 - val_accuracy: 0.4703\n",
            "Epoch 120/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.7149 - val_loss: 2.8343 - val_accuracy: 0.4777\n",
            "Epoch 121/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.7123 - val_loss: 2.6887 - val_accuracy: 0.4690\n",
            "Epoch 122/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.7129 - val_loss: 2.5507 - val_accuracy: 0.4690\n",
            "Epoch 123/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.7152 - val_loss: 2.7485 - val_accuracy: 0.4690\n",
            "Epoch 124/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.7104 - val_loss: 2.7282 - val_accuracy: 0.4783\n",
            "Epoch 125/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.7188 - val_loss: 3.0639 - val_accuracy: 0.4808\n",
            "Epoch 126/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.7182 - val_loss: 3.1124 - val_accuracy: 0.4690\n",
            "Epoch 127/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6457 - accuracy: 0.7140 - val_loss: 3.0848 - val_accuracy: 0.4709\n",
            "Epoch 128/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.7178 - val_loss: 3.0971 - val_accuracy: 0.4796\n",
            "Epoch 129/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.7163 - val_loss: 2.9854 - val_accuracy: 0.4721\n",
            "Epoch 130/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.7192 - val_loss: 3.1439 - val_accuracy: 0.4734\n",
            "Epoch 131/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.7188 - val_loss: 3.1081 - val_accuracy: 0.4876\n",
            "Epoch 132/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.7219 - val_loss: 3.2382 - val_accuracy: 0.4814\n",
            "Epoch 133/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.7267 - val_loss: 3.5251 - val_accuracy: 0.4870\n",
            "Epoch 134/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.7271 - val_loss: 3.2154 - val_accuracy: 0.4690\n",
            "Epoch 135/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.7324 - val_loss: 2.9934 - val_accuracy: 0.4820\n",
            "Epoch 136/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6244 - accuracy: 0.7319 - val_loss: 3.0510 - val_accuracy: 0.4765\n",
            "Epoch 137/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.7304 - val_loss: 3.3113 - val_accuracy: 0.4857\n",
            "Epoch 138/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.7319 - val_loss: 3.6421 - val_accuracy: 0.4833\n",
            "Epoch 139/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.7307 - val_loss: 3.5027 - val_accuracy: 0.4796\n",
            "Epoch 140/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6149 - accuracy: 0.7295 - val_loss: 3.3395 - val_accuracy: 0.4579\n",
            "Epoch 141/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.7237 - val_loss: 3.5061 - val_accuracy: 0.4864\n",
            "Epoch 142/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.7258 - val_loss: 3.2393 - val_accuracy: 0.4740\n",
            "Epoch 143/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6072 - accuracy: 0.7304 - val_loss: 3.4481 - val_accuracy: 0.4529\n",
            "Epoch 144/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.7268 - val_loss: 3.6905 - val_accuracy: 0.4969\n",
            "Epoch 145/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.7337 - val_loss: 3.6243 - val_accuracy: 0.4715\n",
            "Epoch 146/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.7357 - val_loss: 3.4071 - val_accuracy: 0.4647\n",
            "Epoch 147/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.7333 - val_loss: 3.2716 - val_accuracy: 0.4727\n",
            "Epoch 148/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6073 - accuracy: 0.7372 - val_loss: 3.6282 - val_accuracy: 0.4870\n",
            "Epoch 149/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6039 - accuracy: 0.7433 - val_loss: 3.8194 - val_accuracy: 0.4814\n",
            "Epoch 150/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6038 - accuracy: 0.7346 - val_loss: 3.5784 - val_accuracy: 0.4981\n",
            "Epoch 151/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6011 - accuracy: 0.7402 - val_loss: 4.0961 - val_accuracy: 0.4771\n",
            "Epoch 152/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6093 - accuracy: 0.7330 - val_loss: 3.4574 - val_accuracy: 0.4740\n",
            "Epoch 153/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6020 - accuracy: 0.7411 - val_loss: 3.2661 - val_accuracy: 0.4827\n",
            "Epoch 154/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5955 - accuracy: 0.7383 - val_loss: 3.6342 - val_accuracy: 0.4857\n",
            "Epoch 155/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6173 - accuracy: 0.7343 - val_loss: 3.7417 - val_accuracy: 0.4758\n",
            "Epoch 156/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.7323 - val_loss: 3.6445 - val_accuracy: 0.4709\n",
            "Epoch 157/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5998 - accuracy: 0.7428 - val_loss: 3.6508 - val_accuracy: 0.4696\n",
            "Epoch 158/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5987 - accuracy: 0.7358 - val_loss: 3.9087 - val_accuracy: 0.4653\n",
            "Epoch 159/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.6015 - accuracy: 0.7397 - val_loss: 4.1350 - val_accuracy: 0.4765\n",
            "Epoch 160/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5946 - accuracy: 0.7430 - val_loss: 3.4966 - val_accuracy: 0.4665\n",
            "Epoch 161/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5900 - accuracy: 0.7454 - val_loss: 3.8344 - val_accuracy: 0.4839\n",
            "Epoch 162/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5903 - accuracy: 0.7450 - val_loss: 3.6406 - val_accuracy: 0.4641\n",
            "Epoch 163/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5923 - accuracy: 0.7485 - val_loss: 4.1296 - val_accuracy: 0.4913\n",
            "Epoch 164/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5985 - accuracy: 0.7409 - val_loss: 4.0292 - val_accuracy: 0.4789\n",
            "Epoch 165/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5861 - accuracy: 0.7457 - val_loss: 3.7881 - val_accuracy: 0.4895\n",
            "Epoch 166/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5906 - accuracy: 0.7453 - val_loss: 4.0284 - val_accuracy: 0.4690\n",
            "Epoch 167/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5867 - accuracy: 0.7484 - val_loss: 4.3056 - val_accuracy: 0.4703\n",
            "Epoch 168/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5803 - accuracy: 0.7468 - val_loss: 4.3309 - val_accuracy: 0.4789\n",
            "Epoch 169/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5841 - accuracy: 0.7445 - val_loss: 4.1946 - val_accuracy: 0.4672\n",
            "Epoch 170/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5846 - accuracy: 0.7467 - val_loss: 4.1835 - val_accuracy: 0.4814\n",
            "Epoch 171/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5886 - accuracy: 0.7488 - val_loss: 3.7978 - val_accuracy: 0.4610\n",
            "Epoch 172/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5773 - accuracy: 0.7482 - val_loss: 4.2920 - val_accuracy: 0.4709\n",
            "Epoch 173/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5804 - accuracy: 0.7479 - val_loss: 4.1864 - val_accuracy: 0.4907\n",
            "Epoch 174/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5818 - accuracy: 0.7505 - val_loss: 4.6959 - val_accuracy: 0.4864\n",
            "Epoch 175/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5784 - accuracy: 0.7504 - val_loss: 4.0604 - val_accuracy: 0.4709\n",
            "Epoch 176/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5771 - accuracy: 0.7490 - val_loss: 4.2135 - val_accuracy: 0.4888\n",
            "Epoch 177/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5706 - accuracy: 0.7544 - val_loss: 3.9228 - val_accuracy: 0.4752\n",
            "Epoch 178/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5797 - accuracy: 0.7488 - val_loss: 4.4263 - val_accuracy: 0.4653\n",
            "Epoch 179/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5862 - accuracy: 0.7546 - val_loss: 4.2700 - val_accuracy: 0.4864\n",
            "Epoch 180/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5681 - accuracy: 0.7567 - val_loss: 4.0811 - val_accuracy: 0.4715\n",
            "Epoch 181/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5682 - accuracy: 0.7524 - val_loss: 4.3156 - val_accuracy: 0.4789\n",
            "Epoch 182/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5663 - accuracy: 0.7560 - val_loss: 4.5150 - val_accuracy: 0.4827\n",
            "Epoch 183/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5615 - accuracy: 0.7564 - val_loss: 4.8517 - val_accuracy: 0.4870\n",
            "Epoch 184/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5729 - accuracy: 0.7589 - val_loss: 4.5442 - val_accuracy: 0.4758\n",
            "Epoch 185/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5857 - accuracy: 0.7524 - val_loss: 4.2144 - val_accuracy: 0.4820\n",
            "Epoch 186/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5684 - accuracy: 0.7586 - val_loss: 4.4660 - val_accuracy: 0.4864\n",
            "Epoch 187/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5756 - accuracy: 0.7522 - val_loss: 4.5757 - val_accuracy: 0.4802\n",
            "Epoch 188/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5496 - accuracy: 0.7569 - val_loss: 4.4819 - val_accuracy: 0.4802\n",
            "Epoch 189/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5645 - accuracy: 0.7631 - val_loss: 4.5250 - val_accuracy: 0.4659\n",
            "Epoch 190/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5631 - accuracy: 0.7595 - val_loss: 4.5079 - val_accuracy: 0.4765\n",
            "Epoch 191/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5586 - accuracy: 0.7614 - val_loss: 4.6676 - val_accuracy: 0.4721\n",
            "Epoch 192/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5711 - accuracy: 0.7567 - val_loss: 4.4795 - val_accuracy: 0.4771\n",
            "Epoch 193/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5525 - accuracy: 0.7603 - val_loss: 4.8445 - val_accuracy: 0.4944\n",
            "Epoch 194/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5632 - accuracy: 0.7605 - val_loss: 4.7526 - val_accuracy: 0.4833\n",
            "Epoch 195/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5793 - accuracy: 0.7580 - val_loss: 4.8118 - val_accuracy: 0.4827\n",
            "Epoch 196/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5600 - accuracy: 0.7648 - val_loss: 4.9773 - val_accuracy: 0.4901\n",
            "Epoch 197/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5667 - accuracy: 0.7592 - val_loss: 4.8478 - val_accuracy: 0.4876\n",
            "Epoch 198/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5658 - accuracy: 0.7605 - val_loss: 4.7806 - val_accuracy: 0.4907\n",
            "Epoch 199/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5492 - accuracy: 0.7662 - val_loss: 5.0911 - val_accuracy: 0.4994\n",
            "Epoch 200/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5631 - accuracy: 0.7645 - val_loss: 4.8797 - val_accuracy: 0.4734\n",
            "Epoch 201/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5464 - accuracy: 0.7631 - val_loss: 4.9722 - val_accuracy: 0.4882\n",
            "Epoch 202/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5657 - accuracy: 0.7625 - val_loss: 4.7425 - val_accuracy: 0.4796\n",
            "Epoch 203/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5533 - accuracy: 0.7628 - val_loss: 4.8825 - val_accuracy: 0.4752\n",
            "Epoch 204/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5547 - accuracy: 0.7614 - val_loss: 4.5009 - val_accuracy: 0.4523\n",
            "Epoch 205/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5776 - accuracy: 0.7619 - val_loss: 4.4974 - val_accuracy: 0.4634\n",
            "Epoch 206/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5553 - accuracy: 0.7651 - val_loss: 4.6017 - val_accuracy: 0.4709\n",
            "Epoch 207/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5520 - accuracy: 0.7623 - val_loss: 4.9662 - val_accuracy: 0.4752\n",
            "Epoch 208/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5478 - accuracy: 0.7626 - val_loss: 5.2146 - val_accuracy: 0.4926\n",
            "Epoch 209/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5469 - accuracy: 0.7687 - val_loss: 5.2089 - val_accuracy: 0.4765\n",
            "Epoch 210/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5633 - accuracy: 0.7702 - val_loss: 5.1076 - val_accuracy: 0.4771\n",
            "Epoch 211/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5388 - accuracy: 0.7705 - val_loss: 5.5246 - val_accuracy: 0.4845\n",
            "Epoch 212/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5460 - accuracy: 0.7677 - val_loss: 4.7094 - val_accuracy: 0.4659\n",
            "Epoch 213/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5510 - accuracy: 0.7673 - val_loss: 4.6838 - val_accuracy: 0.4771\n",
            "Epoch 214/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5386 - accuracy: 0.7693 - val_loss: 5.1200 - val_accuracy: 0.4758\n",
            "Epoch 215/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5530 - accuracy: 0.7698 - val_loss: 5.2930 - val_accuracy: 0.4808\n",
            "Epoch 216/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5466 - accuracy: 0.7673 - val_loss: 5.1267 - val_accuracy: 0.4944\n",
            "Epoch 217/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5274 - accuracy: 0.7685 - val_loss: 5.0742 - val_accuracy: 0.4845\n",
            "Epoch 218/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5544 - accuracy: 0.7670 - val_loss: 5.3545 - val_accuracy: 0.4833\n",
            "Epoch 219/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5624 - accuracy: 0.7609 - val_loss: 5.4530 - val_accuracy: 0.4752\n",
            "Epoch 220/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5335 - accuracy: 0.7729 - val_loss: 5.2503 - val_accuracy: 0.4684\n",
            "Epoch 221/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5349 - accuracy: 0.7663 - val_loss: 4.8162 - val_accuracy: 0.4758\n",
            "Epoch 222/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5388 - accuracy: 0.7713 - val_loss: 5.1428 - val_accuracy: 0.4796\n",
            "Epoch 223/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5384 - accuracy: 0.7767 - val_loss: 5.2298 - val_accuracy: 0.4796\n",
            "Epoch 224/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5337 - accuracy: 0.7677 - val_loss: 4.7907 - val_accuracy: 0.4492\n",
            "Epoch 225/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5457 - accuracy: 0.7730 - val_loss: 5.0158 - val_accuracy: 0.4876\n",
            "Epoch 226/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5419 - accuracy: 0.7729 - val_loss: 4.7804 - val_accuracy: 0.4622\n",
            "Epoch 227/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5342 - accuracy: 0.7708 - val_loss: 5.2222 - val_accuracy: 0.4876\n",
            "Epoch 228/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5230 - accuracy: 0.7733 - val_loss: 5.0967 - val_accuracy: 0.4572\n",
            "Epoch 229/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5188 - accuracy: 0.7735 - val_loss: 5.4140 - val_accuracy: 0.4703\n",
            "Epoch 230/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5445 - accuracy: 0.7704 - val_loss: 5.1956 - val_accuracy: 0.4665\n",
            "Epoch 231/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5362 - accuracy: 0.7681 - val_loss: 5.4299 - val_accuracy: 0.4827\n",
            "Epoch 232/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5326 - accuracy: 0.7730 - val_loss: 5.4411 - val_accuracy: 0.4672\n",
            "Epoch 233/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5304 - accuracy: 0.7784 - val_loss: 5.4331 - val_accuracy: 0.4833\n",
            "Epoch 234/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5371 - accuracy: 0.7673 - val_loss: 5.5421 - val_accuracy: 0.4641\n",
            "Epoch 235/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5217 - accuracy: 0.7722 - val_loss: 5.5665 - val_accuracy: 0.4882\n",
            "Epoch 236/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5307 - accuracy: 0.7681 - val_loss: 5.7322 - val_accuracy: 0.4901\n",
            "Epoch 237/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5308 - accuracy: 0.7694 - val_loss: 5.4038 - val_accuracy: 0.4727\n",
            "Epoch 238/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5178 - accuracy: 0.7789 - val_loss: 5.2968 - val_accuracy: 0.4740\n",
            "Epoch 239/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5338 - accuracy: 0.7718 - val_loss: 5.1267 - val_accuracy: 0.4740\n",
            "Epoch 240/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7775 - val_loss: 5.4215 - val_accuracy: 0.4616\n",
            "Epoch 241/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5229 - accuracy: 0.7851 - val_loss: 6.5201 - val_accuracy: 0.4864\n",
            "Epoch 242/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5409 - accuracy: 0.7780 - val_loss: 6.6427 - val_accuracy: 0.4721\n",
            "Epoch 243/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5563 - accuracy: 0.7691 - val_loss: 5.1742 - val_accuracy: 0.4864\n",
            "Epoch 244/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5322 - accuracy: 0.7761 - val_loss: 6.0155 - val_accuracy: 0.4678\n",
            "Epoch 245/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5411 - accuracy: 0.7749 - val_loss: 4.8176 - val_accuracy: 0.4709\n",
            "Epoch 246/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5310 - accuracy: 0.7756 - val_loss: 5.3521 - val_accuracy: 0.4678\n",
            "Epoch 247/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5199 - accuracy: 0.7794 - val_loss: 5.9257 - val_accuracy: 0.4777\n",
            "Epoch 248/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5272 - accuracy: 0.7806 - val_loss: 5.4868 - val_accuracy: 0.4783\n",
            "Epoch 249/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5457 - accuracy: 0.7739 - val_loss: 6.0775 - val_accuracy: 0.4851\n",
            "Epoch 250/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5242 - accuracy: 0.7742 - val_loss: 5.3151 - val_accuracy: 0.4653\n",
            "Epoch 251/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5263 - accuracy: 0.7818 - val_loss: 5.5925 - val_accuracy: 0.4802\n",
            "Epoch 252/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7780 - val_loss: 5.7828 - val_accuracy: 0.4857\n",
            "Epoch 253/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5396 - accuracy: 0.7730 - val_loss: 5.9290 - val_accuracy: 0.4907\n",
            "Epoch 254/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5388 - accuracy: 0.7739 - val_loss: 5.5039 - val_accuracy: 0.4758\n",
            "Epoch 255/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5224 - accuracy: 0.7755 - val_loss: 5.6520 - val_accuracy: 0.4740\n",
            "Epoch 256/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5220 - accuracy: 0.7798 - val_loss: 5.7123 - val_accuracy: 0.4734\n",
            "Epoch 257/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5285 - accuracy: 0.7756 - val_loss: 5.4405 - val_accuracy: 0.4771\n",
            "Epoch 258/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7815 - val_loss: 6.1938 - val_accuracy: 0.4647\n",
            "Epoch 259/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5157 - accuracy: 0.7750 - val_loss: 5.3308 - val_accuracy: 0.4690\n",
            "Epoch 260/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5074 - accuracy: 0.7801 - val_loss: 6.4338 - val_accuracy: 0.4789\n",
            "Epoch 261/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7826 - val_loss: 6.2934 - val_accuracy: 0.4864\n",
            "Epoch 262/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7797 - val_loss: 6.4452 - val_accuracy: 0.5043\n",
            "Epoch 263/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7787 - val_loss: 5.5208 - val_accuracy: 0.4758\n",
            "Epoch 264/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5003 - accuracy: 0.7876 - val_loss: 6.2972 - val_accuracy: 0.4690\n",
            "Epoch 265/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7860 - val_loss: 5.8778 - val_accuracy: 0.4628\n",
            "Epoch 266/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7797 - val_loss: 6.5372 - val_accuracy: 0.4771\n",
            "Epoch 267/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5325 - accuracy: 0.7789 - val_loss: 5.9880 - val_accuracy: 0.4808\n",
            "Epoch 268/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7835 - val_loss: 5.7876 - val_accuracy: 0.4752\n",
            "Epoch 269/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5044 - accuracy: 0.7835 - val_loss: 6.9556 - val_accuracy: 0.4975\n",
            "Epoch 270/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5052 - accuracy: 0.7822 - val_loss: 6.6849 - val_accuracy: 0.4833\n",
            "Epoch 271/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5130 - accuracy: 0.7871 - val_loss: 6.4985 - val_accuracy: 0.4857\n",
            "Epoch 272/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5329 - accuracy: 0.7818 - val_loss: 5.7686 - val_accuracy: 0.4765\n",
            "Epoch 273/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5048 - accuracy: 0.7811 - val_loss: 5.7490 - val_accuracy: 0.4771\n",
            "Epoch 274/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5073 - accuracy: 0.7831 - val_loss: 6.1846 - val_accuracy: 0.4851\n",
            "Epoch 275/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5041 - accuracy: 0.7839 - val_loss: 6.3930 - val_accuracy: 0.4888\n",
            "Epoch 276/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5048 - accuracy: 0.7848 - val_loss: 6.7408 - val_accuracy: 0.4765\n",
            "Epoch 277/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5083 - accuracy: 0.7848 - val_loss: 5.9303 - val_accuracy: 0.4765\n",
            "Epoch 278/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7866 - val_loss: 6.4713 - val_accuracy: 0.4919\n",
            "Epoch 279/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5023 - accuracy: 0.7851 - val_loss: 6.4087 - val_accuracy: 0.4727\n",
            "Epoch 280/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4974 - accuracy: 0.7826 - val_loss: 6.4820 - val_accuracy: 0.4820\n",
            "Epoch 281/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5033 - accuracy: 0.7834 - val_loss: 6.7705 - val_accuracy: 0.4734\n",
            "Epoch 282/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4970 - accuracy: 0.7897 - val_loss: 6.7469 - val_accuracy: 0.4777\n",
            "Epoch 283/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5064 - accuracy: 0.7911 - val_loss: 6.8370 - val_accuracy: 0.4833\n",
            "Epoch 284/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5065 - accuracy: 0.7831 - val_loss: 6.0856 - val_accuracy: 0.4777\n",
            "Epoch 285/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5120 - accuracy: 0.7839 - val_loss: 6.5625 - val_accuracy: 0.4765\n",
            "Epoch 286/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5106 - accuracy: 0.7820 - val_loss: 6.2019 - val_accuracy: 0.4845\n",
            "Epoch 287/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5237 - accuracy: 0.7828 - val_loss: 6.6588 - val_accuracy: 0.4814\n",
            "Epoch 288/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5071 - accuracy: 0.7888 - val_loss: 6.9096 - val_accuracy: 0.4876\n",
            "Epoch 289/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5144 - accuracy: 0.7859 - val_loss: 6.9236 - val_accuracy: 0.4820\n",
            "Epoch 290/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5074 - accuracy: 0.7888 - val_loss: 6.1074 - val_accuracy: 0.4845\n",
            "Epoch 291/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5005 - accuracy: 0.7922 - val_loss: 6.5616 - val_accuracy: 0.4845\n",
            "Epoch 292/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5011 - accuracy: 0.7876 - val_loss: 6.4376 - val_accuracy: 0.4857\n",
            "Epoch 293/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5087 - accuracy: 0.7918 - val_loss: 7.3085 - val_accuracy: 0.4888\n",
            "Epoch 294/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.5009 - accuracy: 0.7894 - val_loss: 7.3485 - val_accuracy: 0.4857\n",
            "Epoch 295/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4890 - accuracy: 0.7932 - val_loss: 6.9782 - val_accuracy: 0.4758\n",
            "Epoch 296/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4946 - accuracy: 0.7933 - val_loss: 6.5490 - val_accuracy: 0.4820\n",
            "Epoch 297/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5055 - accuracy: 0.7927 - val_loss: 6.7126 - val_accuracy: 0.4802\n",
            "Epoch 298/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7874 - val_loss: 6.2722 - val_accuracy: 0.4789\n",
            "Epoch 299/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4935 - accuracy: 0.7880 - val_loss: 6.1929 - val_accuracy: 0.4529\n",
            "Epoch 300/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5036 - accuracy: 0.7932 - val_loss: 7.2174 - val_accuracy: 0.4802\n",
            "Epoch 301/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7891 - val_loss: 6.3750 - val_accuracy: 0.4765\n",
            "Epoch 302/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4976 - accuracy: 0.7933 - val_loss: 7.0010 - val_accuracy: 0.4610\n",
            "Epoch 303/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4893 - accuracy: 0.7914 - val_loss: 6.8129 - val_accuracy: 0.4907\n",
            "Epoch 304/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4894 - accuracy: 0.7956 - val_loss: 7.1234 - val_accuracy: 0.4721\n",
            "Epoch 305/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4962 - accuracy: 0.7935 - val_loss: 6.9284 - val_accuracy: 0.4895\n",
            "Epoch 306/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5200 - accuracy: 0.7924 - val_loss: 6.7222 - val_accuracy: 0.4721\n",
            "Epoch 307/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4907 - accuracy: 0.7972 - val_loss: 6.7649 - val_accuracy: 0.4777\n",
            "Epoch 308/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4750 - accuracy: 0.7950 - val_loss: 6.7161 - val_accuracy: 0.4857\n",
            "Epoch 309/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4895 - accuracy: 0.7955 - val_loss: 6.6055 - val_accuracy: 0.4734\n",
            "Epoch 310/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4752 - accuracy: 0.7949 - val_loss: 7.0299 - val_accuracy: 0.4566\n",
            "Epoch 311/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4949 - accuracy: 0.7933 - val_loss: 6.3396 - val_accuracy: 0.4548\n",
            "Epoch 312/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4928 - accuracy: 0.7927 - val_loss: 6.6512 - val_accuracy: 0.4659\n",
            "Epoch 313/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4881 - accuracy: 0.7950 - val_loss: 7.4581 - val_accuracy: 0.4734\n",
            "Epoch 314/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4793 - accuracy: 0.7966 - val_loss: 7.1271 - val_accuracy: 0.4845\n",
            "Epoch 315/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4931 - accuracy: 0.7894 - val_loss: 7.0917 - val_accuracy: 0.4783\n",
            "Epoch 316/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5234 - accuracy: 0.7956 - val_loss: 7.2103 - val_accuracy: 0.4665\n",
            "Epoch 317/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4917 - accuracy: 0.7939 - val_loss: 6.7820 - val_accuracy: 0.4783\n",
            "Epoch 318/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7966 - val_loss: 7.4114 - val_accuracy: 0.4721\n",
            "Epoch 319/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4895 - accuracy: 0.7945 - val_loss: 7.3720 - val_accuracy: 0.4876\n",
            "Epoch 320/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4796 - accuracy: 0.7930 - val_loss: 7.2896 - val_accuracy: 0.4839\n",
            "Epoch 321/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4789 - accuracy: 0.7922 - val_loss: 6.9323 - val_accuracy: 0.4857\n",
            "Epoch 322/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4799 - accuracy: 0.7945 - val_loss: 6.7586 - val_accuracy: 0.4746\n",
            "Epoch 323/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4766 - accuracy: 0.7966 - val_loss: 6.9450 - val_accuracy: 0.4802\n",
            "Epoch 324/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4816 - accuracy: 0.8040 - val_loss: 7.6885 - val_accuracy: 0.4715\n",
            "Epoch 325/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4821 - accuracy: 0.7964 - val_loss: 7.4141 - val_accuracy: 0.4814\n",
            "Epoch 326/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4705 - accuracy: 0.7958 - val_loss: 7.9692 - val_accuracy: 0.4864\n",
            "Epoch 327/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4758 - accuracy: 0.7978 - val_loss: 7.2739 - val_accuracy: 0.4678\n",
            "Epoch 328/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4797 - accuracy: 0.7956 - val_loss: 8.0211 - val_accuracy: 0.4926\n",
            "Epoch 329/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4809 - accuracy: 0.7992 - val_loss: 7.2970 - val_accuracy: 0.4610\n",
            "Epoch 330/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4883 - accuracy: 0.7913 - val_loss: 7.9421 - val_accuracy: 0.4802\n",
            "Epoch 331/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4782 - accuracy: 0.8037 - val_loss: 7.0829 - val_accuracy: 0.4734\n",
            "Epoch 332/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4814 - accuracy: 0.7970 - val_loss: 7.0081 - val_accuracy: 0.4672\n",
            "Epoch 333/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4873 - accuracy: 0.7976 - val_loss: 7.9437 - val_accuracy: 0.4715\n",
            "Epoch 334/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4948 - accuracy: 0.7944 - val_loss: 7.1916 - val_accuracy: 0.4752\n",
            "Epoch 335/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4779 - accuracy: 0.8014 - val_loss: 7.5337 - val_accuracy: 0.4591\n",
            "Epoch 336/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4629 - accuracy: 0.8055 - val_loss: 8.3530 - val_accuracy: 0.4721\n",
            "Epoch 337/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4774 - accuracy: 0.7995 - val_loss: 7.8310 - val_accuracy: 0.4789\n",
            "Epoch 338/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4876 - accuracy: 0.7933 - val_loss: 7.1530 - val_accuracy: 0.4721\n",
            "Epoch 339/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4731 - accuracy: 0.8001 - val_loss: 7.3462 - val_accuracy: 0.4647\n",
            "Epoch 340/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4653 - accuracy: 0.8046 - val_loss: 7.4890 - val_accuracy: 0.4672\n",
            "Epoch 341/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4700 - accuracy: 0.8068 - val_loss: 7.8226 - val_accuracy: 0.4628\n",
            "Epoch 342/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4808 - accuracy: 0.7978 - val_loss: 7.1733 - val_accuracy: 0.4851\n",
            "Epoch 343/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4800 - accuracy: 0.7990 - val_loss: 7.6669 - val_accuracy: 0.4820\n",
            "Epoch 344/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4687 - accuracy: 0.8043 - val_loss: 7.8533 - val_accuracy: 0.4647\n",
            "Epoch 345/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4591 - accuracy: 0.8018 - val_loss: 7.0694 - val_accuracy: 0.4467\n",
            "Epoch 346/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4581 - accuracy: 0.8062 - val_loss: 7.3163 - val_accuracy: 0.4566\n",
            "Epoch 347/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4642 - accuracy: 0.8073 - val_loss: 8.1130 - val_accuracy: 0.4758\n",
            "Epoch 348/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4745 - accuracy: 0.8034 - val_loss: 7.6044 - val_accuracy: 0.4647\n",
            "Epoch 349/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4699 - accuracy: 0.8006 - val_loss: 8.0408 - val_accuracy: 0.4678\n",
            "Epoch 350/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4615 - accuracy: 0.8011 - val_loss: 9.0463 - val_accuracy: 0.4603\n",
            "Epoch 351/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4816 - accuracy: 0.8035 - val_loss: 8.3782 - val_accuracy: 0.4746\n",
            "Epoch 352/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4761 - accuracy: 0.8015 - val_loss: 9.1198 - val_accuracy: 0.4696\n",
            "Epoch 353/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4648 - accuracy: 0.8091 - val_loss: 7.6388 - val_accuracy: 0.4647\n",
            "Epoch 354/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4680 - accuracy: 0.8054 - val_loss: 8.4424 - val_accuracy: 0.4703\n",
            "Epoch 355/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4863 - accuracy: 0.7930 - val_loss: 8.4412 - val_accuracy: 0.4839\n",
            "Epoch 356/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4599 - accuracy: 0.8038 - val_loss: 7.6136 - val_accuracy: 0.4808\n",
            "Epoch 357/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4811 - accuracy: 0.8031 - val_loss: 7.5508 - val_accuracy: 0.4734\n",
            "Epoch 358/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4755 - accuracy: 0.8018 - val_loss: 7.6268 - val_accuracy: 0.4591\n",
            "Epoch 359/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4632 - accuracy: 0.8090 - val_loss: 8.0412 - val_accuracy: 0.4771\n",
            "Epoch 360/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4566 - accuracy: 0.8054 - val_loss: 7.8848 - val_accuracy: 0.4802\n",
            "Epoch 361/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4904 - accuracy: 0.8059 - val_loss: 8.0807 - val_accuracy: 0.4727\n",
            "Epoch 362/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4686 - accuracy: 0.8063 - val_loss: 8.0949 - val_accuracy: 0.4603\n",
            "Epoch 363/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4790 - accuracy: 0.8029 - val_loss: 8.3715 - val_accuracy: 0.4659\n",
            "Epoch 364/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4791 - accuracy: 0.8051 - val_loss: 8.2474 - val_accuracy: 0.4808\n",
            "Epoch 365/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4693 - accuracy: 0.7995 - val_loss: 7.5293 - val_accuracy: 0.4734\n",
            "Epoch 366/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4810 - accuracy: 0.8020 - val_loss: 8.4085 - val_accuracy: 0.4734\n",
            "Epoch 367/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4882 - accuracy: 0.7993 - val_loss: 7.9033 - val_accuracy: 0.4585\n",
            "Epoch 368/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4580 - accuracy: 0.8031 - val_loss: 8.8597 - val_accuracy: 0.4734\n",
            "Epoch 369/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4585 - accuracy: 0.8045 - val_loss: 8.1179 - val_accuracy: 0.4659\n",
            "Epoch 370/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4696 - accuracy: 0.8040 - val_loss: 8.2539 - val_accuracy: 0.4678\n",
            "Epoch 371/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4833 - accuracy: 0.8035 - val_loss: 7.6385 - val_accuracy: 0.4802\n",
            "Epoch 372/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4809 - accuracy: 0.8021 - val_loss: 8.2884 - val_accuracy: 0.4944\n",
            "Epoch 373/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4620 - accuracy: 0.8035 - val_loss: 7.1773 - val_accuracy: 0.4752\n",
            "Epoch 374/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4640 - accuracy: 0.8052 - val_loss: 8.0299 - val_accuracy: 0.4641\n",
            "Epoch 375/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4595 - accuracy: 0.8017 - val_loss: 8.3423 - val_accuracy: 0.4678\n",
            "Epoch 376/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4612 - accuracy: 0.8043 - val_loss: 7.4945 - val_accuracy: 0.4653\n",
            "Epoch 377/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4556 - accuracy: 0.8091 - val_loss: 8.3301 - val_accuracy: 0.4560\n",
            "Epoch 378/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4545 - accuracy: 0.8093 - val_loss: 8.2804 - val_accuracy: 0.4690\n",
            "Epoch 379/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4633 - accuracy: 0.8091 - val_loss: 7.5125 - val_accuracy: 0.4684\n",
            "Epoch 380/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4742 - accuracy: 0.8018 - val_loss: 8.3517 - val_accuracy: 0.4771\n",
            "Epoch 381/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4533 - accuracy: 0.8065 - val_loss: 9.6453 - val_accuracy: 0.4777\n",
            "Epoch 382/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4660 - accuracy: 0.8093 - val_loss: 8.7700 - val_accuracy: 0.4603\n",
            "Epoch 383/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4770 - accuracy: 0.8063 - val_loss: 8.7293 - val_accuracy: 0.4827\n",
            "Epoch 384/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4722 - accuracy: 0.8060 - val_loss: 8.3369 - val_accuracy: 0.4641\n",
            "Epoch 385/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4439 - accuracy: 0.8134 - val_loss: 9.0013 - val_accuracy: 0.4839\n",
            "Epoch 386/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4820 - accuracy: 0.8066 - val_loss: 7.9614 - val_accuracy: 0.4529\n",
            "Epoch 387/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4653 - accuracy: 0.8071 - val_loss: 8.0897 - val_accuracy: 0.4628\n",
            "Epoch 388/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4893 - accuracy: 0.8082 - val_loss: 8.2517 - val_accuracy: 0.4678\n",
            "Epoch 389/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4565 - accuracy: 0.8014 - val_loss: 7.9646 - val_accuracy: 0.4727\n",
            "Epoch 390/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4530 - accuracy: 0.8158 - val_loss: 8.4589 - val_accuracy: 0.4641\n",
            "Epoch 391/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4745 - accuracy: 0.8131 - val_loss: 8.4691 - val_accuracy: 0.4882\n",
            "Epoch 392/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4584 - accuracy: 0.8038 - val_loss: 8.1572 - val_accuracy: 0.4603\n",
            "Epoch 393/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4612 - accuracy: 0.8088 - val_loss: 8.4707 - val_accuracy: 0.4684\n",
            "Epoch 394/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4916 - accuracy: 0.8029 - val_loss: 8.9608 - val_accuracy: 0.4709\n",
            "Epoch 395/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4684 - accuracy: 0.8042 - val_loss: 8.8942 - val_accuracy: 0.4758\n",
            "Epoch 396/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4513 - accuracy: 0.8100 - val_loss: 8.6731 - val_accuracy: 0.4721\n",
            "Epoch 397/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4519 - accuracy: 0.8073 - val_loss: 7.9269 - val_accuracy: 0.4622\n",
            "Epoch 398/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4619 - accuracy: 0.8065 - val_loss: 7.9804 - val_accuracy: 0.4827\n",
            "Epoch 399/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4653 - accuracy: 0.8038 - val_loss: 8.7553 - val_accuracy: 0.4703\n",
            "Epoch 400/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4696 - accuracy: 0.8125 - val_loss: 7.9870 - val_accuracy: 0.4647\n",
            "Epoch 401/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4638 - accuracy: 0.8057 - val_loss: 8.4148 - val_accuracy: 0.4777\n",
            "Epoch 402/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4683 - accuracy: 0.8128 - val_loss: 8.6979 - val_accuracy: 0.4560\n",
            "Epoch 403/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4451 - accuracy: 0.8097 - val_loss: 7.8499 - val_accuracy: 0.4603\n",
            "Epoch 404/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4639 - accuracy: 0.8034 - val_loss: 8.4957 - val_accuracy: 0.4771\n",
            "Epoch 405/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4658 - accuracy: 0.8119 - val_loss: 9.8150 - val_accuracy: 0.4901\n",
            "Epoch 406/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4526 - accuracy: 0.8091 - val_loss: 8.3783 - val_accuracy: 0.4827\n",
            "Epoch 407/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4411 - accuracy: 0.8128 - val_loss: 8.7402 - val_accuracy: 0.4802\n",
            "Epoch 408/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4508 - accuracy: 0.8094 - val_loss: 7.8975 - val_accuracy: 0.4628\n",
            "Epoch 409/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4689 - accuracy: 0.8077 - val_loss: 8.6262 - val_accuracy: 0.4864\n",
            "Epoch 410/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4504 - accuracy: 0.8145 - val_loss: 9.1212 - val_accuracy: 0.4802\n",
            "Epoch 411/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4642 - accuracy: 0.8068 - val_loss: 8.1606 - val_accuracy: 0.4758\n",
            "Epoch 412/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4728 - accuracy: 0.8066 - val_loss: 8.4493 - val_accuracy: 0.4616\n",
            "Epoch 413/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4616 - accuracy: 0.8038 - val_loss: 8.6766 - val_accuracy: 0.4603\n",
            "Epoch 414/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4528 - accuracy: 0.8094 - val_loss: 8.9871 - val_accuracy: 0.4709\n",
            "Epoch 415/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4619 - accuracy: 0.8035 - val_loss: 8.6920 - val_accuracy: 0.4752\n",
            "Epoch 416/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4621 - accuracy: 0.8113 - val_loss: 9.4044 - val_accuracy: 0.4876\n",
            "Epoch 417/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4727 - accuracy: 0.8071 - val_loss: 8.1981 - val_accuracy: 0.4802\n",
            "Epoch 418/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4463 - accuracy: 0.8119 - val_loss: 8.9313 - val_accuracy: 0.4665\n",
            "Epoch 419/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4554 - accuracy: 0.8127 - val_loss: 8.7816 - val_accuracy: 0.4715\n",
            "Epoch 420/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4722 - accuracy: 0.8083 - val_loss: 8.4770 - val_accuracy: 0.4610\n",
            "Epoch 421/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4676 - accuracy: 0.8026 - val_loss: 7.8971 - val_accuracy: 0.4554\n",
            "Epoch 422/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4546 - accuracy: 0.8066 - val_loss: 8.2497 - val_accuracy: 0.4523\n",
            "Epoch 423/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4577 - accuracy: 0.8049 - val_loss: 9.6271 - val_accuracy: 0.4486\n",
            "Epoch 424/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4479 - accuracy: 0.8085 - val_loss: 8.9803 - val_accuracy: 0.4653\n",
            "Epoch 425/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4627 - accuracy: 0.8104 - val_loss: 8.4190 - val_accuracy: 0.4597\n",
            "Epoch 426/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4606 - accuracy: 0.8172 - val_loss: 8.4732 - val_accuracy: 0.4752\n",
            "Epoch 427/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4465 - accuracy: 0.8125 - val_loss: 9.7028 - val_accuracy: 0.4498\n",
            "Epoch 428/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4412 - accuracy: 0.8125 - val_loss: 9.6526 - val_accuracy: 0.4622\n",
            "Epoch 429/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4582 - accuracy: 0.8138 - val_loss: 8.8691 - val_accuracy: 0.4765\n",
            "Epoch 430/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4665 - accuracy: 0.8057 - val_loss: 8.9591 - val_accuracy: 0.4845\n",
            "Epoch 431/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4588 - accuracy: 0.8117 - val_loss: 8.9373 - val_accuracy: 0.4535\n",
            "Epoch 432/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4711 - accuracy: 0.8083 - val_loss: 10.5044 - val_accuracy: 0.4665\n",
            "Epoch 433/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4669 - accuracy: 0.8116 - val_loss: 8.4444 - val_accuracy: 0.4783\n",
            "Epoch 434/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4600 - accuracy: 0.8091 - val_loss: 9.7145 - val_accuracy: 0.4653\n",
            "Epoch 435/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4631 - accuracy: 0.8060 - val_loss: 10.3194 - val_accuracy: 0.4888\n",
            "Epoch 436/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4464 - accuracy: 0.8139 - val_loss: 9.3628 - val_accuracy: 0.4758\n",
            "Epoch 437/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4648 - accuracy: 0.8083 - val_loss: 9.3448 - val_accuracy: 0.4783\n",
            "Epoch 438/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4415 - accuracy: 0.8136 - val_loss: 8.5288 - val_accuracy: 0.4740\n",
            "Epoch 439/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4309 - accuracy: 0.8105 - val_loss: 9.3222 - val_accuracy: 0.4820\n",
            "Epoch 440/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4462 - accuracy: 0.8119 - val_loss: 9.9449 - val_accuracy: 0.4796\n",
            "Epoch 441/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4578 - accuracy: 0.8122 - val_loss: 10.5112 - val_accuracy: 0.4796\n",
            "Epoch 442/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4791 - accuracy: 0.8131 - val_loss: 8.8692 - val_accuracy: 0.4653\n",
            "Epoch 443/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4304 - accuracy: 0.8172 - val_loss: 10.6093 - val_accuracy: 0.4572\n",
            "Epoch 444/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4518 - accuracy: 0.8156 - val_loss: 9.3639 - val_accuracy: 0.4647\n",
            "Epoch 445/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4427 - accuracy: 0.8172 - val_loss: 8.7580 - val_accuracy: 0.4727\n",
            "Epoch 446/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4259 - accuracy: 0.8152 - val_loss: 9.3029 - val_accuracy: 0.4566\n",
            "Epoch 447/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4431 - accuracy: 0.8204 - val_loss: 9.5303 - val_accuracy: 0.4603\n",
            "Epoch 448/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4702 - accuracy: 0.8153 - val_loss: 9.3262 - val_accuracy: 0.4684\n",
            "Epoch 449/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4405 - accuracy: 0.8152 - val_loss: 9.3625 - val_accuracy: 0.4771\n",
            "Epoch 450/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4518 - accuracy: 0.8156 - val_loss: 9.5043 - val_accuracy: 0.4603\n",
            "Epoch 451/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4666 - accuracy: 0.8085 - val_loss: 9.3588 - val_accuracy: 0.4653\n",
            "Epoch 452/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4460 - accuracy: 0.8055 - val_loss: 9.4766 - val_accuracy: 0.4758\n",
            "Epoch 453/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4273 - accuracy: 0.8136 - val_loss: 9.6031 - val_accuracy: 0.4851\n",
            "Epoch 454/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4316 - accuracy: 0.8117 - val_loss: 9.8484 - val_accuracy: 0.4647\n",
            "Epoch 455/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4384 - accuracy: 0.8136 - val_loss: 9.7199 - val_accuracy: 0.4703\n",
            "Epoch 456/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4545 - accuracy: 0.8136 - val_loss: 8.8934 - val_accuracy: 0.4653\n",
            "Epoch 457/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4621 - accuracy: 0.8127 - val_loss: 10.7456 - val_accuracy: 0.4864\n",
            "Epoch 458/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4499 - accuracy: 0.8179 - val_loss: 9.3666 - val_accuracy: 0.4684\n",
            "Epoch 459/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4604 - accuracy: 0.8165 - val_loss: 9.6263 - val_accuracy: 0.4814\n",
            "Epoch 460/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4521 - accuracy: 0.8147 - val_loss: 10.2731 - val_accuracy: 0.4777\n",
            "Epoch 461/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4605 - accuracy: 0.8183 - val_loss: 9.5928 - val_accuracy: 0.4690\n",
            "Epoch 462/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4561 - accuracy: 0.8200 - val_loss: 10.0173 - val_accuracy: 0.4808\n",
            "Epoch 463/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4407 - accuracy: 0.8114 - val_loss: 9.9407 - val_accuracy: 0.4529\n",
            "Epoch 464/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4352 - accuracy: 0.8237 - val_loss: 9.6066 - val_accuracy: 0.4548\n",
            "Epoch 465/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4463 - accuracy: 0.8164 - val_loss: 9.9753 - val_accuracy: 0.4641\n",
            "Epoch 466/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4741 - accuracy: 0.8134 - val_loss: 9.7829 - val_accuracy: 0.4579\n",
            "Epoch 467/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4419 - accuracy: 0.8141 - val_loss: 9.7681 - val_accuracy: 0.4727\n",
            "Epoch 468/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4577 - accuracy: 0.8201 - val_loss: 10.6178 - val_accuracy: 0.4628\n",
            "Epoch 469/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4487 - accuracy: 0.8127 - val_loss: 10.0050 - val_accuracy: 0.4796\n",
            "Epoch 470/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4576 - accuracy: 0.8134 - val_loss: 9.6561 - val_accuracy: 0.4777\n",
            "Epoch 471/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4552 - accuracy: 0.8124 - val_loss: 9.3616 - val_accuracy: 0.4703\n",
            "Epoch 472/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4346 - accuracy: 0.8178 - val_loss: 8.8433 - val_accuracy: 0.4734\n",
            "Epoch 473/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4532 - accuracy: 0.8152 - val_loss: 9.1908 - val_accuracy: 0.4727\n",
            "Epoch 474/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4313 - accuracy: 0.8155 - val_loss: 10.0046 - val_accuracy: 0.4628\n",
            "Epoch 475/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4467 - accuracy: 0.8127 - val_loss: 9.6954 - val_accuracy: 0.4740\n",
            "Epoch 476/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4264 - accuracy: 0.8162 - val_loss: 9.4351 - val_accuracy: 0.4486\n",
            "Epoch 477/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4306 - accuracy: 0.8193 - val_loss: 9.4342 - val_accuracy: 0.4752\n",
            "Epoch 478/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4601 - accuracy: 0.8086 - val_loss: 10.4562 - val_accuracy: 0.4610\n",
            "Epoch 479/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4404 - accuracy: 0.8183 - val_loss: 9.2982 - val_accuracy: 0.4591\n",
            "Epoch 480/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4371 - accuracy: 0.8173 - val_loss: 11.4068 - val_accuracy: 0.4535\n",
            "Epoch 481/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4323 - accuracy: 0.8178 - val_loss: 10.0461 - val_accuracy: 0.4591\n",
            "Epoch 482/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4615 - accuracy: 0.8153 - val_loss: 11.0847 - val_accuracy: 0.4827\n",
            "Epoch 483/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4527 - accuracy: 0.8155 - val_loss: 10.5901 - val_accuracy: 0.4709\n",
            "Epoch 484/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4673 - accuracy: 0.8131 - val_loss: 10.4457 - val_accuracy: 0.4777\n",
            "Epoch 485/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4623 - accuracy: 0.8076 - val_loss: 10.3345 - val_accuracy: 0.4542\n",
            "Epoch 486/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4667 - accuracy: 0.8186 - val_loss: 9.7154 - val_accuracy: 0.4715\n",
            "Epoch 487/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4219 - accuracy: 0.8187 - val_loss: 11.3486 - val_accuracy: 0.4616\n",
            "Epoch 488/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4536 - accuracy: 0.8190 - val_loss: 10.3182 - val_accuracy: 0.4845\n",
            "Epoch 489/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4312 - accuracy: 0.8215 - val_loss: 9.6411 - val_accuracy: 0.4554\n",
            "Epoch 490/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4442 - accuracy: 0.8187 - val_loss: 10.7550 - val_accuracy: 0.4647\n",
            "Epoch 491/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4633 - accuracy: 0.8162 - val_loss: 9.9297 - val_accuracy: 0.4529\n",
            "Epoch 492/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4432 - accuracy: 0.8186 - val_loss: 10.6899 - val_accuracy: 0.4789\n",
            "Epoch 493/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4438 - accuracy: 0.8186 - val_loss: 10.4249 - val_accuracy: 0.4758\n",
            "Epoch 494/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4399 - accuracy: 0.8121 - val_loss: 11.3934 - val_accuracy: 0.4424\n",
            "Epoch 495/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4601 - accuracy: 0.8186 - val_loss: 10.3342 - val_accuracy: 0.4641\n",
            "Epoch 496/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4390 - accuracy: 0.8201 - val_loss: 10.5311 - val_accuracy: 0.4721\n",
            "Epoch 497/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4254 - accuracy: 0.8217 - val_loss: 11.7666 - val_accuracy: 0.4777\n",
            "Epoch 498/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4226 - accuracy: 0.8176 - val_loss: 10.9905 - val_accuracy: 0.4678\n",
            "Epoch 499/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4442 - accuracy: 0.8158 - val_loss: 10.8079 - val_accuracy: 0.4461\n",
            "Epoch 500/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4414 - accuracy: 0.8175 - val_loss: 10.0580 - val_accuracy: 0.4616\n",
            "Epoch 501/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4463 - accuracy: 0.8127 - val_loss: 10.0427 - val_accuracy: 0.4641\n",
            "Epoch 502/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4208 - accuracy: 0.8206 - val_loss: 11.3819 - val_accuracy: 0.4727\n",
            "Epoch 503/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4288 - accuracy: 0.8162 - val_loss: 11.0436 - val_accuracy: 0.4672\n",
            "Epoch 504/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4469 - accuracy: 0.8183 - val_loss: 10.2073 - val_accuracy: 0.4690\n",
            "Epoch 505/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4535 - accuracy: 0.8179 - val_loss: 9.9457 - val_accuracy: 0.4566\n",
            "Epoch 506/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4296 - accuracy: 0.8189 - val_loss: 10.2148 - val_accuracy: 0.4696\n",
            "Epoch 507/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4379 - accuracy: 0.8173 - val_loss: 10.7263 - val_accuracy: 0.4740\n",
            "Epoch 508/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4291 - accuracy: 0.8248 - val_loss: 10.2053 - val_accuracy: 0.4634\n",
            "Epoch 509/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4497 - accuracy: 0.8246 - val_loss: 11.1321 - val_accuracy: 0.4641\n",
            "Epoch 510/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4260 - accuracy: 0.8215 - val_loss: 10.7920 - val_accuracy: 0.4653\n",
            "Epoch 511/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4533 - accuracy: 0.8252 - val_loss: 10.5877 - val_accuracy: 0.4746\n",
            "Epoch 512/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4402 - accuracy: 0.8215 - val_loss: 10.5204 - val_accuracy: 0.4659\n",
            "Epoch 513/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4339 - accuracy: 0.8186 - val_loss: 10.2573 - val_accuracy: 0.4665\n",
            "Epoch 514/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4677 - accuracy: 0.8153 - val_loss: 11.5038 - val_accuracy: 0.4678\n",
            "Epoch 515/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4367 - accuracy: 0.8196 - val_loss: 11.1296 - val_accuracy: 0.4703\n",
            "Epoch 516/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4683 - accuracy: 0.8153 - val_loss: 10.9031 - val_accuracy: 0.4777\n",
            "Epoch 517/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4151 - accuracy: 0.8238 - val_loss: 10.9213 - val_accuracy: 0.4542\n",
            "Epoch 518/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4340 - accuracy: 0.8200 - val_loss: 9.4983 - val_accuracy: 0.4659\n",
            "Epoch 519/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4536 - accuracy: 0.8195 - val_loss: 10.9139 - val_accuracy: 0.4659\n",
            "Epoch 520/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4366 - accuracy: 0.8238 - val_loss: 10.5543 - val_accuracy: 0.4709\n",
            "Epoch 521/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4365 - accuracy: 0.8170 - val_loss: 11.2430 - val_accuracy: 0.4690\n",
            "Epoch 522/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4533 - accuracy: 0.8183 - val_loss: 11.4466 - val_accuracy: 0.4585\n",
            "Epoch 523/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4306 - accuracy: 0.8204 - val_loss: 11.1633 - val_accuracy: 0.4672\n",
            "Epoch 524/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4154 - accuracy: 0.8262 - val_loss: 10.5786 - val_accuracy: 0.4783\n",
            "Epoch 525/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4214 - accuracy: 0.8255 - val_loss: 10.6671 - val_accuracy: 0.4752\n",
            "Epoch 526/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4319 - accuracy: 0.8189 - val_loss: 10.5537 - val_accuracy: 0.4746\n",
            "Epoch 527/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4125 - accuracy: 0.8251 - val_loss: 11.2452 - val_accuracy: 0.4696\n",
            "Epoch 528/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4289 - accuracy: 0.8203 - val_loss: 11.3000 - val_accuracy: 0.4715\n",
            "Epoch 529/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4499 - accuracy: 0.8214 - val_loss: 10.9872 - val_accuracy: 0.4715\n",
            "Epoch 530/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4423 - accuracy: 0.8214 - val_loss: 11.5611 - val_accuracy: 0.4690\n",
            "Epoch 531/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4318 - accuracy: 0.8232 - val_loss: 10.7973 - val_accuracy: 0.4746\n",
            "Epoch 532/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4563 - accuracy: 0.8187 - val_loss: 11.0250 - val_accuracy: 0.4672\n",
            "Epoch 533/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4415 - accuracy: 0.8217 - val_loss: 11.9422 - val_accuracy: 0.4752\n",
            "Epoch 534/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4568 - accuracy: 0.8162 - val_loss: 10.8283 - val_accuracy: 0.4727\n",
            "Epoch 535/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4341 - accuracy: 0.8241 - val_loss: 10.4999 - val_accuracy: 0.4665\n",
            "Epoch 536/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4327 - accuracy: 0.8155 - val_loss: 11.6519 - val_accuracy: 0.4777\n",
            "Epoch 537/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4293 - accuracy: 0.8235 - val_loss: 10.0640 - val_accuracy: 0.4572\n",
            "Epoch 538/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4391 - accuracy: 0.8201 - val_loss: 10.3242 - val_accuracy: 0.4628\n",
            "Epoch 539/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4185 - accuracy: 0.8241 - val_loss: 10.3683 - val_accuracy: 0.4752\n",
            "Epoch 540/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4179 - accuracy: 0.8286 - val_loss: 11.2883 - val_accuracy: 0.4827\n",
            "Epoch 541/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4690 - accuracy: 0.8220 - val_loss: 10.7924 - val_accuracy: 0.4591\n",
            "Epoch 542/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4128 - accuracy: 0.8262 - val_loss: 10.3760 - val_accuracy: 0.4473\n",
            "Epoch 543/1000\n",
            "202/202 [==============================] - 1s 5ms/step - loss: 0.4154 - accuracy: 0.8255 - val_loss: 12.1751 - val_accuracy: 0.4839\n",
            "Epoch 544/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4453 - accuracy: 0.8217 - val_loss: 12.1938 - val_accuracy: 0.4734\n",
            "Epoch 545/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4263 - accuracy: 0.8249 - val_loss: 12.6725 - val_accuracy: 0.4789\n",
            "Epoch 546/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4369 - accuracy: 0.8215 - val_loss: 10.9710 - val_accuracy: 0.4622\n",
            "Epoch 547/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4364 - accuracy: 0.8221 - val_loss: 11.2609 - val_accuracy: 0.4641\n",
            "Epoch 548/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4305 - accuracy: 0.8220 - val_loss: 10.4478 - val_accuracy: 0.4511\n",
            "Epoch 549/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4238 - accuracy: 0.8238 - val_loss: 9.9987 - val_accuracy: 0.4672\n",
            "Epoch 550/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4353 - accuracy: 0.8217 - val_loss: 10.5226 - val_accuracy: 0.4634\n",
            "Epoch 551/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4316 - accuracy: 0.8204 - val_loss: 11.0464 - val_accuracy: 0.4603\n",
            "Epoch 552/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3997 - accuracy: 0.8299 - val_loss: 12.5630 - val_accuracy: 0.4579\n",
            "Epoch 553/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4323 - accuracy: 0.8243 - val_loss: 10.9486 - val_accuracy: 0.4498\n",
            "Epoch 554/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4380 - accuracy: 0.8223 - val_loss: 11.6243 - val_accuracy: 0.4678\n",
            "Epoch 555/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4417 - accuracy: 0.8257 - val_loss: 12.5842 - val_accuracy: 0.4672\n",
            "Epoch 556/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4457 - accuracy: 0.8176 - val_loss: 10.3746 - val_accuracy: 0.4882\n",
            "Epoch 557/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4250 - accuracy: 0.8257 - val_loss: 11.2442 - val_accuracy: 0.4827\n",
            "Epoch 558/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4224 - accuracy: 0.8218 - val_loss: 11.0624 - val_accuracy: 0.4777\n",
            "Epoch 559/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4225 - accuracy: 0.8248 - val_loss: 12.1578 - val_accuracy: 0.4758\n",
            "Epoch 560/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4416 - accuracy: 0.8227 - val_loss: 11.6526 - val_accuracy: 0.4789\n",
            "Epoch 561/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4509 - accuracy: 0.8246 - val_loss: 10.6872 - val_accuracy: 0.4696\n",
            "Epoch 562/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4492 - accuracy: 0.8231 - val_loss: 10.9575 - val_accuracy: 0.4566\n",
            "Epoch 563/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4526 - accuracy: 0.8172 - val_loss: 11.3022 - val_accuracy: 0.4765\n",
            "Epoch 564/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4067 - accuracy: 0.8300 - val_loss: 12.2987 - val_accuracy: 0.4709\n",
            "Epoch 565/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4238 - accuracy: 0.8252 - val_loss: 11.5819 - val_accuracy: 0.4579\n",
            "Epoch 566/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4075 - accuracy: 0.8244 - val_loss: 12.6864 - val_accuracy: 0.4672\n",
            "Epoch 567/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4370 - accuracy: 0.8218 - val_loss: 11.3148 - val_accuracy: 0.4715\n",
            "Epoch 568/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4087 - accuracy: 0.8291 - val_loss: 10.6829 - val_accuracy: 0.4734\n",
            "Epoch 569/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4427 - accuracy: 0.8229 - val_loss: 11.3561 - val_accuracy: 0.4721\n",
            "Epoch 570/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4196 - accuracy: 0.8286 - val_loss: 11.0344 - val_accuracy: 0.4622\n",
            "Epoch 571/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4155 - accuracy: 0.8294 - val_loss: 12.0908 - val_accuracy: 0.4690\n",
            "Epoch 572/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4208 - accuracy: 0.8305 - val_loss: 11.0906 - val_accuracy: 0.4771\n",
            "Epoch 573/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4632 - accuracy: 0.8217 - val_loss: 12.4811 - val_accuracy: 0.4634\n",
            "Epoch 574/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4193 - accuracy: 0.8260 - val_loss: 10.9828 - val_accuracy: 0.4492\n",
            "Epoch 575/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4349 - accuracy: 0.8258 - val_loss: 11.2929 - val_accuracy: 0.4740\n",
            "Epoch 576/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4149 - accuracy: 0.8252 - val_loss: 11.3742 - val_accuracy: 0.4517\n",
            "Epoch 577/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4056 - accuracy: 0.8282 - val_loss: 11.2167 - val_accuracy: 0.4684\n",
            "Epoch 578/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4225 - accuracy: 0.8238 - val_loss: 10.9162 - val_accuracy: 0.4504\n",
            "Epoch 579/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4106 - accuracy: 0.8308 - val_loss: 10.5821 - val_accuracy: 0.4641\n",
            "Epoch 580/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4308 - accuracy: 0.8263 - val_loss: 11.0829 - val_accuracy: 0.4560\n",
            "Epoch 581/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4439 - accuracy: 0.8285 - val_loss: 11.7267 - val_accuracy: 0.4845\n",
            "Epoch 582/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4271 - accuracy: 0.8229 - val_loss: 11.7330 - val_accuracy: 0.4783\n",
            "Epoch 583/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4144 - accuracy: 0.8288 - val_loss: 12.5795 - val_accuracy: 0.4715\n",
            "Epoch 584/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4420 - accuracy: 0.8255 - val_loss: 11.3197 - val_accuracy: 0.4715\n",
            "Epoch 585/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4259 - accuracy: 0.8275 - val_loss: 11.9467 - val_accuracy: 0.4727\n",
            "Epoch 586/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4389 - accuracy: 0.8231 - val_loss: 12.6238 - val_accuracy: 0.4610\n",
            "Epoch 587/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4184 - accuracy: 0.8289 - val_loss: 12.8488 - val_accuracy: 0.4603\n",
            "Epoch 588/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4226 - accuracy: 0.8262 - val_loss: 11.4649 - val_accuracy: 0.4653\n",
            "Epoch 589/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4797 - accuracy: 0.8204 - val_loss: 12.0835 - val_accuracy: 0.4721\n",
            "Epoch 590/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4334 - accuracy: 0.8268 - val_loss: 10.9418 - val_accuracy: 0.4461\n",
            "Epoch 591/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4346 - accuracy: 0.8302 - val_loss: 10.9142 - val_accuracy: 0.4746\n",
            "Epoch 592/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4024 - accuracy: 0.8272 - val_loss: 11.0243 - val_accuracy: 0.4634\n",
            "Epoch 593/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4136 - accuracy: 0.8263 - val_loss: 13.0211 - val_accuracy: 0.4833\n",
            "Epoch 594/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4251 - accuracy: 0.8317 - val_loss: 12.2782 - val_accuracy: 0.4585\n",
            "Epoch 595/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4383 - accuracy: 0.8265 - val_loss: 10.7419 - val_accuracy: 0.4665\n",
            "Epoch 596/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4054 - accuracy: 0.8258 - val_loss: 11.5468 - val_accuracy: 0.4665\n",
            "Epoch 597/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4380 - accuracy: 0.8215 - val_loss: 12.6585 - val_accuracy: 0.4734\n",
            "Epoch 598/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4394 - accuracy: 0.8260 - val_loss: 11.3065 - val_accuracy: 0.4560\n",
            "Epoch 599/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4438 - accuracy: 0.8277 - val_loss: 11.9063 - val_accuracy: 0.4672\n",
            "Epoch 600/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4315 - accuracy: 0.8224 - val_loss: 12.3861 - val_accuracy: 0.4814\n",
            "Epoch 601/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4154 - accuracy: 0.8308 - val_loss: 12.4067 - val_accuracy: 0.4771\n",
            "Epoch 602/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4080 - accuracy: 0.8277 - val_loss: 13.4656 - val_accuracy: 0.4665\n",
            "Epoch 603/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4152 - accuracy: 0.8288 - val_loss: 11.8838 - val_accuracy: 0.4696\n",
            "Epoch 604/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4257 - accuracy: 0.8268 - val_loss: 13.6377 - val_accuracy: 0.4802\n",
            "Epoch 605/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4221 - accuracy: 0.8266 - val_loss: 11.8402 - val_accuracy: 0.4833\n",
            "Epoch 606/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4132 - accuracy: 0.8268 - val_loss: 12.0180 - val_accuracy: 0.4715\n",
            "Epoch 607/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4428 - accuracy: 0.8255 - val_loss: 12.1686 - val_accuracy: 0.4684\n",
            "Epoch 608/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4303 - accuracy: 0.8266 - val_loss: 11.7975 - val_accuracy: 0.4771\n",
            "Epoch 609/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4075 - accuracy: 0.8291 - val_loss: 11.9515 - val_accuracy: 0.4715\n",
            "Epoch 610/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4134 - accuracy: 0.8324 - val_loss: 12.0042 - val_accuracy: 0.4473\n",
            "Epoch 611/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4225 - accuracy: 0.8254 - val_loss: 12.5327 - val_accuracy: 0.4616\n",
            "Epoch 612/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4194 - accuracy: 0.8241 - val_loss: 12.4260 - val_accuracy: 0.4610\n",
            "Epoch 613/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4252 - accuracy: 0.8296 - val_loss: 13.3836 - val_accuracy: 0.4616\n",
            "Epoch 614/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4460 - accuracy: 0.8275 - val_loss: 13.1433 - val_accuracy: 0.4765\n",
            "Epoch 615/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4902 - accuracy: 0.8220 - val_loss: 11.8341 - val_accuracy: 0.4727\n",
            "Epoch 616/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4309 - accuracy: 0.8294 - val_loss: 12.4944 - val_accuracy: 0.4659\n",
            "Epoch 617/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4308 - accuracy: 0.8283 - val_loss: 11.8450 - val_accuracy: 0.4740\n",
            "Epoch 618/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4324 - accuracy: 0.8263 - val_loss: 12.5757 - val_accuracy: 0.4684\n",
            "Epoch 619/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4318 - accuracy: 0.8277 - val_loss: 12.5305 - val_accuracy: 0.4833\n",
            "Epoch 620/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4069 - accuracy: 0.8359 - val_loss: 12.9655 - val_accuracy: 0.4672\n",
            "Epoch 621/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4506 - accuracy: 0.8263 - val_loss: 11.2412 - val_accuracy: 0.4579\n",
            "Epoch 622/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3900 - accuracy: 0.8341 - val_loss: 11.8141 - val_accuracy: 0.4579\n",
            "Epoch 623/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4296 - accuracy: 0.8234 - val_loss: 13.2991 - val_accuracy: 0.4783\n",
            "Epoch 624/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4072 - accuracy: 0.8252 - val_loss: 12.6366 - val_accuracy: 0.4684\n",
            "Epoch 625/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3929 - accuracy: 0.8320 - val_loss: 12.7867 - val_accuracy: 0.4709\n",
            "Epoch 626/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4479 - accuracy: 0.8203 - val_loss: 12.2661 - val_accuracy: 0.4628\n",
            "Epoch 627/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4204 - accuracy: 0.8285 - val_loss: 12.1027 - val_accuracy: 0.4715\n",
            "Epoch 628/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4235 - accuracy: 0.8271 - val_loss: 12.3124 - val_accuracy: 0.4641\n",
            "Epoch 629/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4434 - accuracy: 0.8274 - val_loss: 10.6871 - val_accuracy: 0.4523\n",
            "Epoch 630/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4114 - accuracy: 0.8350 - val_loss: 12.8692 - val_accuracy: 0.4672\n",
            "Epoch 631/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4385 - accuracy: 0.8286 - val_loss: 12.6951 - val_accuracy: 0.4740\n",
            "Epoch 632/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4522 - accuracy: 0.8258 - val_loss: 12.2949 - val_accuracy: 0.4535\n",
            "Epoch 633/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4257 - accuracy: 0.8265 - val_loss: 12.5740 - val_accuracy: 0.4616\n",
            "Epoch 634/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4056 - accuracy: 0.8294 - val_loss: 11.8139 - val_accuracy: 0.4523\n",
            "Epoch 635/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4152 - accuracy: 0.8300 - val_loss: 12.0827 - val_accuracy: 0.4560\n",
            "Epoch 636/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4080 - accuracy: 0.8272 - val_loss: 13.7783 - val_accuracy: 0.4696\n",
            "Epoch 637/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3980 - accuracy: 0.8322 - val_loss: 13.1091 - val_accuracy: 0.4684\n",
            "Epoch 638/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4283 - accuracy: 0.8328 - val_loss: 12.6104 - val_accuracy: 0.4703\n",
            "Epoch 639/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4259 - accuracy: 0.8311 - val_loss: 11.5483 - val_accuracy: 0.4597\n",
            "Epoch 640/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4127 - accuracy: 0.8280 - val_loss: 11.3056 - val_accuracy: 0.4591\n",
            "Epoch 641/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4122 - accuracy: 0.8330 - val_loss: 12.5711 - val_accuracy: 0.4715\n",
            "Epoch 642/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4250 - accuracy: 0.8296 - val_loss: 12.1779 - val_accuracy: 0.4603\n",
            "Epoch 643/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4133 - accuracy: 0.8317 - val_loss: 13.1493 - val_accuracy: 0.4808\n",
            "Epoch 644/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4207 - accuracy: 0.8297 - val_loss: 12.2682 - val_accuracy: 0.4517\n",
            "Epoch 645/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4430 - accuracy: 0.8277 - val_loss: 13.4894 - val_accuracy: 0.4591\n",
            "Epoch 646/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4472 - accuracy: 0.8271 - val_loss: 12.3756 - val_accuracy: 0.4579\n",
            "Epoch 647/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4259 - accuracy: 0.8302 - val_loss: 12.1739 - val_accuracy: 0.4653\n",
            "Epoch 648/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4145 - accuracy: 0.8265 - val_loss: 11.6241 - val_accuracy: 0.4727\n",
            "Epoch 649/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4109 - accuracy: 0.8317 - val_loss: 12.8557 - val_accuracy: 0.4591\n",
            "Epoch 650/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3960 - accuracy: 0.8317 - val_loss: 12.2200 - val_accuracy: 0.4715\n",
            "Epoch 651/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4249 - accuracy: 0.8291 - val_loss: 11.9170 - val_accuracy: 0.4740\n",
            "Epoch 652/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4208 - accuracy: 0.8297 - val_loss: 13.5936 - val_accuracy: 0.4634\n",
            "Epoch 653/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4042 - accuracy: 0.8286 - val_loss: 13.7702 - val_accuracy: 0.4641\n",
            "Epoch 654/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4190 - accuracy: 0.8322 - val_loss: 12.6004 - val_accuracy: 0.4566\n",
            "Epoch 655/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3942 - accuracy: 0.8305 - val_loss: 12.9007 - val_accuracy: 0.4796\n",
            "Epoch 656/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4252 - accuracy: 0.8296 - val_loss: 13.3773 - val_accuracy: 0.4628\n",
            "Epoch 657/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4280 - accuracy: 0.8254 - val_loss: 12.7708 - val_accuracy: 0.4659\n",
            "Epoch 658/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4353 - accuracy: 0.8316 - val_loss: 12.7739 - val_accuracy: 0.4455\n",
            "Epoch 659/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4820 - accuracy: 0.8257 - val_loss: 12.6123 - val_accuracy: 0.4665\n",
            "Epoch 660/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4162 - accuracy: 0.8359 - val_loss: 12.7431 - val_accuracy: 0.4696\n",
            "Epoch 661/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4514 - accuracy: 0.8280 - val_loss: 12.1678 - val_accuracy: 0.4572\n",
            "Epoch 662/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4213 - accuracy: 0.8288 - val_loss: 12.7317 - val_accuracy: 0.4721\n",
            "Epoch 663/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4141 - accuracy: 0.8279 - val_loss: 14.0526 - val_accuracy: 0.4690\n",
            "Epoch 664/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4077 - accuracy: 0.8300 - val_loss: 13.7316 - val_accuracy: 0.4765\n",
            "Epoch 665/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4225 - accuracy: 0.8322 - val_loss: 12.8322 - val_accuracy: 0.4665\n",
            "Epoch 666/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4042 - accuracy: 0.8358 - val_loss: 12.4346 - val_accuracy: 0.4473\n",
            "Epoch 667/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4171 - accuracy: 0.8299 - val_loss: 13.1973 - val_accuracy: 0.4430\n",
            "Epoch 668/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4091 - accuracy: 0.8339 - val_loss: 12.7674 - val_accuracy: 0.4597\n",
            "Epoch 669/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4264 - accuracy: 0.8274 - val_loss: 13.6862 - val_accuracy: 0.4641\n",
            "Epoch 670/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4243 - accuracy: 0.8351 - val_loss: 12.3742 - val_accuracy: 0.4616\n",
            "Epoch 671/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4234 - accuracy: 0.8275 - val_loss: 12.9189 - val_accuracy: 0.4703\n",
            "Epoch 672/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3939 - accuracy: 0.8353 - val_loss: 12.8935 - val_accuracy: 0.4684\n",
            "Epoch 673/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4538 - accuracy: 0.8240 - val_loss: 12.2410 - val_accuracy: 0.4566\n",
            "Epoch 674/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4381 - accuracy: 0.8342 - val_loss: 14.0435 - val_accuracy: 0.4554\n",
            "Epoch 675/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4245 - accuracy: 0.8300 - val_loss: 12.5274 - val_accuracy: 0.4566\n",
            "Epoch 676/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4250 - accuracy: 0.8313 - val_loss: 12.7982 - val_accuracy: 0.4622\n",
            "Epoch 677/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4110 - accuracy: 0.8319 - val_loss: 13.2219 - val_accuracy: 0.4641\n",
            "Epoch 678/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4158 - accuracy: 0.8286 - val_loss: 12.7094 - val_accuracy: 0.4765\n",
            "Epoch 679/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4109 - accuracy: 0.8294 - val_loss: 12.2726 - val_accuracy: 0.4430\n",
            "Epoch 680/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4106 - accuracy: 0.8313 - val_loss: 13.3546 - val_accuracy: 0.4542\n",
            "Epoch 681/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4203 - accuracy: 0.8300 - val_loss: 12.3554 - val_accuracy: 0.4690\n",
            "Epoch 682/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4376 - accuracy: 0.8322 - val_loss: 14.0467 - val_accuracy: 0.4690\n",
            "Epoch 683/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4124 - accuracy: 0.8324 - val_loss: 12.7891 - val_accuracy: 0.4610\n",
            "Epoch 684/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4257 - accuracy: 0.8339 - val_loss: 13.2118 - val_accuracy: 0.4678\n",
            "Epoch 685/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3964 - accuracy: 0.8344 - val_loss: 13.5864 - val_accuracy: 0.4579\n",
            "Epoch 686/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4449 - accuracy: 0.8306 - val_loss: 13.1974 - val_accuracy: 0.4572\n",
            "Epoch 687/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4153 - accuracy: 0.8353 - val_loss: 14.0792 - val_accuracy: 0.4690\n",
            "Epoch 688/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4208 - accuracy: 0.8296 - val_loss: 14.2805 - val_accuracy: 0.4622\n",
            "Epoch 689/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4307 - accuracy: 0.8319 - val_loss: 13.6166 - val_accuracy: 0.4641\n",
            "Epoch 690/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4150 - accuracy: 0.8328 - val_loss: 13.0385 - val_accuracy: 0.4535\n",
            "Epoch 691/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4335 - accuracy: 0.8265 - val_loss: 13.1179 - val_accuracy: 0.4610\n",
            "Epoch 692/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4281 - accuracy: 0.8274 - val_loss: 12.8389 - val_accuracy: 0.4622\n",
            "Epoch 693/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3982 - accuracy: 0.8348 - val_loss: 15.4359 - val_accuracy: 0.4597\n",
            "Epoch 694/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4111 - accuracy: 0.8373 - val_loss: 13.1096 - val_accuracy: 0.4634\n",
            "Epoch 695/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4197 - accuracy: 0.8306 - val_loss: 13.6518 - val_accuracy: 0.4672\n",
            "Epoch 696/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4024 - accuracy: 0.8389 - val_loss: 13.1510 - val_accuracy: 0.4665\n",
            "Epoch 697/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4405 - accuracy: 0.8316 - val_loss: 14.1707 - val_accuracy: 0.4486\n",
            "Epoch 698/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4550 - accuracy: 0.8272 - val_loss: 15.1518 - val_accuracy: 0.4622\n",
            "Epoch 699/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4065 - accuracy: 0.8355 - val_loss: 13.8750 - val_accuracy: 0.4517\n",
            "Epoch 700/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4015 - accuracy: 0.8375 - val_loss: 13.4950 - val_accuracy: 0.4560\n",
            "Epoch 701/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4599 - accuracy: 0.8302 - val_loss: 13.3736 - val_accuracy: 0.4597\n",
            "Epoch 702/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4420 - accuracy: 0.8313 - val_loss: 14.1003 - val_accuracy: 0.4628\n",
            "Epoch 703/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4263 - accuracy: 0.8286 - val_loss: 14.1347 - val_accuracy: 0.4715\n",
            "Epoch 704/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4543 - accuracy: 0.8320 - val_loss: 13.3493 - val_accuracy: 0.4696\n",
            "Epoch 705/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4209 - accuracy: 0.8271 - val_loss: 15.0366 - val_accuracy: 0.4765\n",
            "Epoch 706/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4148 - accuracy: 0.8358 - val_loss: 13.1134 - val_accuracy: 0.4690\n",
            "Epoch 707/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4087 - accuracy: 0.8345 - val_loss: 12.7912 - val_accuracy: 0.4734\n",
            "Epoch 708/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3995 - accuracy: 0.8393 - val_loss: 13.7954 - val_accuracy: 0.4672\n",
            "Epoch 709/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4165 - accuracy: 0.8308 - val_loss: 13.9603 - val_accuracy: 0.4647\n",
            "Epoch 710/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4060 - accuracy: 0.8302 - val_loss: 12.9185 - val_accuracy: 0.4572\n",
            "Epoch 711/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4380 - accuracy: 0.8241 - val_loss: 12.9808 - val_accuracy: 0.4424\n",
            "Epoch 712/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3952 - accuracy: 0.8367 - val_loss: 14.3775 - val_accuracy: 0.4678\n",
            "Epoch 713/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4012 - accuracy: 0.8364 - val_loss: 12.7432 - val_accuracy: 0.4535\n",
            "Epoch 714/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4238 - accuracy: 0.8297 - val_loss: 14.3447 - val_accuracy: 0.4517\n",
            "Epoch 715/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4596 - accuracy: 0.8308 - val_loss: 14.6605 - val_accuracy: 0.4665\n",
            "Epoch 716/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4789 - accuracy: 0.8314 - val_loss: 12.8848 - val_accuracy: 0.4721\n",
            "Epoch 717/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4404 - accuracy: 0.8269 - val_loss: 12.9559 - val_accuracy: 0.4827\n",
            "Epoch 718/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4058 - accuracy: 0.8314 - val_loss: 12.2859 - val_accuracy: 0.4647\n",
            "Epoch 719/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4126 - accuracy: 0.8331 - val_loss: 13.4437 - val_accuracy: 0.4690\n",
            "Epoch 720/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4421 - accuracy: 0.8328 - val_loss: 12.8872 - val_accuracy: 0.4585\n",
            "Epoch 721/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4322 - accuracy: 0.8348 - val_loss: 12.9377 - val_accuracy: 0.4548\n",
            "Epoch 722/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4179 - accuracy: 0.8376 - val_loss: 14.3852 - val_accuracy: 0.4610\n",
            "Epoch 723/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3960 - accuracy: 0.8364 - val_loss: 12.5416 - val_accuracy: 0.4622\n",
            "Epoch 724/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4205 - accuracy: 0.8325 - val_loss: 12.8348 - val_accuracy: 0.4603\n",
            "Epoch 725/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4401 - accuracy: 0.8336 - val_loss: 14.4670 - val_accuracy: 0.4690\n",
            "Epoch 726/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4200 - accuracy: 0.8293 - val_loss: 13.7242 - val_accuracy: 0.4802\n",
            "Epoch 727/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4029 - accuracy: 0.8342 - val_loss: 13.1028 - val_accuracy: 0.4734\n",
            "Epoch 728/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4190 - accuracy: 0.8328 - val_loss: 13.3049 - val_accuracy: 0.4703\n",
            "Epoch 729/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4394 - accuracy: 0.8341 - val_loss: 13.4462 - val_accuracy: 0.4647\n",
            "Epoch 730/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3830 - accuracy: 0.8410 - val_loss: 15.0894 - val_accuracy: 0.4734\n",
            "Epoch 731/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4394 - accuracy: 0.8272 - val_loss: 13.3700 - val_accuracy: 0.4665\n",
            "Epoch 732/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4211 - accuracy: 0.8328 - val_loss: 12.9546 - val_accuracy: 0.4690\n",
            "Epoch 733/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.3802 - accuracy: 0.8404 - val_loss: 14.3604 - val_accuracy: 0.4734\n",
            "Epoch 734/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4321 - accuracy: 0.8319 - val_loss: 13.6154 - val_accuracy: 0.4492\n",
            "Epoch 735/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4240 - accuracy: 0.8341 - val_loss: 14.4524 - val_accuracy: 0.4672\n",
            "Epoch 736/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4078 - accuracy: 0.8367 - val_loss: 14.0686 - val_accuracy: 0.4703\n",
            "Epoch 737/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4126 - accuracy: 0.8328 - val_loss: 13.2739 - val_accuracy: 0.4572\n",
            "Epoch 738/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4104 - accuracy: 0.8303 - val_loss: 15.3560 - val_accuracy: 0.4603\n",
            "Epoch 739/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.3931 - accuracy: 0.8337 - val_loss: 14.5887 - val_accuracy: 0.4777\n",
            "Epoch 740/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4147 - accuracy: 0.8325 - val_loss: 13.7655 - val_accuracy: 0.4449\n",
            "Epoch 741/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4331 - accuracy: 0.8334 - val_loss: 14.6465 - val_accuracy: 0.4709\n",
            "Epoch 742/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4188 - accuracy: 0.8375 - val_loss: 16.0315 - val_accuracy: 0.4659\n",
            "Epoch 743/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4254 - accuracy: 0.8356 - val_loss: 13.7614 - val_accuracy: 0.4857\n",
            "Epoch 744/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4057 - accuracy: 0.8325 - val_loss: 13.1057 - val_accuracy: 0.4628\n",
            "Epoch 745/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4165 - accuracy: 0.8350 - val_loss: 13.8167 - val_accuracy: 0.4672\n",
            "Epoch 746/1000\n",
            "202/202 [==============================] - 1s 4ms/step - loss: 0.4194 - accuracy: 0.8344 - val_loss: 12.4737 - val_accuracy: 0.4597\n",
            "Epoch 747/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3934 - accuracy: 0.8359 - val_loss: 13.3865 - val_accuracy: 0.4603\n",
            "Epoch 748/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3960 - accuracy: 0.8324 - val_loss: 14.7157 - val_accuracy: 0.4622\n",
            "Epoch 749/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3881 - accuracy: 0.8337 - val_loss: 15.1475 - val_accuracy: 0.4696\n",
            "Epoch 750/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3982 - accuracy: 0.8379 - val_loss: 14.3662 - val_accuracy: 0.4610\n",
            "Epoch 751/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4222 - accuracy: 0.8322 - val_loss: 13.4544 - val_accuracy: 0.4622\n",
            "Epoch 752/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4520 - accuracy: 0.8282 - val_loss: 13.0810 - val_accuracy: 0.4628\n",
            "Epoch 753/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4100 - accuracy: 0.8342 - val_loss: 14.4899 - val_accuracy: 0.4752\n",
            "Epoch 754/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4156 - accuracy: 0.8322 - val_loss: 13.0204 - val_accuracy: 0.4616\n",
            "Epoch 755/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4347 - accuracy: 0.8308 - val_loss: 13.7542 - val_accuracy: 0.4498\n",
            "Epoch 756/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4004 - accuracy: 0.8399 - val_loss: 14.2068 - val_accuracy: 0.4721\n",
            "Epoch 757/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4048 - accuracy: 0.8324 - val_loss: 13.5150 - val_accuracy: 0.4715\n",
            "Epoch 758/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4074 - accuracy: 0.8322 - val_loss: 15.0016 - val_accuracy: 0.4696\n",
            "Epoch 759/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4000 - accuracy: 0.8398 - val_loss: 16.0151 - val_accuracy: 0.4492\n",
            "Epoch 760/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3964 - accuracy: 0.8372 - val_loss: 13.3927 - val_accuracy: 0.4486\n",
            "Epoch 761/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4410 - accuracy: 0.8294 - val_loss: 15.5261 - val_accuracy: 0.4628\n",
            "Epoch 762/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4378 - accuracy: 0.8300 - val_loss: 14.8267 - val_accuracy: 0.4703\n",
            "Epoch 763/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3872 - accuracy: 0.8404 - val_loss: 15.1679 - val_accuracy: 0.4517\n",
            "Epoch 764/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4022 - accuracy: 0.8336 - val_loss: 13.3658 - val_accuracy: 0.4703\n",
            "Epoch 765/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4262 - accuracy: 0.8356 - val_loss: 13.3619 - val_accuracy: 0.4504\n",
            "Epoch 766/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4455 - accuracy: 0.8328 - val_loss: 15.8700 - val_accuracy: 0.4901\n",
            "Epoch 767/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3947 - accuracy: 0.8353 - val_loss: 16.2662 - val_accuracy: 0.4442\n",
            "Epoch 768/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4173 - accuracy: 0.8320 - val_loss: 14.4655 - val_accuracy: 0.4511\n",
            "Epoch 769/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4003 - accuracy: 0.8412 - val_loss: 17.3727 - val_accuracy: 0.4542\n",
            "Epoch 770/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4257 - accuracy: 0.8364 - val_loss: 14.2181 - val_accuracy: 0.4517\n",
            "Epoch 771/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3935 - accuracy: 0.8401 - val_loss: 15.5582 - val_accuracy: 0.4715\n",
            "Epoch 772/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4929 - accuracy: 0.8294 - val_loss: 13.7729 - val_accuracy: 0.4734\n",
            "Epoch 773/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4204 - accuracy: 0.8350 - val_loss: 13.5132 - val_accuracy: 0.4796\n",
            "Epoch 774/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4162 - accuracy: 0.8365 - val_loss: 13.9712 - val_accuracy: 0.4721\n",
            "Epoch 775/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3993 - accuracy: 0.8328 - val_loss: 13.7270 - val_accuracy: 0.4312\n",
            "Epoch 776/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4184 - accuracy: 0.8412 - val_loss: 14.7862 - val_accuracy: 0.4771\n",
            "Epoch 777/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4081 - accuracy: 0.8348 - val_loss: 13.6918 - val_accuracy: 0.4659\n",
            "Epoch 778/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4624 - accuracy: 0.8317 - val_loss: 13.4122 - val_accuracy: 0.4486\n",
            "Epoch 779/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4149 - accuracy: 0.8356 - val_loss: 15.3265 - val_accuracy: 0.4665\n",
            "Epoch 780/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3999 - accuracy: 0.8404 - val_loss: 16.0418 - val_accuracy: 0.4653\n",
            "Epoch 781/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4175 - accuracy: 0.8395 - val_loss: 14.5305 - val_accuracy: 0.4461\n",
            "Epoch 782/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3892 - accuracy: 0.8355 - val_loss: 13.0859 - val_accuracy: 0.4665\n",
            "Epoch 783/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4211 - accuracy: 0.8322 - val_loss: 14.4911 - val_accuracy: 0.4591\n",
            "Epoch 784/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4342 - accuracy: 0.8368 - val_loss: 15.2659 - val_accuracy: 0.4554\n",
            "Epoch 785/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3984 - accuracy: 0.8409 - val_loss: 14.6855 - val_accuracy: 0.4746\n",
            "Epoch 786/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4178 - accuracy: 0.8341 - val_loss: 14.3958 - val_accuracy: 0.4734\n",
            "Epoch 787/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4287 - accuracy: 0.8347 - val_loss: 13.8503 - val_accuracy: 0.4374\n",
            "Epoch 788/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3967 - accuracy: 0.8403 - val_loss: 14.6442 - val_accuracy: 0.4523\n",
            "Epoch 789/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3885 - accuracy: 0.8364 - val_loss: 14.7117 - val_accuracy: 0.4696\n",
            "Epoch 790/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4138 - accuracy: 0.8350 - val_loss: 12.8593 - val_accuracy: 0.4461\n",
            "Epoch 791/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4260 - accuracy: 0.8359 - val_loss: 15.3360 - val_accuracy: 0.4721\n",
            "Epoch 792/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4400 - accuracy: 0.8303 - val_loss: 12.8013 - val_accuracy: 0.4715\n",
            "Epoch 793/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4186 - accuracy: 0.8334 - val_loss: 13.7067 - val_accuracy: 0.4473\n",
            "Epoch 794/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4316 - accuracy: 0.8358 - val_loss: 13.9640 - val_accuracy: 0.4504\n",
            "Epoch 795/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4062 - accuracy: 0.8410 - val_loss: 14.9612 - val_accuracy: 0.4789\n",
            "Epoch 796/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4118 - accuracy: 0.8415 - val_loss: 14.1729 - val_accuracy: 0.4622\n",
            "Epoch 797/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4216 - accuracy: 0.8325 - val_loss: 12.9359 - val_accuracy: 0.4566\n",
            "Epoch 798/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4062 - accuracy: 0.8337 - val_loss: 14.1545 - val_accuracy: 0.4597\n",
            "Epoch 799/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4023 - accuracy: 0.8365 - val_loss: 14.1384 - val_accuracy: 0.4560\n",
            "Epoch 800/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4007 - accuracy: 0.8310 - val_loss: 13.9752 - val_accuracy: 0.4442\n",
            "Epoch 801/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3984 - accuracy: 0.8382 - val_loss: 14.5826 - val_accuracy: 0.4622\n",
            "Epoch 802/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4156 - accuracy: 0.8356 - val_loss: 14.5253 - val_accuracy: 0.4579\n",
            "Epoch 803/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4075 - accuracy: 0.8375 - val_loss: 13.5031 - val_accuracy: 0.4665\n",
            "Epoch 804/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4145 - accuracy: 0.8409 - val_loss: 14.3554 - val_accuracy: 0.4461\n",
            "Epoch 805/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3937 - accuracy: 0.8403 - val_loss: 16.9685 - val_accuracy: 0.4820\n",
            "Epoch 806/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3994 - accuracy: 0.8426 - val_loss: 14.7009 - val_accuracy: 0.4690\n",
            "Epoch 807/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4068 - accuracy: 0.8381 - val_loss: 15.8721 - val_accuracy: 0.4628\n",
            "Epoch 808/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3934 - accuracy: 0.8339 - val_loss: 14.5101 - val_accuracy: 0.4560\n",
            "Epoch 809/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3895 - accuracy: 0.8392 - val_loss: 15.4995 - val_accuracy: 0.4721\n",
            "Epoch 810/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4076 - accuracy: 0.8330 - val_loss: 14.0694 - val_accuracy: 0.4517\n",
            "Epoch 811/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4041 - accuracy: 0.8311 - val_loss: 15.8019 - val_accuracy: 0.4616\n",
            "Epoch 812/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4029 - accuracy: 0.8365 - val_loss: 14.1501 - val_accuracy: 0.4684\n",
            "Epoch 813/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4048 - accuracy: 0.8429 - val_loss: 14.0020 - val_accuracy: 0.4678\n",
            "Epoch 814/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4229 - accuracy: 0.8418 - val_loss: 14.6151 - val_accuracy: 0.4467\n",
            "Epoch 815/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4301 - accuracy: 0.8432 - val_loss: 15.4744 - val_accuracy: 0.4603\n",
            "Epoch 816/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4157 - accuracy: 0.8353 - val_loss: 15.2382 - val_accuracy: 0.4467\n",
            "Epoch 817/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4001 - accuracy: 0.8359 - val_loss: 14.8238 - val_accuracy: 0.4548\n",
            "Epoch 818/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4251 - accuracy: 0.8395 - val_loss: 16.0989 - val_accuracy: 0.4529\n",
            "Epoch 819/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4178 - accuracy: 0.8347 - val_loss: 15.6879 - val_accuracy: 0.4721\n",
            "Epoch 820/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3849 - accuracy: 0.8381 - val_loss: 15.5299 - val_accuracy: 0.4523\n",
            "Epoch 821/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4342 - accuracy: 0.8362 - val_loss: 17.8190 - val_accuracy: 0.4746\n",
            "Epoch 822/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4239 - accuracy: 0.8361 - val_loss: 16.2829 - val_accuracy: 0.4696\n",
            "Epoch 823/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3957 - accuracy: 0.8389 - val_loss: 15.9357 - val_accuracy: 0.4653\n",
            "Epoch 824/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3823 - accuracy: 0.8418 - val_loss: 15.9542 - val_accuracy: 0.4548\n",
            "Epoch 825/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4201 - accuracy: 0.8328 - val_loss: 14.7347 - val_accuracy: 0.4777\n",
            "Epoch 826/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4200 - accuracy: 0.8376 - val_loss: 14.3520 - val_accuracy: 0.4665\n",
            "Epoch 827/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4220 - accuracy: 0.8359 - val_loss: 17.2054 - val_accuracy: 0.4653\n",
            "Epoch 828/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3978 - accuracy: 0.8407 - val_loss: 15.8527 - val_accuracy: 0.4405\n",
            "Epoch 829/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4118 - accuracy: 0.8382 - val_loss: 15.3396 - val_accuracy: 0.4591\n",
            "Epoch 830/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3958 - accuracy: 0.8364 - val_loss: 14.8237 - val_accuracy: 0.4362\n",
            "Epoch 831/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3751 - accuracy: 0.8384 - val_loss: 16.0972 - val_accuracy: 0.4827\n",
            "Epoch 832/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4287 - accuracy: 0.8406 - val_loss: 14.8989 - val_accuracy: 0.4511\n",
            "Epoch 833/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3942 - accuracy: 0.8378 - val_loss: 16.0219 - val_accuracy: 0.4566\n",
            "Epoch 834/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4504 - accuracy: 0.8409 - val_loss: 16.2311 - val_accuracy: 0.4641\n",
            "Epoch 835/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3917 - accuracy: 0.8435 - val_loss: 16.3067 - val_accuracy: 0.4641\n",
            "Epoch 836/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4373 - accuracy: 0.8348 - val_loss: 16.6921 - val_accuracy: 0.4709\n",
            "Epoch 837/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4102 - accuracy: 0.8379 - val_loss: 15.7194 - val_accuracy: 0.4740\n",
            "Epoch 838/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4023 - accuracy: 0.8367 - val_loss: 16.3684 - val_accuracy: 0.4647\n",
            "Epoch 839/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3967 - accuracy: 0.8446 - val_loss: 16.3070 - val_accuracy: 0.4703\n",
            "Epoch 840/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4121 - accuracy: 0.8358 - val_loss: 15.3708 - val_accuracy: 0.4597\n",
            "Epoch 841/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4080 - accuracy: 0.8385 - val_loss: 15.4722 - val_accuracy: 0.4690\n",
            "Epoch 842/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4035 - accuracy: 0.8399 - val_loss: 15.6703 - val_accuracy: 0.4492\n",
            "Epoch 843/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4113 - accuracy: 0.8365 - val_loss: 15.2953 - val_accuracy: 0.4709\n",
            "Epoch 844/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4066 - accuracy: 0.8348 - val_loss: 15.6209 - val_accuracy: 0.4734\n",
            "Epoch 845/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4278 - accuracy: 0.8342 - val_loss: 13.5791 - val_accuracy: 0.4585\n",
            "Epoch 846/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3772 - accuracy: 0.8396 - val_loss: 15.5231 - val_accuracy: 0.4672\n",
            "Epoch 847/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4293 - accuracy: 0.8358 - val_loss: 15.4996 - val_accuracy: 0.4616\n",
            "Epoch 848/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4060 - accuracy: 0.8387 - val_loss: 13.9789 - val_accuracy: 0.4727\n",
            "Epoch 849/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3990 - accuracy: 0.8375 - val_loss: 16.3639 - val_accuracy: 0.4734\n",
            "Epoch 850/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3943 - accuracy: 0.8440 - val_loss: 15.2461 - val_accuracy: 0.4684\n",
            "Epoch 851/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4252 - accuracy: 0.8404 - val_loss: 16.2520 - val_accuracy: 0.4498\n",
            "Epoch 852/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4277 - accuracy: 0.8401 - val_loss: 15.9379 - val_accuracy: 0.4610\n",
            "Epoch 853/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4236 - accuracy: 0.8415 - val_loss: 15.7747 - val_accuracy: 0.4535\n",
            "Epoch 854/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4265 - accuracy: 0.8382 - val_loss: 14.3842 - val_accuracy: 0.4616\n",
            "Epoch 855/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4122 - accuracy: 0.8368 - val_loss: 15.4143 - val_accuracy: 0.4486\n",
            "Epoch 856/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3996 - accuracy: 0.8395 - val_loss: 15.0051 - val_accuracy: 0.4857\n",
            "Epoch 857/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4020 - accuracy: 0.8348 - val_loss: 16.0102 - val_accuracy: 0.4659\n",
            "Epoch 858/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4057 - accuracy: 0.8361 - val_loss: 16.0791 - val_accuracy: 0.4802\n",
            "Epoch 859/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3840 - accuracy: 0.8367 - val_loss: 14.4891 - val_accuracy: 0.4566\n",
            "Epoch 860/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4242 - accuracy: 0.8361 - val_loss: 15.1671 - val_accuracy: 0.4665\n",
            "Epoch 861/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4392 - accuracy: 0.8333 - val_loss: 15.8215 - val_accuracy: 0.4535\n",
            "Epoch 862/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3989 - accuracy: 0.8421 - val_loss: 16.5341 - val_accuracy: 0.4572\n",
            "Epoch 863/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3842 - accuracy: 0.8382 - val_loss: 18.5885 - val_accuracy: 0.4616\n",
            "Epoch 864/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3849 - accuracy: 0.8376 - val_loss: 16.8708 - val_accuracy: 0.4622\n",
            "Epoch 865/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4241 - accuracy: 0.8314 - val_loss: 17.1647 - val_accuracy: 0.4641\n",
            "Epoch 866/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4078 - accuracy: 0.8356 - val_loss: 15.2349 - val_accuracy: 0.4634\n",
            "Epoch 867/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4578 - accuracy: 0.8344 - val_loss: 16.6367 - val_accuracy: 0.4622\n",
            "Epoch 868/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4000 - accuracy: 0.8430 - val_loss: 16.8099 - val_accuracy: 0.4566\n",
            "Epoch 869/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3877 - accuracy: 0.8413 - val_loss: 16.7826 - val_accuracy: 0.4597\n",
            "Epoch 870/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4301 - accuracy: 0.8375 - val_loss: 16.0532 - val_accuracy: 0.4678\n",
            "Epoch 871/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3889 - accuracy: 0.8452 - val_loss: 17.5116 - val_accuracy: 0.4628\n",
            "Epoch 872/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4229 - accuracy: 0.8407 - val_loss: 17.5553 - val_accuracy: 0.4641\n",
            "Epoch 873/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4371 - accuracy: 0.8351 - val_loss: 14.9999 - val_accuracy: 0.4486\n",
            "Epoch 874/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4189 - accuracy: 0.8359 - val_loss: 15.4331 - val_accuracy: 0.4641\n",
            "Epoch 875/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4353 - accuracy: 0.8293 - val_loss: 15.8641 - val_accuracy: 0.4622\n",
            "Epoch 876/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3737 - accuracy: 0.8426 - val_loss: 16.5575 - val_accuracy: 0.4492\n",
            "Epoch 877/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3825 - accuracy: 0.8457 - val_loss: 15.2110 - val_accuracy: 0.4672\n",
            "Epoch 878/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4034 - accuracy: 0.8440 - val_loss: 15.0064 - val_accuracy: 0.4616\n",
            "Epoch 879/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3882 - accuracy: 0.8381 - val_loss: 16.9820 - val_accuracy: 0.4647\n",
            "Epoch 880/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3903 - accuracy: 0.8368 - val_loss: 16.2547 - val_accuracy: 0.4678\n",
            "Epoch 881/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4171 - accuracy: 0.8306 - val_loss: 17.7867 - val_accuracy: 0.4560\n",
            "Epoch 882/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3778 - accuracy: 0.8389 - val_loss: 17.5967 - val_accuracy: 0.4765\n",
            "Epoch 883/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3967 - accuracy: 0.8409 - val_loss: 14.3479 - val_accuracy: 0.4480\n",
            "Epoch 884/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4003 - accuracy: 0.8390 - val_loss: 16.5054 - val_accuracy: 0.4796\n",
            "Epoch 885/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4085 - accuracy: 0.8404 - val_loss: 16.1342 - val_accuracy: 0.4783\n",
            "Epoch 886/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4087 - accuracy: 0.8410 - val_loss: 16.0674 - val_accuracy: 0.4504\n",
            "Epoch 887/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3933 - accuracy: 0.8420 - val_loss: 16.0222 - val_accuracy: 0.4597\n",
            "Epoch 888/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4190 - accuracy: 0.8444 - val_loss: 15.1055 - val_accuracy: 0.4523\n",
            "Epoch 889/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4019 - accuracy: 0.8350 - val_loss: 16.6713 - val_accuracy: 0.4591\n",
            "Epoch 890/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4282 - accuracy: 0.8364 - val_loss: 15.7220 - val_accuracy: 0.4634\n",
            "Epoch 891/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4136 - accuracy: 0.8438 - val_loss: 16.9827 - val_accuracy: 0.4535\n",
            "Epoch 892/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3964 - accuracy: 0.8432 - val_loss: 17.4433 - val_accuracy: 0.4653\n",
            "Epoch 893/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4232 - accuracy: 0.8392 - val_loss: 16.7588 - val_accuracy: 0.4641\n",
            "Epoch 894/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3943 - accuracy: 0.8393 - val_loss: 17.6053 - val_accuracy: 0.4653\n",
            "Epoch 895/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4236 - accuracy: 0.8324 - val_loss: 18.1160 - val_accuracy: 0.4672\n",
            "Epoch 896/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4047 - accuracy: 0.8416 - val_loss: 17.0124 - val_accuracy: 0.4498\n",
            "Epoch 897/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3743 - accuracy: 0.8409 - val_loss: 18.0584 - val_accuracy: 0.4888\n",
            "Epoch 898/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4138 - accuracy: 0.8293 - val_loss: 17.3252 - val_accuracy: 0.4548\n",
            "Epoch 899/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4116 - accuracy: 0.8412 - val_loss: 18.5696 - val_accuracy: 0.4585\n",
            "Epoch 900/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3782 - accuracy: 0.8451 - val_loss: 16.6110 - val_accuracy: 0.4188\n",
            "Epoch 901/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4174 - accuracy: 0.8392 - val_loss: 17.2473 - val_accuracy: 0.4597\n",
            "Epoch 902/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4495 - accuracy: 0.8317 - val_loss: 15.4656 - val_accuracy: 0.4170\n",
            "Epoch 903/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4348 - accuracy: 0.8381 - val_loss: 17.1128 - val_accuracy: 0.4684\n",
            "Epoch 904/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4087 - accuracy: 0.8404 - val_loss: 17.9008 - val_accuracy: 0.4789\n",
            "Epoch 905/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4118 - accuracy: 0.8379 - val_loss: 15.8574 - val_accuracy: 0.4554\n",
            "Epoch 906/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3899 - accuracy: 0.8429 - val_loss: 18.1122 - val_accuracy: 0.4616\n",
            "Epoch 907/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4255 - accuracy: 0.8322 - val_loss: 15.4857 - val_accuracy: 0.4424\n",
            "Epoch 908/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3978 - accuracy: 0.8413 - val_loss: 17.5657 - val_accuracy: 0.4783\n",
            "Epoch 909/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4566 - accuracy: 0.8416 - val_loss: 16.7986 - val_accuracy: 0.4585\n",
            "Epoch 910/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3872 - accuracy: 0.8401 - val_loss: 16.9253 - val_accuracy: 0.4703\n",
            "Epoch 911/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4293 - accuracy: 0.8410 - val_loss: 17.0525 - val_accuracy: 0.4585\n",
            "Epoch 912/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4289 - accuracy: 0.8378 - val_loss: 15.4852 - val_accuracy: 0.4467\n",
            "Epoch 913/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3939 - accuracy: 0.8361 - val_loss: 17.1724 - val_accuracy: 0.4771\n",
            "Epoch 914/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4151 - accuracy: 0.8373 - val_loss: 16.5694 - val_accuracy: 0.4585\n",
            "Epoch 915/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4412 - accuracy: 0.8337 - val_loss: 15.4147 - val_accuracy: 0.4653\n",
            "Epoch 916/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4065 - accuracy: 0.8412 - val_loss: 16.2938 - val_accuracy: 0.4529\n",
            "Epoch 917/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3732 - accuracy: 0.8432 - val_loss: 15.7164 - val_accuracy: 0.4473\n",
            "Epoch 918/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4275 - accuracy: 0.8353 - val_loss: 15.6067 - val_accuracy: 0.4783\n",
            "Epoch 919/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3965 - accuracy: 0.8367 - val_loss: 16.7327 - val_accuracy: 0.4480\n",
            "Epoch 920/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4009 - accuracy: 0.8418 - val_loss: 14.5744 - val_accuracy: 0.4703\n",
            "Epoch 921/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4033 - accuracy: 0.8410 - val_loss: 15.5467 - val_accuracy: 0.4318\n",
            "Epoch 922/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3840 - accuracy: 0.8416 - val_loss: 16.4773 - val_accuracy: 0.4603\n",
            "Epoch 923/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3878 - accuracy: 0.8381 - val_loss: 15.8105 - val_accuracy: 0.4461\n",
            "Epoch 924/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3845 - accuracy: 0.8454 - val_loss: 15.9360 - val_accuracy: 0.4628\n",
            "Epoch 925/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4055 - accuracy: 0.8387 - val_loss: 17.7244 - val_accuracy: 0.4653\n",
            "Epoch 926/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3852 - accuracy: 0.8416 - val_loss: 15.4142 - val_accuracy: 0.4721\n",
            "Epoch 927/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3763 - accuracy: 0.8416 - val_loss: 16.5042 - val_accuracy: 0.4746\n",
            "Epoch 928/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4158 - accuracy: 0.8410 - val_loss: 16.8144 - val_accuracy: 0.4746\n",
            "Epoch 929/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4170 - accuracy: 0.8297 - val_loss: 16.4776 - val_accuracy: 0.4523\n",
            "Epoch 930/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4160 - accuracy: 0.8334 - val_loss: 17.3431 - val_accuracy: 0.4665\n",
            "Epoch 931/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4284 - accuracy: 0.8368 - val_loss: 17.4445 - val_accuracy: 0.4758\n",
            "Epoch 932/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4382 - accuracy: 0.8355 - val_loss: 17.9966 - val_accuracy: 0.4703\n",
            "Epoch 933/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4081 - accuracy: 0.8412 - val_loss: 18.0391 - val_accuracy: 0.4647\n",
            "Epoch 934/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4044 - accuracy: 0.8387 - val_loss: 16.0662 - val_accuracy: 0.4653\n",
            "Epoch 935/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4264 - accuracy: 0.8361 - val_loss: 16.5683 - val_accuracy: 0.4634\n",
            "Epoch 936/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4425 - accuracy: 0.8355 - val_loss: 16.8907 - val_accuracy: 0.4715\n",
            "Epoch 937/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3998 - accuracy: 0.8407 - val_loss: 17.4280 - val_accuracy: 0.4566\n",
            "Epoch 938/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3977 - accuracy: 0.8399 - val_loss: 17.0157 - val_accuracy: 0.4535\n",
            "Epoch 939/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4045 - accuracy: 0.8418 - val_loss: 17.9249 - val_accuracy: 0.4411\n",
            "Epoch 940/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4268 - accuracy: 0.8368 - val_loss: 16.8339 - val_accuracy: 0.4684\n",
            "Epoch 941/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4352 - accuracy: 0.8358 - val_loss: 18.2425 - val_accuracy: 0.4418\n",
            "Epoch 942/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4161 - accuracy: 0.8390 - val_loss: 15.4981 - val_accuracy: 0.4523\n",
            "Epoch 943/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4179 - accuracy: 0.8468 - val_loss: 16.6027 - val_accuracy: 0.4721\n",
            "Epoch 944/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4139 - accuracy: 0.8365 - val_loss: 17.3617 - val_accuracy: 0.4734\n",
            "Epoch 945/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4040 - accuracy: 0.8390 - val_loss: 15.3754 - val_accuracy: 0.4498\n",
            "Epoch 946/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4085 - accuracy: 0.8432 - val_loss: 18.6321 - val_accuracy: 0.4665\n",
            "Epoch 947/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3819 - accuracy: 0.8392 - val_loss: 17.9516 - val_accuracy: 0.4554\n",
            "Epoch 948/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3809 - accuracy: 0.8478 - val_loss: 16.8393 - val_accuracy: 0.4665\n",
            "Epoch 949/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4239 - accuracy: 0.8389 - val_loss: 16.6086 - val_accuracy: 0.4672\n",
            "Epoch 950/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4164 - accuracy: 0.8395 - val_loss: 17.3322 - val_accuracy: 0.4486\n",
            "Epoch 951/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3948 - accuracy: 0.8432 - val_loss: 17.6451 - val_accuracy: 0.4665\n",
            "Epoch 952/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3879 - accuracy: 0.8451 - val_loss: 19.5675 - val_accuracy: 0.4709\n",
            "Epoch 953/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4000 - accuracy: 0.8424 - val_loss: 17.0281 - val_accuracy: 0.4461\n",
            "Epoch 954/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3923 - accuracy: 0.8410 - val_loss: 16.8786 - val_accuracy: 0.4368\n",
            "Epoch 955/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4108 - accuracy: 0.8441 - val_loss: 18.0443 - val_accuracy: 0.4647\n",
            "Epoch 956/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4078 - accuracy: 0.8446 - val_loss: 17.1955 - val_accuracy: 0.4703\n",
            "Epoch 957/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3894 - accuracy: 0.8451 - val_loss: 17.1679 - val_accuracy: 0.4758\n",
            "Epoch 958/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4054 - accuracy: 0.8423 - val_loss: 17.5934 - val_accuracy: 0.4721\n",
            "Epoch 959/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3888 - accuracy: 0.8429 - val_loss: 17.7968 - val_accuracy: 0.4634\n",
            "Epoch 960/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4050 - accuracy: 0.8389 - val_loss: 17.2433 - val_accuracy: 0.4709\n",
            "Epoch 961/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3995 - accuracy: 0.8385 - val_loss: 19.0367 - val_accuracy: 0.4616\n",
            "Epoch 962/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3736 - accuracy: 0.8444 - val_loss: 17.0817 - val_accuracy: 0.4616\n",
            "Epoch 963/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4073 - accuracy: 0.8413 - val_loss: 16.8187 - val_accuracy: 0.4616\n",
            "Epoch 964/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3857 - accuracy: 0.8451 - val_loss: 16.1357 - val_accuracy: 0.4449\n",
            "Epoch 965/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3900 - accuracy: 0.8387 - val_loss: 18.9377 - val_accuracy: 0.4467\n",
            "Epoch 966/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4034 - accuracy: 0.8415 - val_loss: 20.1698 - val_accuracy: 0.4752\n",
            "Epoch 967/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4374 - accuracy: 0.8441 - val_loss: 16.3789 - val_accuracy: 0.4449\n",
            "Epoch 968/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4181 - accuracy: 0.8381 - val_loss: 19.1792 - val_accuracy: 0.4746\n",
            "Epoch 969/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3897 - accuracy: 0.8395 - val_loss: 16.3268 - val_accuracy: 0.4789\n",
            "Epoch 970/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3990 - accuracy: 0.8379 - val_loss: 17.2877 - val_accuracy: 0.4368\n",
            "Epoch 971/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4139 - accuracy: 0.8396 - val_loss: 18.5081 - val_accuracy: 0.4585\n",
            "Epoch 972/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3803 - accuracy: 0.8443 - val_loss: 18.4915 - val_accuracy: 0.4554\n",
            "Epoch 973/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4030 - accuracy: 0.8407 - val_loss: 19.2772 - val_accuracy: 0.4603\n",
            "Epoch 974/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4075 - accuracy: 0.8406 - val_loss: 16.9187 - val_accuracy: 0.4480\n",
            "Epoch 975/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4538 - accuracy: 0.8404 - val_loss: 19.1842 - val_accuracy: 0.4579\n",
            "Epoch 976/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4211 - accuracy: 0.8362 - val_loss: 18.3508 - val_accuracy: 0.4715\n",
            "Epoch 977/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3904 - accuracy: 0.8432 - val_loss: 16.4055 - val_accuracy: 0.4585\n",
            "Epoch 978/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3759 - accuracy: 0.8447 - val_loss: 17.2140 - val_accuracy: 0.4560\n",
            "Epoch 979/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4020 - accuracy: 0.8434 - val_loss: 17.9088 - val_accuracy: 0.4473\n",
            "Epoch 980/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3938 - accuracy: 0.8404 - val_loss: 18.6427 - val_accuracy: 0.4634\n",
            "Epoch 981/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3809 - accuracy: 0.8446 - val_loss: 18.3367 - val_accuracy: 0.4678\n",
            "Epoch 982/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4179 - accuracy: 0.8398 - val_loss: 18.0809 - val_accuracy: 0.4597\n",
            "Epoch 983/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4190 - accuracy: 0.8398 - val_loss: 18.0101 - val_accuracy: 0.4616\n",
            "Epoch 984/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4116 - accuracy: 0.8437 - val_loss: 17.3122 - val_accuracy: 0.4591\n",
            "Epoch 985/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3958 - accuracy: 0.8401 - val_loss: 19.7600 - val_accuracy: 0.4672\n",
            "Epoch 986/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3823 - accuracy: 0.8506 - val_loss: 20.8709 - val_accuracy: 0.4789\n",
            "Epoch 987/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4027 - accuracy: 0.8387 - val_loss: 19.4945 - val_accuracy: 0.4665\n",
            "Epoch 988/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4378 - accuracy: 0.8430 - val_loss: 18.8254 - val_accuracy: 0.4628\n",
            "Epoch 989/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3898 - accuracy: 0.8412 - val_loss: 17.0555 - val_accuracy: 0.4455\n",
            "Epoch 990/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3985 - accuracy: 0.8413 - val_loss: 16.9615 - val_accuracy: 0.4603\n",
            "Epoch 991/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4123 - accuracy: 0.8460 - val_loss: 19.1156 - val_accuracy: 0.4641\n",
            "Epoch 992/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3869 - accuracy: 0.8451 - val_loss: 17.8225 - val_accuracy: 0.4560\n",
            "Epoch 993/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3988 - accuracy: 0.8393 - val_loss: 18.8702 - val_accuracy: 0.4783\n",
            "Epoch 994/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3890 - accuracy: 0.8410 - val_loss: 19.5459 - val_accuracy: 0.4603\n",
            "Epoch 995/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4417 - accuracy: 0.8342 - val_loss: 18.1489 - val_accuracy: 0.4641\n",
            "Epoch 996/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4177 - accuracy: 0.8427 - val_loss: 17.4696 - val_accuracy: 0.4473\n",
            "Epoch 997/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.3760 - accuracy: 0.8423 - val_loss: 20.9602 - val_accuracy: 0.4362\n",
            "Epoch 998/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4039 - accuracy: 0.8416 - val_loss: 18.5697 - val_accuracy: 0.4746\n",
            "Epoch 999/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4043 - accuracy: 0.8387 - val_loss: 20.4643 - val_accuracy: 0.4641\n",
            "Epoch 1000/1000\n",
            "202/202 [==============================] - 1s 3ms/step - loss: 0.4107 - accuracy: 0.8395 - val_loss: 17.6928 - val_accuracy: 0.4647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzDOlly4dIzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "56b90faa-62e0-4ea8-f540-0fbf027345f2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "acc      = history.history[     'accuracy' ]\n",
        "val_acc  = history.history[ 'val_accuracy' ]\n",
        "loss     = history.history[    'loss' ]\n",
        "val_loss = history.history['val_loss' ]\n",
        "\n",
        "epochs   = range(len(acc)) # Get number of epochs\n",
        "\n",
        "\n",
        "plt.plot  ( epochs,     acc, label='Training')\n",
        "plt.plot  ( epochs, val_acc, label='Validation')\n",
        "plt.title ('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.plot  ( epochs,     loss, label='Training')\n",
        "plt.plot  ( epochs, val_loss, label='Validation')\n",
        "plt.legend()\n",
        "plt.title ('Training and validation loss')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Training and validation loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3hURdfAf5NGIAECoTdDb0oNoIAKYkFReS0oWFFfeMXeXwsq9ob1s5cXu6jYBQRpgoJKFek1SKghtJCQssl8f8zd3bst2YRsNsme3/Pk2Xvnzr333LubOTPnnDmjtNYIgiAIkUtUuAUQBEEQwosoAkEQhAhHFIEgCEKEI4pAEAQhwhFFIAiCEOGIIhAEQYhwRBEIPiilpiulri7vuuFEKZWmlDo9BNfVSql21vabSqkHg6lbhvtcrpSaWVY5BaE4lMwjqB4opY7YdmsBeUChtf8frfUnFS9V5UEplQb8W2s9q5yvq4H2WutN5VVXKZUCbAVitdaO8pBTEIojJtwCCOWD1jrRuV1co6eUipHGRagsyO+xciCmoWqOUmqQUipdKfVfpdRuYJJSqp5S6kelVIZS6oC13cJ2zjyl1L+t7dFKqV+VUhOtuluVUmeXsW5rpdR8pVSWUmqWUuo1pdTHAeQORsbHlFK/WdebqZRqYDt+pVJqm1IqUyn1QDHvp59SardSKtpWdoFSaqW13VcptUgpdVAptUsp9apSKi7Atd5XSj1u27/bOmenUupar7rDlFLLlVKHlVLblVITbIfnW58HlVJHlFInOd+t7fz+SqnFSqlD1mf/YN9NKd9zfaXUJOsZDiilvrUdG66UWmE9w2al1FCr3MMMp5Sa4PyelVIplonsOqXUP8Acq/xL63s4ZP1GutrOr6mUet76Pg9Zv7GaSqmpSqmbvZ5npVLqAn/PKgRGFEFk0ASoDxwHjMV875Os/VbAUeDVYs7vB6wHGgDPAu8ppVQZ6n4K/AkkAxOAK4u5ZzAyXgZcAzQC4oC7AJRSXYA3rOs3s+7XAj9orf8AsoHTvK77qbVdCNxuPc9JwBDghmLkxpJhqCXPGUB7wNs/kQ1cBSQBw4BxSql/WcdOsT6TtNaJWutFXteuD0wFXrGe7QVgqlIq2esZfN6NH0p6zx9hTI1drWu9aMnQF/gQuNt6hlOAtEDvww+nAp2Bs6z96Zj31AhYBthNmROB3kB/zO/4HqAI+AC4wllJKdUdaI55N0Jp0FrLXzX7w/xDnm5tDwLygfhi6vcADtj252FMSwCjgU22Y7UADTQpTV1MI+MAatmOfwx8HOQz+ZNxvG3/BuAna/shYLLtWIL1Dk4PcO3Hgf9Z27UxjfRxAereBnxj29dAO2v7feBxa/t/wNO2eh3sdf1c9yXgRWs7xaobYzs+GvjV2r4S+NPr/EXA6JLeTWneM9AU0+DW81PvLae8xf3+rP0Jzu/Z9mxtipEhyapTF6OojgLd/dSLBw5g/C5gFMbrFf3/Vh3+ZEQQGWRorXOdO0qpWkqpt6yh9mGMKSLJbh7xYrdzQ2udY20mlrJuM2C/rQxgeyCBg5Rxt207xyZTM/u1tdbZQGage2F6/xcqpWoAFwLLtNbbLDk6WOaS3ZYcT2JGByXhIQOwzev5+iml5lommUPA9UFe13ntbV5l2zC9YSeB3o0HJbznlpjv7ICfU1sCm4OU1x+ud6OUilZKPW2Zlw7jHlk0sP7i/d3L+k1/DlyhlIoCRmFGMEIpEUUQGXiHht0JdAT6aa3r4DZFBDL3lAe7gPpKqVq2spbF1D8WGXfZr23dMzlQZa31GkxDejaeZiEwJqZ1mF5nHeD+ssiAGRHZ+RT4Hmipta4LvGm7bkmhfDsxphw7rYAdQcjlTXHveTvmO0vyc952oG2Aa2ZjRoNOmvipY3/Gy4DhGPNZXcyowSnDPiC3mHt9AFyOMdnlaC8zmhAcoggik9qY4fZBy978cKhvaPWwlwATlFJxSqmTgPNCJOMU4Fyl1EDLsfsoJf/WPwVuxTSEX3rJcRg4opTqBIwLUoYvgNFKqS6WIvKWvzamt51r2dsvsx3LwJhk2gS49jSgg1LqMqVUjFLqUqAL8GOQsnnL4fc9a613YWz3r1tO5VillFNRvAdco5QaopSKUko1t94PwApgpFU/Fbg4CBnyMKO2WphRl1OGIoyZ7QWlVDNr9HCSNXrDaviLgOeR0UCZEUUQmbwE1MT0tn4Hfqqg+16OcbhmYuzyn2MaAH+UWUat9WrgRkzjvgtjR04v4bTPMA7MOVrrfbbyuzCNdBbwjiVzMDJMt55hDrDJ+rRzA/CoUioL49P4wnZuDvAE8Jsy0Uonel07EzgX05vPxDhPz/WSO1hKes9XAgWYUdFejI8ErfWfGGf0i8Ah4Bfco5QHMT34A8AjeI6w/PEhZkS2A1hjyWHnLuBvYDGwH3gGz7brQ+AEjM9JKAMyoUwIG0qpz4F1WuuQj0iE6otS6ipgrNZ6YLhlqarIiECoMJRSfZRSbS1TwlCMXfjbks4ThEBYZrcbgLfDLUtVRhSBUJE0wYQ2HsHEwI/TWi8Pq0RClUUpdRbGn7KHks1PQjGIaUgQBCHCkRGBIAhChFPlks41aNBAp6SkhFsMQRCEKsXSpUv3aa0b+jtW5RRBSkoKS5YsCbcYgiAIVQqllPdsdBdiGhIEQYhwRBEIgiBEOKIIBEEQIpwq5yPwR0FBAenp6eTm5pZcWQiK+Ph4WrRoQWxsbLhFEQQhxFQLRZCenk7t2rVJSUkh8HopQrBorcnMzCQ9PZ3WrVuHWxxBEEJMtTAN5ebmkpycLEqgnFBKkZycLCMsQYgQqoUiAEQJlDPyPgUhcqg2ikAQBKG6kJ3n4Kul6VRUCiBRBOVAZmYmPXr0oEePHjRp0oTmzZu79vPz84s9d8mSJdxyyy0l3qN///7lJa4gCGFg35E8cgsKg6r76A9ruPPLv1i6zd8qoeVPtXAWh5vk5GRWrFgBwIQJE0hMTOSuu+5yHXc4HMTE+H/VqamppKamlniPhQsXlo+wgiAUy9H8Qg4dLaBJ3fhSn+soLCIm2rN/vWbnYb5als57v26lf9tkPh1zYoCz3ew4eBSApdsO0LhOPC3r1yrhjGNDRgQhYvTo0Vx//fX069ePe+65hz///JOTTjqJnj170r9/f9avXw/AvHnzOPfccwGjRK699loGDRpEmzZteOWVV1zXS0xMdNUfNGgQF198MZ06deLyyy93DR+nTZtGp06d6N27N7fccovruoJQ3dFa++1ta63ZuCeLu7/8y9W4/rX9IOt3Z/nU/eGvnaTcO5Vr3v+TE5+aTU6+g+w8B+O//Zuvl6XjKCzizV82c87LC3zOzXMUsnTbfto9MJ0lafs9jl31vz9479etACzcnEm3CTMYPHEeL83aQMq9U7nyvT+YvyGD71bsYOKM9fR8dCa/bjKLzT01fR0nPzv3mN9PSVS7EcEjP6xmzc7D5XrNLs3q8PB5XUt9Xnp6OgsXLiQ6OprDhw+zYMECYmJimDVrFvfffz9fffWVzznr1q1j7ty5ZGVl0bFjR8aNG+cTy798+XJWr15Ns2bNGDBgAL/99hupqan85z//Yf78+bRu3ZpRo0aV+XkFoTzJcxRSIyY6qLqOwiKych0k1YolMzufBok1PI4XFBbx945D9GyZxP7sfG7/4i8ePq8LQ57/BYD1jw9l895sCgqL6N4yiRd/3sArczYBcPBoAe9clcrw134DYMn406lfK45X527ihOZ1ufkzszTG71tMQ75s20F+3bSPj3//h49//4cf/trJ3PUZAIz5cAkrth/kg2v68vuWTB79cY1Lxju//IuZt59CTl4he7Jy2XfE0zx8ONfB4VwHL83aCMCCjftYsLH4VUYHPjOHpy/sxsD2DYJ6j6Wl2imCysSIESOIjjb/AIcOHeLqq69m48aNKKUoKCjwe86wYcOoUaMGNWrUoFGjRuzZs4cWLVp41Onbt6+rrEePHqSlpZGYmEibNm1ccf+jRo3i7bdl0SYhNBQUFqE1xMUYo8Lf6Ydo1yiRmnGeDf4XS7Zzz5SVdGpSmx9vHuhjNtlx8Cj/N3sjzZJqsmBjBi3r1eLr5TsY0qkRs9ftpVHtGtSKiyYtM4f7z+nE9v1H+ej3bXw17iQuemMRAGM+cCeh/GnVbm6dvMKvzD+v2cPXy9xLV6c+Pot3rkrlhZ83+K1/xXt/eOw7lYDzWgDnvOI7OtiWmUO3CTPJcxT5vW5ZSD9wlCve+4NlD55B/YS4cruuk2qnCMrScw8VCQkJru0HH3yQwYMH880335CWlsagQYP8nlOjhrsHFB0djcPhKFMdQQgV+Y4iOoyfDsAbl/eid0o9znv1V87r3oyJI7pRIyaaOev2EBcdzZQlpuFdtzuLLfuyyXcUkZ3noFOTOjz901r+3LqfzRnZrmsvTjPO0dnr9gKwNyvPdezJaeto29D8Tz38/WpX+ZZ97vMDKQEnd3zxl8f+mA9Dk8m4OCXw4qXdiY+JZtwny0p93a+WpjPmlDbHIppfqp0iqKwcOnSI5s2bA/D++++X+/U7duzIli1bSEtLIyUlhc8//7zc7yFULV6atYH2jWozrFtTtmQcoWZcNE3r1gxY/+tl6ezNyiM2OoooBbsP53LdwNY0qm2cpnuzcvl9y36W2SJZxn2yjKtPOg4wNvaFm/axZPzpXPu+aWA7Nantqnvmi/OP+ZmcSmPVjmMz/57euTGz1u4psd51A1u77PvBcGqHhvyyIcPvsc5N6zBxRDe6NquL1pqJI7rTIDGOT//4h5lr9tCmYQJbMrKZe9cgWjdIYMHGDHq0TOKECTMBePOKXpzVtUnQspQGUQQVxD333MPVV1/N448/zrBhw8r9+jVr1uT1119n6NChJCQk0KdPn3K/h1D5OZLnoEZMFLHRUS4bdNdmgzjNsqGveuQsftu0j7iYKAZ3bAQYh2pBofbpLQO89csW/nr4TFbtOMTl7/7hcxzgg0XuNPeZ2fl8sDDNtb/Oj1O2omjXKJEXL+nBea/+6lF+fvdmvDyyB6c8N5ft+48We40T2yTzxZLtAGTl+o687zijg8u09OzF3bgktSUp9051HR/UsSFJNWNJaZDADYPauUxpSiku7t3CqmO+h8wjeUxbtZuUZBMhdHJ7zzVkhh7fNOhnLy1Vbs3i1NRU7b0wzdq1a+ncuXOYJKo8HDlyhMTERLTW3HjjjbRv357bb7+9zNeT9xo+HIVFRClFrqOQrfuyadMgkTnr9nLOCU1QSpFbUMgjP6ymSZ2anNu9KQdzCujVKonW901jWLemvDKyJ23vn+ZzXXuP9bqBrVm4OZO1u8o3uCIUNKsbT66jiP3ZbsdrnfgYPrqun8v5m/b0MDbtPcKizft48LvVLB1/OsmJNcg8kke9WnFk5TqYsWY3F/dqQVSUeYc/rdrNbZ8bc1KvVkmcc0JTHp+61nWPSdf0oX/bZDKy8hj4jG/0zh/3D+Hzxds5rVMjjm9eF8ClCNY+OtTHZ1IW0vZls+dwLv3aJB/TdZRSS7XWfmPVZURQjXjnnXf44IMPyM/Pp2fPnvznP/8Jt0hCAAqLNN+t2MHwHs2JjlI88sNqPvn9HzY8cTYA7R6YTnxsFLkFxtbsdJ52aVqHC3s192isXpxleqQfXdcXgKkrdzF15S6/97WbLUpj8igt9wztyLM/rfcpb1W/Fh9d15eCQs3pL5hRSt2asRw6aoIn7E7gd69KJSoKopTi5PYNeWLqWv7321a+v2kAl7/zB48M70r3lkncfVZH9h0xvoR2jRJp2zCBkX1bEWs5ppOtyKO6tWK5JLWlS5b42Gj+1bM5Qzo34uHvVvPAsM7MsXwTiTViOJLnoH2jRGrERNOiXi1uHNyW1+ZuBuCbG/qT7yiicZ14bhnS3u87KA8lAJDSIIGUBgklVzwGZEQgBETea/mxOG0/xzer62ocPlqUxoPfrWbsKW24/5zOrl7k0vGnUz8hjtb3+fbmw0VcTBTndmvKMxd1Y/irv7EmiBHEcxd3o1lSTWau3s0Hi7bRqn4tplx/ErXjY13vYNBzc0nLzCHt6WG8M38LbRomMKRzY/7afpBl/xzgmgGemW/zHUXsO5JHs6TAfo5jJbegkIe/W82dZ3YgObEG0VHK77FGdQJPNlux/SD5jiL6tq4fMjnLQnEjAlEEQkAi/b0WFWn2Hckr9p/eH4/+sIb//baVj6/rR4Pacew+lMvoSYsBmH/3YPIchdz79d+u9AFXnNiKj3//p9zlD8T71/ShQ+Pa/PuDJazZdZg/7h9Cvydnu47Pu2sQv2/J5N6v/wZg61PnuJIQ/pOZw7n/t4AXLunByvSDjOrXis/++IesPAeTfkuje8skmtaJ57kR3agdH4vWmtfmbuLcbs18erXZeQ7yHUXUC0E4pOCLKAKhTET6e311zkYmztzAwntP89sLPZRTgIqCxLgY1u4+TNdmdTmaX0jnh36qEPkePLcLj9kmMv1480B+2ZDBczOMSearcf256A2TmiQ+Noofbz6Z1g0SPHq5Trbuy+anVbu5/MRW1Ik3ExgHPjOHA9n5rH50qEddrbVPdtodB48y4Ok5vHlFb4YeH5rIFuHYEB+BIJTAwk37WLrtADfb7L0TZxrb+/3f/M3/jepJrbgY9hzOdSmFXo//THJCHMN7NOOdBVu5oGfzkOWEeWx4V3q2qse89XtdciXVdM84f/eqVI5vXpfjm9elcZ147vryL9o0SODNK3px/cfLiI2Ool2jxIDXb90ggXGD2nqUzb1rEP76if5SlDdPqkna0+UfDSdUDKIIhIhi96FckmrFEh/rduRlHsnjMis0ctygtjiKNHd+6Q6lnLc+gzNemE/7xoks2LiP7i3qcnL7hhQWafZm5fHOAuN0/Wb5jlLJcmGv5tx3dmcOHS3gy6XbeeuXLQB0bFyb9XuyuGVIe24/vT0ZR/JcsfzHN6/LiNSWPDVtLWef0ISB7RtQKy6a2vFupXBx7xau0MQzuzThrK6NfeztwRAbLanIIgX5psuBwYMHM2PGDI+yl156iXHjxvmtP2jQIJzmrXPOOYeDBw/61JkwYQITJ04s9r7ffvsta9a4TQMPPfQQs2bNKq34VZ7PF//D6p2HSqyntebEp2Zzg9eMzt6Pu9/ZlKXp/LIhwyfqZvfhXFc+mL/SD/Hq3E3F3uuaASke+03rxvP+NX148dLurrK7zuxIw9o1aNcokfvO7sybV/QGTFgnwPHN6qCUcikBJ43rxPPSSDNCaVwn3kMJeBMVpXjrylROPMbQQ6F6IyOCcmDUqFFMnjyZs846y1U2efJknn322RLPnTat7NEh3377Leeeey5dunQB4NFHHy3ztaoiuw/lsmBjBv/9yjg1054eRr6jiLTMbOonxHHSU7M5s2sTXrusF3sP5xJl2cbnrNvLxj1ZfPT7NlZs91TCTgdpMKQeV48lfvLFL3vwDA4dLWDSb2kATL1lIG0bJhIfG43Wmsd+XMv+7HySEz2dpEOPb+Iyr5zYJplWyaFNPSwITmREUA5cfPHFTJ061bUITVpaGjt37uSzzz4jNTWVrl278vDDD/s9NyUlhX37TE/ziSeeoEOHDgwcONCVphrM/IA+ffrQvXt3LrroInJycli4cCHff/89d999Nz169GDz5s2MHj2aKVOmADB79mx69uzJCSecwLXXXkteXp7rfg8//DC9evXihBNOYN26daF8NSHl2vcXc/eUlR5lHcZP58wX55P6+CwKCjVTV+7itsnL6fvkbI+4+TNenM+Hi7axMt2MJJ656ASG92hWqvvvzcrj/nM6ATCqr4lPv+309ib8s0ECm544m61PnUPXZnVdpiilFMsePIO0p4cVm5FTlIBQkVS/EcH0e2F38L26oGhyApz9dMDD9evXp2/fvkyfPp3hw4czefJkLrnkEu6//37q169PYWEhQ4YMYeXKlXTr1s3vNZYuXcrkyZNZsWIFDoeDXr160bu3MRVceOGFjBkzBoDx48fz3nvvcfPNN3P++edz7rnncvHFF3tcKzc3l9GjRzN79mw6dOjAVVddxRtvvMFtt90GQIMGDVi2bBmvv/46EydO5N133y2Pt1RuFBQW8d6vW7mwZ3N+3bSPC3o293BQTv7zHxZs2ucTz26f2m/n2xU7zWcAG37rBgmM6N2Sw0cdfGfVnXHbKcxau4dDRwu4JLUlf2zNpMBRxIQf1pCcEEdmdj6Hcwu4un8KRRquHdCaawa0pm1Dt0PWO9OmIFRWqp8iCBNO85BTEbz33nt88cUXvP322zgcDnbt2sWaNWsCKoIFCxZwwQUXUKuW6Qmef/75rmOrVq1i/PjxHDx4kCNHjniYoPyxfv16WrduTYcOHQC4+uqree2111yK4MILLwSgd+/efP3118f87OXBLxsySE6I47M//yGxRgxvzd/C09PNaMVRpF0zQrXWpTLf2Nl1KNdjtq6Tn247magoRWNrRar+bZPp2KQ2HW0J09o1SmTHwaNM+GEN4wa15fGpa6kVG02NmGiuP9VE23RoXBtBqIqEVBEopYYCLwPRwLta66e9jrcCPgCSrDr3aq2PbUplMT33UDJ8+HBuv/12li1bRk5ODvXr12fixIksXryYevXqMXr0aHJzc8t07dGjR/Ptt9/SvXt33n//febNm3dMsjrTWIcrhfXOg0dpWLsGUUoxf2MGt3y6nKy8wHLcM2Ul+Y4iEmvEMHf93qDv07RuPLsOeb5zbyXw0LldXCaars3qAHhEFNlpnlSTdY8NdSV1G9AuNIuECEJFEzJFoJSKBl4DzgDSgcVKqe+11mts1cYDX2it31BKdQGmASmhkimUJCYmMnjwYK699lpGjRrF4cOHSUhIoG7duuzZs4fp06cHXIMA4JRTTmH06NHcd999OBwOfvjhB1euoKysLJo2bUpBQQGffPKJK5117dq1ycryze7YsWNH0tLS2LRpE+3ateOjjz7i1FNPDclzl5bDuQX0f3pOqWfTjv92ld/yc05owrS/d/s9Nv+ewbR/YLpH2T1DO3JK+4YoZXLG92yZ5DrWtmEiz13cjVM7NPS+lAunkri6f0rQsgtCZSeUI4K+wCat9RYApdRkYDhgVwQaqGNt1wV2hlCekDNq1CguuOACJk+eTKdOnejZsyedOnWiZcuWDBgwoNhze/XqxaWXXkr37t1p1KiRRxrpxx57jH79+tGwYUP69evnavxHjhzJmDFjeOWVV1xOYoD4+HgmTZrEiBEjcDgc9OnTh+uvvz40D11KMq1l+35aVXIu+JIY2rUJJ7VtwLS/d5MQF80dZ3Yk9bh6RClFVm4BsdFRzLjtFJb9c4BGtWvw/sI0xp3a1u+EKCcjbEnJBCFSCFmKCaXUxcBQrfW/rf0rgX5a65tsdZoCM4F6QAJwutZ6qZ9rjQXGArRq1ar3tm3bPI5HeiqEUFEe7zU7z0FCDXd/Y3Hafka8uShg/X8PbM2qnYdc68bOuuMUHvhmFX9s3U/XZnX4+Lp+ZGbnUzs+hqRascRGRTF91W7OPr6JKzxUEARfKnOKiVHA+1rr55VSJwEfKaWO11p7GHK11m8Db4PJNRQGOYUy8MLPG3hl9kbGnNyaP7buZ2X6IU4uYfHtO87sQK24GFcEUKv6Cbw3ug/zN2RwUptk6iXE+SQpG9YtdAt2CEIkEEpFsAOwj7NbWGV2rgOGAmitFyml4oEGQPAeQaFSsWFPFkfzC5m9bi+vzDYrZDlTMACu2bn+2PzkOT4J0eJiooiLieKcE6SxF4RQEUpFsBhor5RqjVEAI4HLvOr8AwwB3ldKdQbiAf8LfpaAv4yIQtkJ1mR4IDufrZnZdGpSmwFPz+FATkGJ5/RomcQZXRrz3Iz1xEVHkV9YRIfGiR5KoEPjRDbsOVJm+QVBCJ6QKQKttUMpdRMwAxMa+j+t9Wql1KPAEq3198CdwDtKqdsxjuPRugxOi/j4eDIzM0lOThZlUA5orcnMzCQ+vuQ8/Cc/O5cjeQ7uPbuTXyXwzEUn0CelvmvN3IfO7cLJ7RtQqDXPzVjPZ2NPpFuLunh/a1/fMICcYkJKBUEoP6rFegQFBQWkp6eXOU5f8CU+Pp4WLVoQG2sSmu04eJTnflrHhPO78s3yHYzs04r1e7L4l7VerD+evagbl/Qx1sEJ36+mTs1Y7jijQ4XILwiCJ5XZWVwuxMbG0rp16dPsCp4czMln0m9p3DKkvY+t/vsVO/l2xU5+37Kf3Ydz+WJJut9Fz18e2YMFG/fRv20yF/Zq4SqfcH7XkMsvCELZqBaKQCgfHvtxLV8tS6d7y7qc1qmxK/SzoLCIKUu3AyYdM+BXCdx+egeG92jO8B7NK1RuQRCODcmKJbg4kGMme2XlOrjkrUV0fXgGXyzZTvsHprM5I9un/h1ndKBWnDsdw+UntqowWQVBKD+qhY9AODbmrd9LTn6ha8GW+glx7M/OD1h/7CltaJAYx9hTTLK17DwH2XmOUi/yLghCxVHtfQRC6Xll9kaa1I3nktSWjJ602ONYICVweufGtKhXk/8O7eThQ0ioEeMxe1gQhKqF/PdGKC/8bBZAT0lOCPqcd6/225kQBKGKI4ogAjn/1V9d25e85T/vT5+UeixOM8swPj+iO72Pq1chsgmCUPGIIogQHv1hDV8s2c7IPi1dyzMWx5fX9yf9QA5H8hx0alKnxPqCIFRdRBFEAIdyCvjfbybfz7u2dXsBohQU2eIFWtavySFrhnCLerJuriBEAqIIqilz1+3l2g8W83+jerJ6p2/MP8DZxzdhwvldufD1hew4eJRJ1/RhYLsGVLFAMkEQjhEJH61GOAqLePOXzQzv0ZyTn53rc/y7GwfgKCriojeMXyDt6WEVLaIgCGFCwkcjhHnrM5g4cwMTZ27wezwlOYHDucbs00Ri/gVBsBBFUE2Y/vcuxlkTwuw89q/j2ZJxhLGntKFurVgSakTTs1USt50uyd8EQTCIIqgGvDJ7o2tegJ3bT+/AlSce51EWEx3FNzcUv36yIAiRhSiCKs7b8zf7KIGzujbm5tPac3zzumGSShCEqoQogirOk9PW+ZS9daXMABYEIUU4W6gAACAASURBVHgk+2gVpbBI8+S0ta79vin1ARg/rHO4RBIEoYoiI4IqxKGjBZz8zBxqxcW41gVw8tnYE30WkxEEQQgGUQRViHfmb+FwroPDue61fO84owMD2iWLEhAEocyIIqjEHDpawDfL0rm6fwqb9h7h1bmbPI6/PLKHrAYmCMIxI4qgEnP/N38zdeUuft2Uyay1ezyOfXxdPwa0Sw6TZIIgVCdEEVRi0vfnAPgogfO6N2Ng+wbhEEkQhGqIKIJKTFaew6ds8QOn07B2jTBIIwhCdUXCRyspv27cxxY/C8aLEhAEobyREUElYtk/B/h2+Q5G9mnFlKXbfY5ff2rbMEglCEJ1RxRBJeLJqWtZsu0AHy7a5ir75ob+fLUsnZmr93BZ31ZhlE4QhOqKKIJKxKGjBT5lPVvVo2erejz+rxPCIJEgCJGA+AgqCWn7stm49wjXDEgJtyiCIEQYoggqCSu2HwTggp7uCWK/3XtauMQRBCGCENNQJSEtMxuloEPj2tx2envqxMfSPKlmuMUSBCECEEUQZtL2ZZOZnc9LszbSIDGO+NhoWT1MEIQKRRRBmLnry79Ysu0AgCwkIwhCWBAfQZjYdySPGz5Z6lICAC9d2iOMEgmCEKnIiCBMpD4+y2P/9M6NSaoVFyZpBEGIZGREUEl444pe4RZBEIQIRUYEFUxBYRHvLNji2u/ctA7jBrUlNlp0siAI4UEUQQXz+tzNvDhrAwDDTmjKa5fLSEAQhPAS0m6oUmqoUmq9UmqTUupeP8dfVEqtsP42KKUOhlKeysCGvVmubVECgiBUBkI2IlBKRQOvAWcA6cBipdT3Wus1zjpa69tt9W8GeoZKnspATr6DqSt3AXC7zBUQBKGSEMoRQV9gk9Z6i9Y6H5gMDC+m/ijgsxDKE1bW7jrM4InzXPu3nt4+fMIIgiDYCKUiaA7Yk+qnW2U+KKWOA1oDcwIcH6uUWqKUWpKRkVHuglYEl761iD2H8wD44Nq+YZZGEATBTWUJVRkJTNFaF/o7qLV+W2udqrVObdiwYQWLduwczi3gcK572clTZL1hQRAqEaFUBDuAlrb9FlaZP0ZSjc1CD3+32rU9oF0ySqkwSiMIguBJKBXBYqC9Uqq1UioO09h/711JKdUJqAcsCqEsYSUjK8+1fXm/48IoiSAIgi8hUwRaawdwEzADWAt8obVerZR6VCl1vq3qSGCy1lqHSpZwExNtRgDHJdfinBOahlkaQRAET0I6oUxrPQ2Y5lX2kNf+hFDKEE6278/h5GfnuvavG9g6jNIIgiD4p7I4i6slE2eu99i/6qSU8AgiCIJQDKIIQsT+7Hy+W7HTtd+mQUIYpREEQQiM5BoKAYVFmps/WwbAiW3qc9/ZnWnbKDHMUgmCIPhHFEEImLV2D79tygTglVE9aVQ7PswSCYIgBCYo05BS6mul1DCllJiSgmDDbpNY7r2rU0UJCIJQ6Qm2YX8duAzYqJR6WinVMYQyVXm27c+hUe0aDOncONyiCIIglEhQikBrPUtrfTnQC0gDZimlFiqlrlFKxYZSwKrGnsO5TFmaLgvNCIJQZQi6tVJKJQOjgX8Dy4GXMYrh55BIVkWZuWYPADsOHg2zJIIgCMERlLNYKfUN0BH4CDhPa73LOvS5UmpJqISrahQWad6ctxmA1y6TRWcEQagaBBs19IrWeq6/A1rr1HKUp0rzwcI010jgjC7iHxAEoWoQrGmoi1IqybmjlKqnlLohRDJVSXYfyuXRH12LrxEXIz4CQRCqBsG2VmO01q71hLXWB4AxoRGpajJn3V7XdlIt8Z8LglB1CFYRRCtbEn1rPeK40IhU9fhuxQ7u/+ZvAAZ3bMhv/z0tzBIJgiAET7A+gp8wjuG3rP3/WGUC8Numfa7tSdfIMpSCIFQtglUE/8U0/uOs/Z+Bd0MikSAIglChBKUItNZFwBvWn2CjqEjzxZJ0QEJGBUGomgQ7j6A98BTQBXAlz9FatwmRXFWGN+dvdm0P6yarjwmCUPUI1lk8CTMacACDgQ+Bj0MlVFVh454snv1pfckVBUEQKjHBKoKaWuvZgNJab7OWlxwWOrGqBlv2Zbu2PxtzYhglEQRBKDvBOovzrBTUG5VSNwE7gIhfaeXQ0QLX9kltk8MoiSAIQtkJdkRwK1ALuAXoDVwBXB0qoaoCU5amc8+UleEWQxAE4ZgpcURgTR67VGt9F3AEuCbkUlVyCgqLuOvLv1z7PVslFVNbEAShclOiItBaFyqlBlaEMFWFu21KYN1jQ2XtAUEQqjTB+giWK6W+B74EXB5SrfXXIZGqEnM4t4BvV+wEoF2jROJjo8MskSAIwrERrCKIBzIBexIdDUScIphrSy63SxafEQShGhDszOKI9ws4iYlym4HeulKWYhAEoeoT7MziSZgRgAda62vLXaJKTpF2v4aB7RuEURJBEITyIVjT0I+27XjgAmBn+YtT+TmQkx9uEQRBEMqVYE1DX9n3lVKfAb+GRKJKTGGR5qHvVgMw+85TwyyNIAhC+VDWuMf2QKPyFKQq8MsG4yju3LQObRtG/MRqQRCqCcH6CLLw9BHsxqxREFF8u3wnyQlxfHfjgHCLIgiCUG4EaxqqHWpBqgJpmdl0bV5XFqYXBKFaEVSLppS6QClV17afpJT6V+jEqpzsPpRLkzo1wi2GIAhCuRJs1/ZhrfUh547W+iDwcGhEqpy8+PMG9mbl0aROfMmVBUEQqhDBKgJ/9YINPa3y/N/sjbw8eyMATZNqhlkaQRCE8iVYRbBEKfWCUqqt9fcCsDSUglUmnv95g2u7uSgCQRCqGcEqgpuBfOBzYDKQC9xY0klKqaFKqfVKqU1KqXsD1LlEKbVGKbVaKfVpsIJXFAey3RPI+rWuzykdGoZRGkEQhPIn2KihbMBvQx4Iax2D14AzgHRgsVLqe631Glud9sB9wACt9QGlVKWbm3Djp8tc2x9d1y+MkgiCIISGYKOGflZKJdn26ymlZpRwWl9gk9Z6i9Y6HzOSGO5VZwzwmtb6AIDWei+VjK3WusSpx9WTsFFBEKolwbZsDaxIIQCshruk3ntzYLttP90qs9MB6KCU+k0p9btSaqi/CymlxiqlliillmRkZAQpcvngTDIXHaUq9L6CIAgVRbCKoEgp1cq5o5RKwU820jIQg0lXMQgYBbxjH3k40Vq/rbVO1VqnNmxYcTb6gsIi9hzOA6BOzdgKu68gCEJFEmwI6APAr0qpXwAFnAyMLeGcHUBL234Lq8xOOvCH1roA2KqU2oBRDIuDlCuknPHCL67tJ/51fBglEQRBCB1BjQi01j8BqcB64DPgTqCk5bkWA+2VUq2VUnHASOB7rzrfYkYDKKUaYExFW4IVPpTk5DtIy8wB4POxJ9JIJpIJglBNCTbp3L+BWzG9+hXAicAiPJeu9EBr7VBK3QTMAKKB/2mtVyulHgWWaK2/t46dqZRaAxQCd2utM4/lgcqLNTsPu7Y7N6sTRkkEQRBCS7CmoVuBPsDvWuvBSqlOwJMlnaS1ngZM8yp7yLatgTusv0rFoz+aKNfnLu5GnXjxDwiCUH0J1lmcq7XOBVBK1dBarwM6hk6s8FJUpCkoNL7wC3p6BzoJgiBUL4IdEaRb0TzfAj8rpQ4A20InVng57fl5pGXmcGlqS2KiZe6AIAjVm2BnFl9gbU5QSs0F6gI/hUyqMLL8nwMuJ3HrhglhlkYQBCH0lDqDqNb6l5JrVV0ueH2ha7tnS58pDYIgCNUOsXvY0No9R+6tK3vTr01yGKURBEGoGEQR2Hh/YZprWxanFwQhUhBFYOPrZWbic1x0FO0aiSIQBCEyEEVgkecoZM2uw/RomcSfDwwJtziCIAgVhigCi637siks0lwzIIWkWnHhFkcQBKHCEEVgsWHPEQA6NK4dZkkEQRAqFlEEFhv3ZBEdpWgjcwcEQYgwRBFgwkb/b84mEmvEUCMmOtziCIIgVCiiCIBOD5pJ0q3q1wqzJIIgCBVPxCuC7DwHeY4iAF4e2SPM0giCIFQ8Ea8InJPIOjetQxuZRCYIQgQS0YqgoLCI2Wv3APD+NX3CLI0gCEJ4iGhFMH3Vbpb9c5D7z+lEY1mKUhCECCWiFcH+v2dyc42pXDewTbhFEQRBCBulTkNdnRi96VZQwJHxUKdZuMURBEEIC5E7IigscG+/0Dl8cgiCIISZiFUEy1euCLcIgiAIlYLIVATznqbnd6d7lhUVhkcWQRCEMBOhiuAp37Kf7oMNM8G2SpkgCEIkEJmKwB9/vgWfjoBHkmD2o+GWRhAEocIQReCPBc+HWwJBEIQKQxSBIAhChBNximBvVi5ZumbJFbf8YhzIq76GJ1uAIy/0wgmCIISByFIE+7eSN2UctdXRkut+eD4seAFmPAD5WXBkLzjyYdIwmNgRMjeHXl5BEIQKILIUwTuDabnt6+Drz33cvV3kgIx1sO1XOLIbplxT/vIJgiCEgchSBEcP+JadM7H4c4oc5nPHUlg/3V2etaf85BIEQQgjkZVrSEWD9po4FluCvyB7r/n86jrP8qjIenWCIFRfImtE4K0EAGLKmH46Khq2zIO3B3vmLRIEQahiRJQi0FGxvoUljQgCERUNHw6Hncsga7cpc+TB9ze79wVBEKoAEaUIdrc617ewuBFB6nWBj9n9DU4z0fppsOxDmHF/2QQUBEEIAxGlCPIdfkxDzhFBYhPfY4Puhf63+L+YXRF8fKG1ocxHYX6ZZRQEQahoIkoRFDgcvoUxNcynUr7H4hLgzMdgxPvFX3jvGvhrMijrdRYVHZOcgiAIFUlEKQKHX0Xg9BHYFEHfsdYxy2wUHVfyxb/5D2BlLvXnlBYEQaikhFQRKKWGKqXWK6U2KaXu9XN8tFIqQym1wvr7dyjlcRTYonviapvPWD8+gqHPwP27jEMYoOM5cPEkuORDuHZm4Bvk55hP59oGh3fC401g18pjFz7cLH4XdspiPoJQHQlZMLxSKhp4DTgDSAcWK6W+11qv8ar6udb6plDJYcdRaBsR/HsW7F4Jzkgiu2koKgriarn3lYLjL6RECrLNp7ZMQ5vngOMoLHoNLnzLlO3fCtP/CyMmGdNTVWHqneZzwqHwyiEIQrkTyhFBX2CT1nqL1jofmAwMD+H9SqTQbhqqdxx0u8Szwkk3QcMg1i8ecJv/8j2WjtuzCrL3Qc16Zj/3oLvOzPGwcQZsmh284GVl6/zySZYni/UIQrUmlIqgObDdtp9ulXlzkVJqpVJqilKqpb8LKaXGKqWWKKWWZGRklFmgIvvEL2WZfZx2fRSc9QTc+HvJFxrykP/yJe+ZzyN74P1hbh/DUZsicJqNSpqZnLXbnPfJCPjnj5Jl8mbHMvjgPJjzWOnP9cauTLb8YtJtCIJQbQi3s/gHIEVr3Q34GfjAXyWt9dta61StdWrDhg3LfLPCQpsT12n/r1HHfHY6J/gLOc8tjox1kG0pre2/w6LXzbYzd1G0n8ltdp7vCM+kwMaZ8HUpXSeOPDiQZradn/ZjpY1qctiytX54PrxzWunOLy+0hoLc8NxbEKoxoVQEOwB7D7+FVeZCa52ptXZ2N98FeodQHorsPgJnqGfNJLhjHZzlZx3jY+Wb/7i3Z9xnCWHJ4C9c1QdrtKJK+TU93sidHTWuNhQ6IGOD+9i315fuegVBpO0uLVm7IWd/6c5ZMBGeaOw5whIE4ZgJpSJYDLRXSrVWSsUBI4Hv7RWUUk1tu+cDa0MlTL6jyG2WMTd3b9dpCtEVkERuQl13o1roJ5Q1EK75CYXG0bx/q9nfNAtyvZy33vb8Gokw51F4rQ/s32LKVn7ue4/8bN+cSZmbYddf/hVBfo55nuUf+5f56EHjLA/E8x3hxa6Bj/tj2YfWtf1kkbWjNeRlle7aQvjRGvKOhFuKiCRkikBr7QBuAmZgGvgvtNarlVKPKqXOt6rdopRarZT6C7gFGB0qeQ4ezSdaVYKJXtste/+BrcahHAxOf8ae1fDHm/BKD9PQfnwRPN3Ks2F0eJlOsvdB2q/WPdP8X7+oCJ5s5rvGwv/1grdO8a8I5j9nPn951l2WudmEmQJMuRY+usA4xwNRkBP4mF85LUWuvb7HnP1w8B/3/pL/wVMtAj8vmEWGFjxfuUxNhQ7YvSrcUoSPRa/BU81N2HVl5MA2WPp+uKUICSH1EWitp2mtO2it22qtn7DKHtJaf29t36e17qq17q61Hqy1XhcqWbJyHURTCRSB09zz073wXDt38bNtTA97/1ZfZ2xBjrHt2/0K9pHAMykw/V6Y9YhxVNtZbVuIx95Lzthg7jehLuy3Vltb+wPMe9p33oM/RfDrC+ZTRZkGbEJdozim3mnMPvssU9TC/3OPNGY/ZuplZ/peLxicZjVvZfdKD3jpBPe+c92IuU+Zkc6EukYOO8s/hNmPwq8v+t4na7fJLFvR/PoivDnAjMKqK1oHHtGt/sZ8HkqvGFlyD5duZP7hcPjh1mo52gy3s7jCKCzSIVQEwdj7/WEz4+RYjeMrPXydsYd3wAfnezbyuV528j/eMI3zN37s/84f7nzbIjz2hi7fNhyf9xS8dTJ8dKH/496oKFg/1bNs11+efo3Jl5tGboF1/+fa+F5n5ZcmyslJzn7Yt8mzjlMROHvx+TlmlORtHnPOz1g52f0+Zj7oPr5xFqz/yWx7N0rT7jZmqw9LiHTW2iiXw7uKr1ccRUUmusuJU3nuWe2//sZZ5q8q89tLpuPi973ZIvgqgqdbwlfXBl/fOYL3HpFWAyJMEYQo9UPKQLhrE9zuPVfOD/F1y3aP7b97Nk5Oe783u//2LXM2drttPX3ngjsAbw/yPWezbZ7DUcupG5foW2//ZvjiKs+yvyZ7RlZtnAGzJviX18nX/zbzHpwrv31wHrza29Pn4RoRHDVmoiebwhv93ce3/wk/3e85UW/LL9aG7TqfXASbfjbba74z61E7+fNt93Yge/W8p42JbuZ4+HJ08c8ViKMH4NF68M5gSPvNlCU2Mp+B0ph/cpH5c54/oS4s+8gozd9e8fRB7d9q9jcUMxM+FGTthq/GuGfZe7PmO6veMShQ+72Ke75Ns2Duk8VfwylPaSiqfilkIkYRFOkQjgj6XQ+JDaGuv2kSXrTs57n/80PG1l9aAjVAzt57DZvCyfFjilnqN1LXP0cyPK9dEqu/DqyovHE2tnVamM/MjeZzj2Ur//YGeL2/MfM4/wELcv1f/70z4PfXPFOL55UwE/rIbpjY3v+xp5rDyi9g7zpPZTHvKWPaAzMaKcuEO7upyvnMNZMsmYJYBtXp//jzbXi2NfxsjXickVjpi83nysn+z//wXzDnCff+tkXlkyxx5nj4+wtYNxV2Lve1qTt70/6i5uzvcdvCkt/rB+fDpyMCm3c+vgh+ecb/MY8ORoDnzs+G9CXufafM5b0QldZhn7QZOYqgCKJCoQgmHILOftY5GPmZ//oXvOW5/9vLpudS3oyY5N72N5S1jwhKwtmolMRDJUTz+OOn/5qe62HLLuzs0cZavfq/PoW9q+GXpz1HBNv/9L2W06m+8ovSy3Ekw63w7Hx3E7zeL7CyyFhrRi/LP/b9Zz6wzfRYHfnw5TVGoTjv5YyAAmN3BneD9sebbtNVYYHv+thFhe7v1Pu7zfQyp636yv8s9i1zYb7l6N/4M0waCn++5VuvtDj9N9GxZqT5w63u97JvU3D+j5Wfw6SzYdGrvsd+vMP8XsAEXIBvB+XoAfh7SvH3KLIpj7T5/ussfBXeHeIesTlNVqu+MjK8Pdj4045kwJTryp5TbOqd8EiSZ5nzHnvWwGejPDsiISBiFEGh1sRUpLO43en+y2vVhw5nh/7+/sw4ZWVVCf9UAM1TTY6m0uIdfjrncdNg+EsG6BoRHIXvbvA97sz6ml+MMy9Q4rzcgzCxnW+5XY78bP8KKG0BfHejOzoLTM/j1T6mx7pvgxklfXqJkX1iO/dkQzt2J/hnl5pG4Ntx8HwHT3NEdoa7cc3P9rzGh+fDjAfg6zHuso9LyJN1yEoAsHeNaUAndnQrpYPb4ZF6xTfgr/aBxdaseoe1Foc9Y6/Tn/WqbZpQocMoOfs7c5rvMiyFOXO8aVy1dvfCnbP3wT3ysysCreGbcb5rjHtTUq++sADmWWalPV6RXE4Zdi4z/rSJ7cz/iDPq7uhBE4gQKPrJkWeeLWe/GRHbn8nJso/M50/3mgWvfns5pKOGiFEEtXYuon3UjpIrlhfRsdDHmhF8o1eP+ryXQ3PPurb5e0l+s3WUD20Gw1lettfhr5nPE288tmvvWmEaDH/mrCLrn7es9mWt4e1T/R/bHiiNh82E8WQzY34KxOGdpvGedI6x/xdacyWdYbIHt8ETfhZAcuKdF+qr6+DvL822PQhg+5+mpwruXrEdfz3pTbPMMqrerPoafrzdbOsi04M/stvtF1o/3ZTbRzB28rONopt6h9VgW4rAnop92t2wwmuE7Mg1Jrb3h7nNL86Gzr6w057VplF9rIFn9Joj360I7L6cL66EDdM977XwVfj9TXOtdVPhj7c8f0P+lrD19/tz/hSKixraMBOeOc6EJn9lKePCAhNW7QxqWPejMQ3OfqQYhWW9C6evbdGr8FTo/qcrYBZV5aBmRgWH5CkF50w0Ka2jY6DDULejuHbj4K/T6VzjAD64reS6Zz7m9h0kNoFW/eGfhWY/Psn0zNoMOrbQyDaD4MpvzPM5l+S8+kdo1MlsD33S2OlDyX4/jV9Q5/nxK9RpYcxS3wVQYN7RWcXxzVj/5X8FsNN748iFhEZus91eW/DB3zZz1xdXBi+TE6cf6tyXYY8toMA+d6TgqLt3nXvIpF9xjlICLenqDPkE06A7G3H7/Iy/v3QrNCeFee7oqHeHwA2/u++VYYsiL8h2K7ZJtpF0QY57tPZ6P7hlhWlw1/7gK+PMB3zL+o3z/zxO7CMwb/NbIEVQWOCphLb9Cn+8bd7p7EeMoms3xG1CWvYh1Kzvrn8gzfhUTnvIrRTtchQ30j1GImZEUFRRj9rjcve2Uu4Zy5d9DhfaIlKOG+B5XrIfswRAq5Ng5Ccl3/eCt3F1WVS0MdNcOx3qtjJlSdZnMIvs2LnkI899R77bafbfNPjvNmh9cuDzo2t47l/2JdRKLp0M3vj7Zw8Ge8oPJxeWg128JPwN/T2w3qcjr/g1tEtKVBgMj9YzkwT9seor9/bfU0w6j38Wmf1Fr8LWBbDc67doV6C5h9wRahumG6UWiJVfwoaf3Puvn+hWAPZw4Pwc9+JRO5e7ywuOer6r314uXQSQ3ZdS6CdDr31U4jIjWd9ToImQB7fBvo2eZdPvdvtoomPhlZ4mhBaMgsmxTSr9crQJs7Z3ALxNfyEighSBGWLtb3FG+eTUP+kmSPHTAP7r9eCuP/JTqG1l2IiJh9HToFFXX99CTA1ocoLv+d4cf5HbAdblfHe580frHI1Ex5VOGThTaTvJO+x5rKaXk8ubVK847Vb9YFSQPeRA5AQ5I9vnPD+5jbyfL5Q0DvQ9auMb+etTOPRPgDp4OjhDjTN8eP00d9kH5xrfTNYeE5a5dT60PNF9/JOL3I3Yqq98J/7ZCRTN5M2S9/xHfhXkuOddACydFJwvy4k9RNc5m/ubcTBpmImgeqWH+3hhnlEGwYwO/Y3ct1l+EKcJLhC51v/WmwNgqxX27K0IQuQniBxFYEWUKMrpRZ71BIz+sezn10yCO9cZpTF+jzEX3bDQd60Df//8V33vWxYd41YsTbq5y52KoJG1zkLTHnD35uJlS7XZLZ22Xqfz2a4IgsF78Z24RGjRp3TXKC/2+3nuGrUD1w80SisrY+dZIzc/OFN2lJRgsLjRVKBrlwV/81GcbJlrwjI/OM/0alNOdq/4Z6e0vxV/BEoT4i+SLWN98Ne1K5efHzS+nb8+NY32pKGedR35xkcRDIeK8UOWNBHN3/EMr/Rru0Oz2mHkKALno1bQpMUy4x1f3dnq3V87A+qlmO2GndzHxy2CYc+b7ZQBZilNuzJxKoIOQ82xk++A+DrFy3DuCzBmDgx/HZp2N+Ylp1mrtAvdOBVBk25G6UVFB5l5tRyp0wJqN/Mtb3e68Z3444qvg88FFYgBt3ruR8dA90vh6mJMW6dPKP6a/pyYALeuNKMtJ636F29mKonievN2E9uh7ZDQEJr38l83Psm29kc5sm6qb1lpTGc5XqHOb5wUuO78ZwMf8+ZY1isP5twQrQUSOYrA6mmpim6ESou9R9j/FvcktVYnws3LTW/e7mxu3MUdnQSmMfAXxlmznnXMzz/laePhnq1uWyxA897Q83Jz3u1/m3Wb+44tvVnHqQi811+w3wvgtAc999sGWPPgpiD/EZJaeY6M/M3oHnCr/xHBhEPGqWcfll9Xyrketyx3K25vimugvd9LsNSo7emPaTcksJIrT7J2mdX+atuioexROANuMWbQ8matn1FxUYCQ0Bp+vvsQOl5ddPIzv8hJs56+ZQeLMQs6z/E2tZYTEaMICnGahio5dkXgPZU9KgoSrCHqtTN8HbnFUZwtPDbBzG+4fTXcGiC6Sik457nAPT9/XPG1TRF4+SX+Mx/Ot82uPeUuz+ODbZEe9n8ap9Pbm6RWcPcW9zVbnQRj5ppe/+kT/Edq1ajtOzo5YYR7296wtCylOat+G3dMvTfFmQhivRTB7V55h5JaQWKAZ4mxKYLUa93rbqeWEFN/rDTvbUYFTurYRl+dz4cYr+++QQfz2f7M0MrlpHGXirmPN13+FfhYcnvfEWNJ1Akic0EZiRhF4DQNVXpF4PwngcA9HDAjBLtTuCRq1Q98zBn6mZAcuBdbFtoNsfkIvN58ww7Q6yq48U8Y+4vnsRv+gBapZv7F2Hnmb9B9cM10z0blGivqpEVfuO1vI/9Ba3JU3ZbGFHPFV9BtBNRt4StfbIJv2UXv+paV9R/QGY2S2MRT6dUoJf+QlwAADxFJREFUxjQXW9Nt4kho5OtjUdHmPXgTHeupbGvVh15Xm/r23xTAkIfd23UDKNaSsD9Doy6eisB5v9anQoP2vp2AvmON0r7cK6Q0VNRKhpPvdO/7MyElHVf+9/X+7uzExPlGDto5+U7fkaNMKDt2nKahCrdPl5Za9c38AyjfKJFAjc+/5wQ2w5QHzt58oLkTDTtCsx6eZU5zTcMO7vMH3QvHWQnmul5gJuUVWKYbZ88XjHJp0s3TXAaQlOJ7b2cP+o615p2PmetfxutKkbjt4klwvRUl4hzR9b7ayOWkcRd3HW+iok1M/QVvwd0bfWeIn/eyu2GNjjN+nI7neD6Pk4G3wYMZpjG208WWvHDMbLh5mfua9tGDPRQa4DbbDFv7yKxeilkAyYnz+6wXoHFVUUZpVxS1kiHW+o0MvN2/OfC8lwOvRV5W6vvJsuskOs539Gen20jfkV9xfptjJGIUgds0FN7kTkHhjPCx27iPlUAK0LsRLg/sPe16KXDlt3DO88GfX1JI6oj3ofdo9z9Ka9ts4aSWcP0Cs+qcHX8huM7z6zSDvmN8zV71WlvHvUYEcbXh1P+ayYLeHH+h+159xxg5T7rJt16gkOD8HNNwdx9p9r19K21O9exh97wcRlmzdv31dKOifZWJfdRXKxmS20LPK8y+87cHcMrdcL8tTUJSS+PLiU3wvFdUtGfvtV5rM1/EufyrtymsojtjCQ3c99Ta02TnDAFv2ddz1FAeeI/E7ETFupWTP2JrukcATuVc2kCNUhAxM4sLqSLOYjBprW/8s/gfUrCMW1j8TNzSroccDHes9vzRth1cuvOL+wex0+SE4N9T0+6e+8HM9Rgzx0ws8v7N3G9bOGXn8sAx8TVqB5dOJCrGPforDOBXsOM0j3mbCgL9tlv2hbOfg8ZdzRyMqGi45EOTXsIVPODn3NhavuaNU+4yf28ONPtOn4rdQdx2sKefwNvXVd6/uc7neU4yvHGxWZrVSfuzYJszcZw2yrXAdm4wYeAP7C4+PYidq74zz1hS7q3iRgRxCW4F6kxP7m/iWzkRMSOCItes2yqgCMCYTMpD1sZd/WdHPfNxE6ESivdRs55nFElpKY1Mwb4nuzzn+8nF449a9Ut+jvKYa/DAbhO11XeseyTgcY/2xpxzvdWYec/WLgmloN9YE17sNAt1GQ6XlJCKvLiGqo2l3M941JLJ6lN2vdBTCYB7ZOuc52JXOiM+cB8f7CcVhD+8RziX2hIXtj3NmBTttOpnGnwwiuuaadCgo9kP9P2daEtqeOtK8y4u9OM/8kfrU6G1n9nbThMeADqIEYGlCJz+lxAuqxoxIwJn50mFOe93paH/zeavMpFyssnkGQqUgjvXm55WcZPIiuMePyOrY1mtavhrZiJTdKzlG3rOf72bl3juO01DzXv71k29tngnZCCcph17eHFximDIw9DnOnej7zQV+QtwSEg2I7Bvb4AVn3gq7q7/MkopY73xM8x9wuTfOepnFriTUZPNLGd/dL/Mf3mD9p6jwJv+NGtsJ7f1rHf3FtOTr1nP+KJ0kdvX0W2EWUDJm/ptPPNY+euY1Khjwmh/us+sJqiLfN9vjTpw9jMmJUVMvHs05gx9DqGPIGIUgXPtiaoyIIhIrv4htMsAHssoBfxHXjknAXUfZWzqpcFply8t0THw79m+TmCAc/2swRwMg/5r/jl6XO5OheDtn/CWwe5rcJqGilsDuM91RhF4BycoZSLXnClAuo+E31832zcuNu/4dSuVxf073aallifCRe+Y7YcOmHpOmVVUyb8lbyUAnk7sln2LPx+s+SKtYeNMk2bcX2gvmEZfKXcoa0GuUXi1kt2TBLtdCj0uM39g8pP9PcX9PYfQNBQxiqDQchKryh9AGrkoFZpZqKGk7RCTbiH1Ov8NS6hokVq+14uva9KmAFw+xXMxmwG3lRxU4HR+n3Bx4DrNexfvm6lV3yz3WruJWxE07OBpEnH2ku9YZ5zAzoY/KgoPS3dFjfxrWY5o5/38TRQDd8Sb06yVn2V8PfdsgUWvw4z7fCd7Jrc1CjrTSo0izuJywGUbCq8YQjWjVb/ySWJYmWh/hvlzcsYjJZ+T1LJ83oO/5V79jUy8o8K8Gf6a/8WLyhP787pGH14NzFXfmRXgThtv9l2KwDZr3RkoEChFhtOUKeGjx06hNl+Q6AFBqGI4e8re630XR8/LS65TnrjWYvZqUtsMMiMtpz/AGRptj7Jy+nwCrSrodCrLiODY0daIQHwEglAFGbfI/+zw4mh9qmmIKwSnxaGEBqZFHzNxrYfNP5R6jXGOD7zN/zmxtUyU06n3lI+ofog4RSBjAkGogpQlX9DVfhLThQodZDSKUr4T12JqwOD7A58TFWWinEJIxJiGioJU2IIgVAIaHx9uCUpH61NM2o3SRo5VEiJmRFDoNA2FWQ5BEIJgzNxjy+1f0dSsZxIfVlEiRhEEa8ITBKES4J26WggpEWcaEgRBEDyJGEWQ2tdMu4/qWsxiEYIgCBFIxJiGGrTqBOMziJIhpyAIggcRMyIAxO4oCMKxYc90Wo2ImBGBIAjCMdP5PLO2Q6tSzHKuAogiEARBKA39xoZbgnInskxDgiAIgg+iCARBECKckCoCpdRQpdR6pdQmpdS9xdS7SCmllVLlnGRdEARBKImQKQKlVDTwGnA20AUYpZTyyRyllKoN3Ar8ESpZBEEQhMCEckTQF9iktd6itc4HJgPD/dR7DHgGCN2qC4IgCEJAQqkImgPbbfvpVpkLpVQvoKXWemoI5RAEQRCKIWzOYqVUFPACcGcQdccqpZYopZZkZGSEXjhBEIQIIpSKYAfQ0rbfwipzUhs4HpinlEoDTgS+9+cw1lq/rbVO1VqnNmzYMIQiC4IgRB7KvXJXOV9YqRhgAzAEowAWA5dprVcHqD8PuEtrvaSE62YA28ooVgNgXxnPrarIM0cG8syRwbE883Faa7896ZDNLNZaO5RSNwEzgGjgf1rr1UqpR4ElWusyrSMX6EGCQSm1RGsdUSGq8syRgTxzZBCqZw5pigmt9TRgmlfZQwHqDgqlLIIgCIJ/ZGaxIAhChBNpiuDtcAsQBuSZIwN55sggJM8cMmexIAiCUDWItBGBIAiC4IUoAkEQhAgnYhRBsJlQqxpKqZZKqblKqTVKqdVKqVut8vpKqZ+VUhutz3pWuVJKvWK9h5VWmo8qh1IqWim1XCn1o7XfWin1h/Vcnyul4qzyGtb+Jut4SjjlLitKqSSl1BSl1Dql1Fql1EkR8B3fbv2mVymlPlNKxVfH71kp9T+l1F6l1CpbWam/W6XU1Vb9jUqpq0sjQ0QogmAzoVZRHMCdWusumNnZN1rPdi8wW2vdHpht7YN5B+2tv7HAGxUvcrlwK7DWtv8M8KLWuh1wALjOKr8OOGCVv2jVq4q8DPykte4EdMc8e7X9jpVSzYFbgFSt9fGYuUgjqZ7f8/vAUK+yUn23Sqn6wMNAP0zCz4edyiMotNbV/g84CZhh278PuC/ccoXoWb8DzgDWA02tsqbAemv7LWCUrb6rXlX5w6QrmQ2cBvwIKMxsyxjv7xszofEka/v/27u3EKuqOI7j319MKV4YL5RYQTYVEkKNBSFpIRgGIt0woszCeuzFeiiiHip66EG6PEQJSVhJRaYVEhVOIfQQXsJMLEgTbEJToiwLw8u/h/U/09G8zVHnOHv/PnDgnLUXe/Y6/5n5n7X3Pv/Vkf3U7jH0c7ydwLYjj7viMW4UrRyTcVsJ3FzVOAMTgE2txha4G1jU1H5YvxM9ajEj4CQqoVZBTocnU9Z2GBcRO3LTTmBcPq/Ce/Ei8ChwKF+PBX6PiAP5unlMfePN7Xuy/2ByKbAbeD1Ph70maTgVjnFE/AwsBLYDOyhxW0+149ysv7E9pZjXJRFUnqQRwPvAgoj4o3lblI8IlbhPWNJsYFdErG/3sQygDuAa4JWImAz8xX+nCoBqxRggT2vcSkmCFwLD+f/pk1oYiNjWJRGcqBLqoCbpXEoSWBoRy7P5F0njc/t4YFe2D/b3YipwS1asfYdyeuglYFQWOoTDx9Q33tzeCfw6kAd8GvQCvRHRWMVvGSUxVDXGADcB2yJid0TsB5ZTYl/lODfrb2xPKeZ1SQRrgSvyjoPzKBedWip6d7aRJGAx8F1EPN+06SOgcefA/ZRrB432+/LugynAnqYp6FkvIh6PiIsjYgIljp9HxFzgC2BOdjtyvI33YU72H1SfnCNiJ/CTpInZNAPYTEVjnLYDUyQNy9/xxpgrG+cj9De2nwIzJY3O2dTMbDs57b5IMoAXY2ZRymJvBZ5o9/GcxnFNo0wbNwIb8jGLcn60B/gBWAWMyf6i3EG1FfiWcldG28fR4tinAyvzeRewBtgCvAcMyfah+XpLbu9q93G3ONZuYF3G+QNgdNVjDDwNfA9sAt4EhlQxzsDblOsg+ymzvwdbiS3wQI5/CzC/P8fgEhNmZjVXl1NDZmZ2DE4EZmY150RgZlZzTgRmZjXnRGBmVnNOBGZnmKTpjSqpZmcjJwIzs5pzIjBLku6VtEbSBkmLcs2DvZJeyLr4PZLOz77dkr7KmvArmurFXy5plaRvJH0t6bLc/Yim9QSW5rdlkfScyloSGyUtbNPQreacCMwASVcCdwFTI6IbOAjMpRQ7WxcRk4DVlJrvAG8Aj0XEVZRveDbalwIvR8TVwPWUb4xCqQq7gLIeRhcwVdJY4HZgUu7n2TM7SrOjcyIwK2YA1wJrJW3I112UUtfvZp+3gGmSOoFREbE625cAN0oaCVwUESsAImJfRPydfdZERG9EHKKUAZlAKZW8D1gs6Q6g0ddsQDkRmBUClkREdz4mRsRTR+nXak2Wf5qeH6QsrnKAsprUMmA28EmL+zY7JU4EZkUPMEfSBdC3ZuwllL+RRrXLe4AvI2IP8JukG7J9HrA6Iv4EeiXdlvsYImnYsX5griHRGREfAw9TlqA0G3AdJ+5iVn0RsVnSk8Bnks6hVIJ8iLIIzHW5bRflOgKU0sCv5j/6H4H52T4PWCTpmdzHncf5sSOBDyUNpcxIHjnNwzI7Ka4+anYckvZGxIh2H4fZmeRTQ2ZmNecZgZlZzXlGYGZWc04EZmY150RgZlZzTgRmZjXnRGBmVnP/AhzfhbdnTawyAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liJcVOHeFBre",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "2175e15f-0e27-4ced-98c4-8ca0364108c1"
      },
      "source": [
        "model.predict(test_ds)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Gender': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'Ever_Married': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=int64>, 'Age': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'Graduated': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int64>, 'Profession': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Work_Experience': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float64>, 'Spending_Score': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Family_Size': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float64>, 'Var_1': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=string>}\n",
            "Consider rewriting this model with the Functional API.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8.0251157e-01, 1.9573884e-01, 7.2357875e-13, 1.7495771e-03],\n",
              "       [1.3007881e-10, 2.2386316e-06, 9.9999774e-01, 1.5740377e-10],\n",
              "       [1.0000000e+00, 3.0961317e-08, 2.4607091e-27, 2.6623243e-13],\n",
              "       ...,\n",
              "       [9.9976951e-01, 3.9877381e-05, 9.6664502e-05, 9.3900839e-05],\n",
              "       [3.7144897e-12, 8.1389606e-02, 9.1861039e-01, 1.8281127e-11],\n",
              "       [1.5008771e-16, 1.0573472e-05, 1.3516941e-02, 9.8647249e-01]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlE7e_bILXnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = model.predict(test_ds)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIs28DyOMaJx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f6f0c9fd-5a30-4e0c-b4a2-0993c59c917f"
      },
      "source": [
        "a.shape"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2627, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1dkIx0mMO__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = np.argmax(a, axis=1)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP1lpl5TUREM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5a317353-6fed-4b6f-9598-78e213c7b865"
      },
      "source": [
        "b"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 0, ..., 0, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMs3g3tFLDzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dff = pd.Series(b)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lbdMPxvbLE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dffff = pd.Series(b)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc8GKpwVM6fz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df4 = dff.rename('Segmentation')"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xETh2scjcEAN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "c69ea93a-89b8-46fc-bd54-dfe9f0caed7f"
      },
      "source": [
        "dffff"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       0\n",
              "1       2\n",
              "2       0\n",
              "3       2\n",
              "4       3\n",
              "       ..\n",
              "2622    3\n",
              "2623    0\n",
              "2624    0\n",
              "2625    2\n",
              "2626    3\n",
              "Length: 2627, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcK3CmELWM9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df4.replace(to_replace = 0, value = 'A', inplace=True)\n",
        "df4.replace(to_replace = 1, value = 'B', inplace=True) \n",
        "df4.replace(to_replace = 2, value = 'C', inplace=True) \n",
        "df4.replace(to_replace = 3, value = 'D', inplace=True) "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W46Y8nDicGwx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "16d74766-1a55-431b-8484-74c09aed453b"
      },
      "source": [
        "df4"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       A\n",
              "1       C\n",
              "2       A\n",
              "3       C\n",
              "4       D\n",
              "       ..\n",
              "2622    D\n",
              "2623    A\n",
              "2624    A\n",
              "2625    C\n",
              "2626    D\n",
              "Name: Segmentation, Length: 2627, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGJt9dZ-NBUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftt = pd.read_csv('test.csv')\n",
        "dfft = dftt['ID']"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2NCy4SANZdF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "ca109ab4-9593-4dd7-d715-6900d0eb81a7"
      },
      "source": [
        "dfft"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       458989\n",
              "1       458994\n",
              "2       458996\n",
              "3       459000\n",
              "4       459001\n",
              "         ...  \n",
              "2622    467954\n",
              "2623    467958\n",
              "2624    467960\n",
              "2625    467961\n",
              "2626    467968\n",
              "Name: ID, Length: 2627, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r21DF-LBNdSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df3 = pd.concat([dfft, df4], axis=1)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb-xKzxLNpWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df3.set_index('ID', inplace=True)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXXxj96zNqqv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "04e709ac-ccb5-4077-f700-e8d9c2b5be7a"
      },
      "source": [
        "df3"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Segmentation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>458989</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>458994</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>458996</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>459000</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>459001</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2622</th>\n",
              "      <td>467954</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2623</th>\n",
              "      <td>467958</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2624</th>\n",
              "      <td>467960</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2625</th>\n",
              "      <td>467961</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2626</th>\n",
              "      <td>467968</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2627 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          ID Segmentation\n",
              "0     458989            A\n",
              "1     458994            C\n",
              "2     458996            A\n",
              "3     459000            C\n",
              "4     459001            D\n",
              "...      ...          ...\n",
              "2622  467954            D\n",
              "2623  467958            A\n",
              "2624  467960            A\n",
              "2625  467961            C\n",
              "2626  467968            D\n",
              "\n",
              "[2627 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZuhukZLOSzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fr = open(\"AV_CS6.csv\", \"w\")\n",
        "df3.to_csv(r'AV_CS6.csv', index = False)\n",
        "fr.close()"
      ],
      "execution_count": 86,
      "outputs": []
    }
  ]
}